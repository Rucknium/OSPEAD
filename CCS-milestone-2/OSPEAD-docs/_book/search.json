[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OSPEAD-docs",
    "section": "",
    "text": "1 Introduction\nThis docs website provides a high-level explanation of the OSPEAD (Optimal Static Parametric Estimation of Arbitrary Distributions) code that produces a new and improved decoy selection distribution for Monero. Starting with Chapter 4, the purpose and major features of each piece of code is explained.\nIt is possible to run each piece of code in sequence to produce the new decoy selection distribution. However, the current version of the code requires a machine with RAM in excess of 150GB and a running time of about two weeks. Future improvements to the code could reduce the RAM and run time requirements. Instead of running the whole OSPEAD code, an interested reader could easily run the simulation code in Chapter 3 in a few hours on a consumer-grade desktop or laptop. The simulation code includes the essential components of OSPEAD and demonstrates its validity.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#procedure-to-estimate-the-real-spend-age-distribution",
    "href": "index.html#procedure-to-estimate-the-real-spend-age-distribution",
    "title": "OSPEAD-docs",
    "section": "1.1 Procedure to estimate the real spend age distribution",
    "text": "1.1 Procedure to estimate the real spend age distribution\nOSPEAD’s objective is to create a new decoy distribution that is very close to the real spend age distribution. The best decoy is a decoy that closely mimics the real spend. From Monero’s blockchain data, the real spend age distribution must be separated from the decoy distribution and measured. The statistical procedure boils down to two steps:\n\nSeparate the distribution of rings created by the standard wallet2 code from the distributions of rings created by nonstandard wallet code. The Bonhomme-Jochmans-Robin (BJR) estimator is used for this step (Bonhomme, Jochmans, and Robin 2016).\nSeparate the wallet2 decoy distribution from the real spend distribution. The Patra-Sen inversion estimator is used for this step (Patra and Sen 2016).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#main-results",
    "href": "index.html#main-results",
    "title": "OSPEAD-docs",
    "section": "1.2 Main results",
    "text": "1.2 Main results\nSee Chapter 13 for full details. The best-fitting decoy distribution was determined to be the log transformation of the generalized beta distribution of the second kind, with parameters scale = 20.62, shape1 = 4.462, shape2 = 0.5553, and shape3 = 7.957. On average, an adversary using the Maximum A Posteriori (MAP) Decoder Attack against users who use this distribution would have correctly guessed the real spend in 7.6 percent of rings (Aeeneh et al. 2021). This corresponds to an effective ring size of 13.2. Note that the minimum possible guessing probability is 1/16=6.25%1/16=6.25\\%.\nThe estimates also reveal how much privacy Monero users lose by using the current wallet2 decoy selection algorithm. The estimated average attack success probability that an adversary can achieve against real Monero users who were using the default decoy selection algorithm since the August 2022 hard fork was 23.5 percent. This corresponds to an effective ring size of 4.2.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "OSPEAD-docs",
    "section": "1.3 License",
    "text": "1.3 License\nAll code in this OSPEAD-docs website is released under the GPL-2 open source license, copyright Rucknium 2025.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "OSPEAD-docs",
    "section": "1.4 Acknowledgements",
    "text": "1.4 Acknowledgements\nThis research was funded by Monero’s Community Crowdfunding System.\nThanks to isthmus (Mitchell P. Krawiec-Thayer) for discussion of certain methodological issues and for creating a nonstandard transactions list. Thanks to jeffro256 for improving the documentation of Monero’s default decoy selection algorithm. Thanks to plowsof for lending computing resources for collecting txpool data. Thanks to gingeropolous for managing the Monero Research Lab Research Computing Cluster, which was used to perform the computations for the statistical estimates.\nThe following people gave feedback on the research process and/or contributed in other ways: ACK-J, ArticMine, bob, coinstudent2048, garth, hyc, jberman, kayabaNerve, koe, mj-xmr, monerobull, moneromooo, neptune, SamsungGalaxyPlayer, SerHack, SethForPrivacy, and Syksy.\n\n\n\n\nAeeneh, Sina, João Otávio Chervinski, Jiangshan Yu, and Nikola Zlatanov. 2021. “New Attacks on the Untraceability of Transactions in CryptoNote-Style Blockchains.” In 2021 IEEE International Conference on Blockchain and Cryptocurrency (ICBC), 1–5. https://doi.org/10.1109/ICBC51069.2021.9461130.\n\n\nBonhomme, Stéphane, Koen Jochmans, and Jean-Marc Robin. 2016. “Non-Parametric Estimation of Finite Mixtures from Repeated Measurements.” Journal of the Royal Statistical Society. Series B (Statistical Methodology) 78 (1): 211–29. http://www.jstor.org/stable/24775334.\n\n\nPatra, Rohit Kumar, and Bodhisattva Sen. 2016. “Estimation of a Two-Component Mixture Model with Applications to Multiple Testing.” Journal of the Royal Statistical Society. Series B (Statistical Methodology) 78 (4): 869–93. http://www.jstor.org/stable/24775367.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "requirements.html",
    "href": "requirements.html",
    "title": "2  Requirements",
    "section": "",
    "text": "2.1 Install R\nIf on Windows or macOS, go here to install R.\nIf on Ubuntu or another Debian-based Linux installation, install r-base and r-base-dev so that you can compile R packages:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Requirements</span>"
    ]
  },
  {
    "objectID": "requirements.html#install-r",
    "href": "requirements.html#install-r",
    "title": "2  Requirements",
    "section": "",
    "text": "sudo apt-get install r-base r-base-dev",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Requirements</span>"
    ]
  },
  {
    "objectID": "requirements.html#linux-system-packages",
    "href": "requirements.html#linux-system-packages",
    "title": "2  Requirements",
    "section": "2.2 Linux system packages",
    "text": "2.2 Linux system packages\nSeveral R packages will need to be installed. If you are using Linux, the R packages will likely be installed from source. Therefore, you need to install a few Linux system packages that are required:\n\nGSL (GNU Scientific Library). Instructions for installing GSL are here. On most Linux distributions, it should be installable with sudo apt-get install libgsl-dev.\nGraphicsMagick. On most Linux distributions, it should be installable with sudo apt-get install graphicsmagick.\nMany system packages are needed for the devtools R package. These can be installed with sudo apt-get install gfortran build-essential libcurl4-openssl-dev libxml2-dev libfontconfig1-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Requirements</span>"
    ]
  },
  {
    "objectID": "requirements.html#r-packages-from-cran",
    "href": "requirements.html#r-packages-from-cran",
    "title": "2  Requirements",
    "section": "2.3 R packages from CRAN",
    "text": "2.3 R packages from CRAN\nThe required R packages hosted on CRAN (Comprehensive R Archive Network) can be installed by initiating an R session and inputting\n\ninstall.packages(c(\"devtools\", \"data.table\", \"RCurl\", \"RJSONIO\", \"future\",\n  \"future.apply\", \"parallelly\", \"lubridate\", \"collapse\", \"actuar\",\n  \"gbutils\", \"binsmooth\", \"spatstat.univar\", \"distributionsrd\",\n  \"VGAM\", \"VaRES\", \"ghyp\", \"extraDistr\", \"GB2\", \"wrswoR\", \"Rfast\",\n  \"fitdistrplus\", \"bsgof\", \"PearsonDS\", \"animation\", \"triangle\",\n  \"viridis\", \"RColorBrewer\", \"ISOweek\", \"ggh4x\"), Ncpus = 4)\n\n\n\n\n\n\n\nImportant\n\n\n\nR may ask you if you want to create a personal library directory to install packages into. Select “yes”.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Requirements</span>"
    ]
  },
  {
    "objectID": "requirements.html#r-packages-from-github",
    "href": "requirements.html#r-packages-from-github",
    "title": "2  Requirements",
    "section": "2.4 R packages from GitHub",
    "text": "2.4 R packages from GitHub\nOne package must be installed from GitHub:\n\nremotes::install_github(\"dracula/dRacula\", upgrade = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Requirements</span>"
    ]
  },
  {
    "objectID": "requirements.html#ospead-r-package",
    "href": "requirements.html#ospead-r-package",
    "title": "2  Requirements",
    "section": "2.5 OSPEAD R package",
    "text": "2.5 OSPEAD R package\nIn an R session launched from a directory that contains decoyanalysis_0.1.0.tar.gz, input:\n\ninstall.packages(\"decoyanalysis_0.1.0.tar.gz\", repos = NULL)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Requirements</span>"
    ]
  },
  {
    "objectID": "requirements.html#fast-blas",
    "href": "requirements.html#fast-blas",
    "title": "2  Requirements",
    "section": "2.6 Fast BLAS",
    "text": "2.6 Fast BLAS\nEnabling a fast BLAS (Basic Linear Algebra Subprograms) is strongly recommended. Instructions for Ubuntu and Windows are here. On Ubuntu, sudo apt-get install libopenblas-base should install and enable the fast BLAS.\nWith Linux, to check that the fast BLAS has been enabled, open a new R session and input sessionInfo(). One of the messages should be\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3\nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/liblapack.so.3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Requirements</span>"
    ]
  },
  {
    "objectID": "requirements.html#multi-machine",
    "href": "requirements.html#multi-machine",
    "title": "2  Requirements",
    "section": "2.7 Multi-machine",
    "text": "2.7 Multi-machine\nIt is possible to run the most computationally-demanding parts of the code on multiple machines in parallel. These OSPEAD-docs instructions assume a single machine. Multi-machine instructions may be added later.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Requirements</span>"
    ]
  },
  {
    "objectID": "successful-simulation.html",
    "href": "successful-simulation.html",
    "title": "3  Successful Simulation",
    "section": "",
    "text": "3.1 Data generation\nThis chapter walks through a realistic simulation that successfully recovers the real spend age distribution of Litecoin (LTC) outputs. Rings are randomly generated as follows:\nApplying OSPEAD techniques to the simulated ring dataset, the empirical LTC distribution of ISO week 2022-10 can be recovered. The successful estimation demonstrates that OSPEAD can recover a realistic real spend distribution from 16-member rings even in the presence of nonstandard rings.\nFirst, the real spend age distribution of LTC is computed. This is done for the 5th, 10th, and 15th ISO week in 2022 using this code.\nNext, decoy distributions are needed. The main decoy distribution is the same as Monero’s wallet2 distribution: a log-gamma distribution with shape parameter 19.28 and rate parameter 1.61, plus a few Monero-specific adjustments. Two other decoy distributions are needed to represent nonstandard decoy selection algorithms that “third-party” Monero wallet developers may be using.\nA uniform distribution, where all outputs are equally likely to be chosen, is a naive distribution that a third-party wallet developer could plausibly use. In fact, a uniform distribution was used by the standard Monero wallet in the initial months of the blockchain (Noether, Noether, and Mackenzie 2014). The second nonstandard distribution will be a log-triangular distribution, another distribution that a third-party wallet developer could plausibly use. For a period of time, the standard Monero wallet used a triangular distribution.\nFor the simulation, 300,000 rings are produced. This is approximately the number of rings that appear on the Monero blockchain in a typical week. (Note that a single transaction may have many rings.)\nThree types of rings are produced:\nNotice that we use LTC distributions from different weeks. This shows that OSPEAD works even when users using the nonstandard wallet implementations do not follow the same spent output age distribution as users who use the standard wallet2 distribution.\nTo generate random numbers from the uniform and log-triangular distribution, the corresponding R functions (runif() and triangle::rltriangle()) are used. Random draws from the wallet2 decoy distribution and the LTC empirical distributions are accomplished by using a standard technique in probability computing. The cumulative distribution function (CDF) of each of these distributions are computed. Then, the CDFs are inverted by numerical methods to obtain the CDF inverse, also known as the quantile function. (Using numerical instead of analytical methods causes a tolerable slowdown in random number generation.) Uniform(0, 1) random numbers are transformed by the inverted CDFs to obtain random numbers with the desired distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Successful Simulation</span>"
    ]
  },
  {
    "objectID": "successful-simulation.html#data-generation",
    "href": "successful-simulation.html#data-generation",
    "title": "3  Successful Simulation",
    "section": "",
    "text": "Important\n\n\n\nKnowledge of the nonstandard decoy distributions is not needed for OSPEAD. In the statistical estimation, these distributions are treated as unknown. The only decoy distribution that needs to be known is the standard wallet2 distribution, which we know because it is in Monero’s open source code.\n\n\n\n\n\nRings where one ring member is drawn from the empirical LTC distribution of ISO week 2022-10 and the remaining 15 rings members are drawn from the wallet2 log-gamma distribution. 93 percent of rings in the dataset are of this type.\nRings where one ring member is drawn from the empirical LTC distribution of ISO week 2022-05 and the remaining 15 rings members are drawn from a log-triangular distribution with minimum one second, maximum one year, and mode one week. 5 percent of rings in the dataset are of this type.\nRings where one ring member is drawn from the empirical LTC distribution of ISO week 2022-15 and the remaining 15 rings members are drawn from the a uniform distribution with minimum one second and maximum one year. 2 percent of rings in the dataset are of this type.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Successful Simulation</span>"
    ]
  },
  {
    "objectID": "successful-simulation.html#sec-bjr-explanation",
    "href": "successful-simulation.html#sec-bjr-explanation",
    "title": "3  Successful Simulation",
    "section": "3.2 First step: Separate the wallet2 ring distribution from nonstandard rings",
    "text": "3.2 First step: Separate the wallet2 ring distribution from nonstandard rings\nDistribution component “A”, associated with transactions created by wallet2, makes up 93 percent of the rings in the simulated dataset. Transactions created by wallet2 also make up the majority of transactions on the Monero blockchain. Analysis focuses on this component “A” because most transactions are in it, but also because the second step requires knowledge of the decoy selection distribution, which is known for transactions created by wallet2, but not necessarily known for transactions created by “third-party” wallet implementations.\nThe CDF of component “A” must be estimated. I use an estimator developed by Bonhomme, Jochmans, and Robin (2016) to accomplish the task. The BJR estimator requires the analyst to specify KK, the number of distribution components in the dataset. In this simulation, I will set KK to 4, which is one more than the true number of components. I intentionally overshoot the true KK to demonstrate the performance of the estimator when the true KK is not exactly known.\nThe BJR estimator should take a few hours to run with the default number of threads.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Successful Simulation</span>"
    ]
  },
  {
    "objectID": "successful-simulation.html#sec-patra-sen-explanation",
    "href": "successful-simulation.html#sec-patra-sen-explanation",
    "title": "3  Successful Simulation",
    "section": "3.3 Second step: Separate the real spend distribution from the wallet2 decoy distribution",
    "text": "3.3 Second step: Separate the real spend distribution from the wallet2 decoy distribution\nLet FA(x)F_{A}(x) be CDF of distribution component “A”, which is a mixture distribution of 116\\frac{1}{16} parts real spend distribution FA,S(x)F_{A,S}(x) and 1516\\frac{15}{16} parts wallet2 decoy distribution FA,D(x)F_{A,D}(x). The CDF of component “A” can be decomposed:\nFA(x)=116FA,S(x)+1516FA,D(x)(3.1)\nF_{A}(x)=\\frac{1}{16}F_{A,S}(x)+\\frac{15}{16}F_{A,D}(x)\n \\qquad(3.1)\nThe BJR estimator in the first step produced an estimate of FA(x)F_{A}(x). The FA,D(x)F_{A,D}(x) CDF is known because it is the distribution produced by Monero’s open source decoy selection algorithm in wallet2. Equation 3.1 can be easily solved for the real spend distribution FA,S(x)F_{A,S}(x) because it is one equation in one unknown:\nFA,S(x)=16FA(x)−15FA,D(x)(3.2)\nF_{A,S}(x)=16F_{A}(x)-15F_{A,D}(x)\n \\qquad(3.2)\nEquation 3.2 can be computed simply, but a valid CDF might not be produced because of sampling variation. Patra and Sen (2016) provides a method for adjusting the computed FA,S(x)F_{A,S}(x) to a valid CDF (i.e. weakly monotonically increasing and satisfying limx→−∞FA,S(x)=0\\underset{x\\rightarrow-\\infty}{\\lim}F_{A,S}(x)=0 and limx→∞FA,S(x)=1\\underset{x\\rightarrow\\infty}{\\lim}F_{A,S}(x)=1).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Successful Simulation</span>"
    ]
  },
  {
    "objectID": "successful-simulation.html#simulation-results",
    "href": "successful-simulation.html#simulation-results",
    "title": "3  Successful Simulation",
    "section": "3.4 Simulation results",
    "text": "3.4 Simulation results\nAfter running the code in Section 3.6, we get the following results.\n\n3.4.1 Mixing proportions\nThe mixing proportions are the share of each distribution component (A, B, C) in the dataset. Above, we set these values in the simulation to 0.93, 0.05, and 0.02, respectively. The BJR estimator can produce an estimate of these mixing proportions, based on the simulated dataset. The exact values of these estimates are not very important for OSPEAD, except that the greatest value is assumed to correspond to the wallet2-associated distribution component. Also, these results establish context for the mixing proportions results from the real Monero dataset.\nThe estimated mixing proportions are: 0.8898, 0.0478, 0.0191, -0.0441\nNotice that the estimates are not constrained to add to 1, nor do the individual mixing proportion estimates necessarily fall between 0 and 1. What we see here is general agreement of these estimates with the mixing proportions set in the simulation parameters. The “phantom” fourth distribution component, which does not actually exist in the dataset, is estimated to have a “negative” mixing proportion.\n\n\n3.4.2 Kolmogorov–Smirnov statistic\nWe want to know how close our estimate was to the actual LTC real spend distribution. In the next section we will view some plots, but first we will compute a numeric metric of distribution “closeness”.\nThe Kolmogorov–Smirnov (KS) statistic is a common way to measure the difference between two distributions. The KS statistic is the maximum vertical distance between two CDFs. Its minimum possible value is 0 and its maximum possible value is 1. Figure 3.1 shows a visual of the KS statistic.\nThe KS statistic between the estimated and actual LTC real spend distribution is 0.0522.\n\n\n\n\n\nFigure 3.1: KS Statistic. Source: Wikipedia.\n\n\n\n\n\n\n\n\n3.4.3 Plots\nAt last, we will view plots that show how accurate our estimate is. Figure 3.2 shows:\n\nIn green, the CDF of the real spend distribution we are trying to estimate.\nIn blue, the OSPEAD estimate of the green line.\nIn violet, the decoy selection distribution of wallet2.\nIn pink, the ring distribution of the first “unknown” decoy distribution (i.e. the log-triangular distribution) combined with the LTC real spend distribution of ISO week 2022-05.\nIn red, the ring distribution of the second “unknown” decoy distribution (i.e. the uniform distribution) combined with the LTC real spend distribution of ISO week 2022-15.\n\nThe blue line is a close, but not exact, match of the green line. The OSPEAD method estimates the real spend distribution accurately, but it is not perfect. The source of the inaccuracy is likely that ring members are not completely statistically independent, yet the BJR estimator requires independence to work flawlessly.\n\n\n\n\nFigure 3.2\n\n\n\n\n\n\n\nThe next plot shows the probability mass functions (PMFs) associated with the CDFs plotted in Figure 3.2 . The PMFs were estimated by a simple method: computing the weighted first difference of the CDFs. The vertical axis has a log scale.\nAgain we see that the estimated and actual LTC real spend distributions follow each other closely. In a few places the estimated PMF is zero. This is an artifact of the Patra-Sen correction that requires the CDF to be valid. These zero-valued PMF points can be smoothed in the parametric distribution fitting step.\n\n\n\n\nFigure 3.3\n\n\n\n\n\n\n\nThe next chapter, Chapter 4, begins the process to estimate the real spend age distribution of the Monero mainnet blockchain.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Successful Simulation</span>"
    ]
  },
  {
    "objectID": "successful-simulation.html#is-the-bjr-estimator-necessary",
    "href": "successful-simulation.html#is-the-bjr-estimator-necessary",
    "title": "3  Successful Simulation",
    "section": "3.5 Is the BJR estimator necessary?",
    "text": "3.5 Is the BJR estimator necessary?\nThe BJR estimator is computationally expensive and consumed the majority of research and development time. Is it necessary? We can re-run the simulation, but skip the BJR step and find out.\nFigure 3.4 shows the results of skipping the BJR step. The estimate is quite inaccurate. It appears to bind against the non-negativity constraint of the Patra-Sen estimator. The KS statistic between this estimate and the real spend CDF is 0.2352.\n\n\n\n\nFigure 3.4\n\n\n\n\n\n\n\nFigure 3.5 shows the PMF of the estimate that skips the BJR estimator.\n\n\n\n\nFigure 3.5",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Successful Simulation</span>"
    ]
  },
  {
    "objectID": "successful-simulation.html#sec-successful-simulation-code",
    "href": "successful-simulation.html#sec-successful-simulation-code",
    "title": "3  Successful Simulation",
    "section": "3.6 Code",
    "text": "3.6 Code\n\nlibrary(decoyanalysis)\nlibrary(ggplot2)\nlibrary(dRacula)\n\nn.rings &lt;- 300000\n\ntheoretical.mixing.proportions &lt;- c(0.93, 0.05, 0.02)\n\nthreads &lt;- 4\n\n\nltc.ecdf.2022.05 &lt;- qs::qread(\"data/ltc-ecdf-2022-05.qs\")\nltc.ecdf.2022.10 &lt;- qs::qread(\"data/ltc-ecdf-2022-10.qs\")\nltc.ecdf.2022.15 &lt;- qs::qread(\"data/ltc-ecdf-2022-15.qs\")\n\nv &lt;-  1.4\nz &lt;- 117464545\n# v is the velocity of new outputs being produced on the Monero\n# blockchain, in number of seconds per one new output\n# z is the total number of outputs on the Monero blockchain\n\nGAMMA_SHAPE = 19.28\nGAMMA_RATE = 1.61\n\nG &lt;- function(x) {\n  actuar::plgamma(x, shapelog = GAMMA_SHAPE, ratelog = GAMMA_RATE)\n}\n\nG_star &lt;- function(x) {\n  (0 &lt;= x*v & x*v &lt;= 1800) *\n    (G(x*v + 1200) - G(1200) +\n        ( (x*v)/(1800) ) * G(1200)\n    )/G(z*v) +\n    (1800 &lt; x*v & x*v &lt;= z*v) * G(x*v + 1200)/G(z*v) +\n    (z*v &lt; x*v) * 1\n}\n\nG_star.inv &lt;- Vectorize(function(x) { gbutils::cdf2quantile(x, cdf = G_star) })\n\nltc.ecdf.2022.05.inv &lt;- Vectorize(function(x) { gbutils::cdf2quantile(x,\n  cdf = function(x) { ltc.ecdf.2022.05(x) }, lower = 0) })\n\nltc.ecdf.2022.10.inv &lt;- Vectorize(function(x) { gbutils::cdf2quantile(x,\n  cdf = function(x) { ltc.ecdf.2022.10(x) }, lower = 0) })\n\nltc.ecdf.2022.15.inv &lt;- Vectorize(function(x) { gbutils::cdf2quantile(x,\n  cdf = function(x) { ltc.ecdf.2022.15(x) }, lower = 0) })\n\n\ndecoy.cdfs &lt;- list(\n  function(x) { G_star(x) },\n  function(x) { triangle::pltriangle(x, a = 1, b = 60*60*24*365, c = 60*60*24*7, logbase = exp(1)) },\n  # a = 0 is not allowed\n  function(x) { punif(x, min = 1, max = 60*60*24*365) }\n)\n\nreal.spend.cdfs &lt;- list(\n  function(x) { ltc.ecdf.2022.10(x) },\n  function(x) { ltc.ecdf.2022.05(x) },\n  function(x) { ltc.ecdf.2022.15(x) }\n)\n\ndecoy.random.draw &lt;- list(\n  function(x) { G_star.inv(runif(x)) },\n  function(x) { triangle::rltriangle(x, a = 1, b = 60*60*24*365, c = 60*60*24*7, logbase = exp(1)) },\n  # a = 0 is not allowed\n  function(x) { runif(x, min = 1, max = 60*60*24*365) }\n)\n\nreal.spend.random.draw &lt;- list(\n  function(x) { ltc.ecdf.2022.10.inv(runif(x)) },\n  function(x) { ltc.ecdf.2022.05.inv(runif(x)) },\n  function(x) { ltc.ecdf.2022.15.inv(runif(x)) }\n)\n\n\n\nset.seed(314)\n\nwhich.component &lt;- sample(1:3, n.rings, replace = TRUE, prob = theoretical.mixing.proportions)\n\nwhich.component &lt;- table(which.component)\n\nring.generation.threads &lt;- 5\nfuture::plan(future::multisession(workers = ring.generation.threads))\n\nrings &lt;- lapply(1:3, FUN = function(x) {\n  cbind(\n    matrix(c(future.apply::future_replicate(15, decoy.random.draw[[x]](which.component[x]))), ncol = 15),\n    matrix(real.spend.random.draw[[x]](which.component[x]), ncol = 1)\n  )\n})\n\nrings &lt;- do.call(rbind, rings)\n\nrings.untransformed &lt;- rings\n\nrings[rings &lt;= 0] &lt;- rings[rings &lt;= 0] + 0.1\nrings &lt;- log(rings)\ncdf.points &lt;- quantile(c(rings), probs = c(0.001, (1:99)/100, 0.999))\nK &lt;- 4\nII &lt;- 10\ncluster.threads &lt;- NULL\noptions(future.globals.maxSize = 8000*1024^2)\nfuture::plan(future::multisession(workers = threads))\n\ny &lt;- rings\n\nprint(system.time(bjr.results &lt;- bjr(y, II = II, K = K, cdf.points = cdf.points,\n  estimate.mean.sd = FALSE, basis = \"Chebychev\", control = list(cluster.threads = cluster.threads ))))\n\n\nwallet2.dist.index &lt;- which.max(bjr.results$mixing.proportions)\nFn.hat.value &lt;- bjr.results$cdf$CDF[, wallet2.dist.index]\nsupp.points &lt;- bjr.results$cdf$cdf\nsupp.points &lt;- expm1(supp.points)\nFb &lt;- decoy.cdfs[[1]]\nM = 16\nalpha &lt;- 1/M\npatra.sen.bjr.results &lt;- patra.sen.bjr(Fn.hat.value, supp.points, Fb, alpha)\n\n\nx &lt;- bjr.results$cdf$cdf.points\nx &lt;- expm1(x)\n\n\ncat(\"Estimated mixing proportions of distribution components:\",\n  paste0(round(sort(bjr.results$mixing.proportions, decreasing = TRUE), 4), collapse = \", \"), \"\\n\")\n\ncat(\"Kolmogorov–Smirnov statistic (i.e. maximum distance between two CDFs) of estimated and actual LTC real spend:\",\n  round(max(abs(real.spend.cdfs[[1]](x) - patra.sen.bjr.results$Fs.hat)), 4), \"\\n\")\n\n\nplot.data.cdf &lt;- data.frame(\n  x = x, \n  y = c(real.spend.cdfs[[1]](x),\n  patra.sen.bjr.results$Fs.hat,\n  decoy.cdfs[[1]](x),\n  alpha * real.spend.cdfs[[2]](x) + (1-alpha) * decoy.cdfs[[2]](x),\n  alpha * real.spend.cdfs[[3]](x) + (1-alpha) * decoy.cdfs[[3]](x)\n  ),\n  label = as.factor(rep(1:5, each = length(x) ))\n)\n\n\n\n# NOTE: Must manualy put in the values of theoretical.mixing.proportions\n# because doing it automatically is complicated.\n\ndistribution.legend.text &lt;- c(\n  expression(paste(\"Empirical real spend of\\nLTC, ISO week 2022-10\")),\n  expression(paste(\"BJR + Patra-Sen est. of\\nwallet2-associated real spend\")),\n  expression(atop(phantom(0), atop(paste(\"Theoretical wallet2 decoy distribution, \",\n    omega == 0.93, \"\"), paste(\"Log-gamma, shape = 19.28, rate = 1.61\")))),\n  expression(atop(phantom(0), atop(paste('Theor. \"unknown\" decoy #1 + LTC_2022-05 real. ',\n    omega == 0.05, \"\"), paste(\"Log-triangular, min = 0, max = 1 year, mode = 1 week\")))),\n  expression(atop(phantom(0), atop(paste('Theor. \"unknown\" decoy #2 + LTC_2022-15 real.'),\n    paste(omega == 0.02, \", Uniform, min = 0, max = 1 year\"))))\n)\n\ndracula.palette &lt;- dracula_tibble$hex[seq_len(data.table::uniqueN(plot.data.cdf$label))]\nnames(dracula.palette) &lt;- unique(plot.data.cdf$label)\n\nif (! dir.exists(\"images\")) { dir.create(\"images\") }\n\npng(\"images/ltc-estimation-simulation-cdf.png\", width = 1000, height = 1000)\n\nggplot(plot.data.cdf,\n  aes(x = x, y = y, colour = label)) +\n  labs(title = \"Simulated estimation of LTC real spend CDF using BJR + Patra-Sen\",\n    y = \"Cumulative distribution function (CDF)\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1, 10, 60, 60*2, 60*30, 60^2, 60^2*12, 60^2*24, 60^2*24*7, 60^2*24*28, 60^2*24*365),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hour\", \"12 hour\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA),\n    plot.title = element_text(size = 20),\n    legend.text = element_text(size = 18),\n    axis.text.y = element_text(size = 15),\n    axis.text.x = element_text(size = 20),\n    axis.title.y = element_text(size = 20),\n    axis.title.x = element_text(size = 20)\n    ) + \n  scale_colour_manual(values = dracula.palette, name = '', \n    labels = distribution.legend.text, guide = guide_legend(nrow = 2, override.aes = list(linewidth = 5)))\n# The order of the themes() matter. Must have theme_dracula() first.\n\ndev.off()\n\n\nplot.data.pmf &lt;- data.frame(\n  x = x[-1], \n  y = c(\n    diff(real.spend.cdfs[[1]](x)) / diff(x),\n    diff(patra.sen.bjr.results$Fs.hat) / diff(x),\n    diff(decoy.cdfs[[1]](x)) / diff(x),\n    diff(alpha * real.spend.cdfs[[2]](x) + (1-alpha) * decoy.cdfs[[2]](x)) / diff(x),\n    diff(alpha * real.spend.cdfs[[3]](x) + (1-alpha) * decoy.cdfs[[3]](x)) / diff(x)\n  ),\n  label = as.factor(rep(1:5, each = length(x) - 1 ))\n)\n\n\n\npng(\"images/ltc-estimation-simulation-pmf.png\", width = 1000, height = 1000)\n\nggplot(plot.data.pmf,\n  aes(x = x, y = y, colour = label)) +\n  labs(title = \"Simulated estimation of LTC real spend PMF using BJR + Patra-Sen\",\n    y = \"Probability mass function (PMF) (Log scale)\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n # coord_cartesian(ylim = c(0.00000000001, 0.1)) +\n  scale_y_log10() + \n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1, 10, 60, 60*2, 60*30, 60^2, 60^2*12, 60^2*24, 60^2*24*7, 60^2*24*28, 60^2*24*365),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hour\", \"12 hour\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA),\n    plot.title = element_text(size = 20),\n    legend.text = element_text(size = 18),\n    axis.text.y = element_text(size = 15),\n    axis.text.x = element_text(size = 20),\n    axis.title.y = element_text(size = 20),\n    axis.title.x = element_text(size = 20)\n  ) + \n  scale_colour_manual(values = dracula.palette, name = '', \n    labels = distribution.legend.text, guide = guide_legend(nrow = 2, override.aes = list(linewidth = 5)))\n# The order of the themes() matter. Must have theme_dracula() first.\n\ndev.off()\n\n\n\n\n# Alternative LTC estimation with only Patra-Sen\n\n\nrings.ecdf &lt;- ecdf(c(rings.untransformed))\nsupp.points &lt;- knots(rings.ecdf)[floor(seq(1, length(knots(rings.ecdf)), length.out = 101))]\nFn.hat.value &lt;- rings.ecdf(supp.points)\n\nFb &lt;- decoy.cdfs[[1]]\nM = 16\nalpha &lt;- 1/M\npatra.sen.bjr.results &lt;- patra.sen.bjr(Fn.hat.value, supp.points, Fb, alpha)\n\nx &lt;- supp.points\n\ncat(\"Kolmogorov–Smirnov statistic (i.e. maximum distance between two CDFs) of estimated and actual LTC real spend:\",\n  round(max(abs(real.spend.cdfs[[1]](x) - patra.sen.bjr.results$Fs.hat)), 4), \"\\n\")\n\n\n\nplot.data.cdf &lt;- data.frame(\n  x = x, \n  y = c(real.spend.cdfs[[1]](x),\n    patra.sen.bjr.results$Fs.hat\n  ),\n  label = as.factor(rep(1:2, each = length(x) ))\n)\n\n\n# NOTE: Must manualy put in the values of theoretical.mixing.proportions\n# because doing it automatically is complicated.\n\ndistribution.legend.text &lt;- c(\n  expression(paste(\"Empirical real spend of LTC, ISO week 2022-10\")),\n  expression(paste(\"Patra-Sen est. of wallet2-associated real spend\")),\n  expression(atop(phantom(0), atop(paste(\"Theoretical wallet2 decoy distribution, \",\n    omega == 0.93, \"\"), paste(\"Log-gamma, shape = 19.28, rate = 1.61\")))),\n  expression(atop(phantom(0), atop(paste('Theor. \"unknown\" decoy #1 + LTC_2022-05 real. ',\n    omega == 0.05, \"\"), paste(\"Log-triangular, min = 0, max = 1 year, mode = 1 week\")))),\n  expression(atop(phantom(0), atop(paste('Theor. \"unknown\" decoy #2 + LTC_2022-15 real.'),\n    paste(omega == 0.02, \", Uniform, min = 0, max = 1 year\"))))\n)\n\ndracula.palette &lt;- dracula_tibble$hex[seq_len(data.table::uniqueN(plot.data.cdf$label))]\nnames(dracula.palette) &lt;- unique(plot.data.cdf$label)\n\nif (! dir.exists(\"images\")) { dir.create(\"images\") }\n\npng(\"images/ltc-estimation-simulation-cdf-only-patra-sen.png\", width = 1000, height = 1000)\n\nggplot(plot.data.cdf,\n  aes(x = x, y = y, colour = label)) +\n  labs(title = \"Simulated estimation of LTC real spend CDF using only Patra-Sen\",\n    y = \"Cumulative distribution function (CDF)\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1, 10, 60, 60*2, 60*30, 60^2, 60^2*12, 60^2*24, 60^2*24*7, 60^2*24*28, 60^2*24*365),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hour\", \"12 hour\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA),\n    plot.title = element_text(size = 20),\n    legend.text = element_text(size = 18),\n    axis.text.y = element_text(size = 15),\n    axis.text.x = element_text(size = 20),\n    axis.title.y = element_text(size = 20),\n    axis.title.x = element_text(size = 20)\n  ) + \n  scale_colour_manual(values = dracula.palette, name = '', \n    labels = distribution.legend.text, guide = guide_legend(nrow = 2, override.aes = list(linewidth = 5)))\n# The order of the themes() matter. Must have theme_dracula() first.\n\ndev.off()\n\n\nplot.data.pmf &lt;- data.frame(\n  x = x[-1], \n  y = c(\n    diff(real.spend.cdfs[[1]](x)) / diff(x),\n    diff(patra.sen.bjr.results$Fs.hat) / diff(x)\n  ),\n  label = as.factor(rep(1:2, each = length(x) - 1 ))\n)\n\n\n\npng(\"images/ltc-estimation-simulation-pmf-only-patra-sen.png\", width = 1000, height = 1000)\n\nggplot(plot.data.pmf,\n  aes(x = x, y = y, colour = label)) +\n  labs(title = \"Simulated estimation of LTC real spend PMF using only Patra-Sen\",\n    y = \"Probability mass function (PMF) (Log scale)\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n  # coord_cartesian(ylim = c(0.00000000001, 0.1)) +\n  scale_y_log10() + \n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1, 10, 60, 60*2, 60*30, 60^2, 60^2*12, 60^2*24, 60^2*24*7, 60^2*24*28, 60^2*24*365),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hour\", \"12 hour\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA),\n    plot.title = element_text(size = 20),\n    legend.text = element_text(size = 18),\n    axis.text.y = element_text(size = 15),\n    axis.text.x = element_text(size = 20),\n    axis.title.y = element_text(size = 20),\n    axis.title.x = element_text(size = 20)\n  ) + \n  scale_colour_manual(values = dracula.palette, name = '', \n    labels = distribution.legend.text, guide = guide_legend(nrow = 2, override.aes = list(linewidth = 5)))\n# The order of the themes() matter. Must have theme_dracula() first.\n\ndev.off()\n\n\n\n\n\nBonhomme, Stéphane, Koen Jochmans, and Jean-Marc Robin. 2016. “Non-Parametric Estimation of Finite Mixtures from Repeated Measurements.” Journal of the Royal Statistical Society. Series B (Statistical Methodology) 78 (1): 211–29. http://www.jstor.org/stable/24775334.\n\n\nNoether, Surae, Sarang Noether, and Adam Mackenzie. 2014. “A Note on Chain Reactions in Traceability in CryptoNote 2.0.” Research Bulletin. https://www.getmonero.org/resources/research-lab/pubs/MRL-0001.pdf.\n\n\nPatra, Rohit Kumar, and Bodhisattva Sen. 2016. “Estimation of a Two-Component Mixture Model with Applications to Multiple Testing.” Journal of the Royal Statistical Society. Series B (Statistical Methodology) 78 (4): 869–93. http://www.jstor.org/stable/24775367.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Successful Simulation</span>"
    ]
  },
  {
    "objectID": "ring-gathering.html",
    "href": "ring-gathering.html",
    "title": "4  Ring Gathering",
    "section": "",
    "text": "4.1 Code\nFirst, the data on ring member ages must be collected from the Monero blockchain. This is done by repeated RPC queries to a running instance of monerod, the Monero node software.\nSeveral parallel processes are spawned to query monerod. The number of processes is half of the CPU’s threads, minus one.\nThe user should specify two variables\nOpen an R session and run the following code. Do not close the R session after running it. On a powerful machine, it will take about 24 hours to finish.\nlibrary(data.table)\n\ncurrent.height &lt;- NA # 3263496\n# current.height should be the most recent height that you want to collect data for\n\nstopifnot(!is.na(current.height))\n\nblock.heights &lt;- 1220516:current.height\n# 1220516 is hard fork height that allowed the first RingCT transactions\n# https://github.com/monero-project/monero#scheduled-softwarenetwork-upgrades\n\nurl.rpc &lt;- \"http://127.0.0.1:18081\"\n# Set the IP address and port of your node. Should usually be \"http://127.0.0.1:18081\"\n# Data can be pulled from multiple monerod instances. In that case, the blockchains\n# of the multiple monerod instances should be on different storage devices because\n# storage I/O seems to be the bottleneck. Specify multiple instances as:\n# url.rpc &lt;- c(\"http://127.0.0.1:18081\", \"http://127.0.0.1:58081\")\n\n\n\n# Modified from TownforgeR::tf_rpc_curl function\nxmr.rpc &lt;- function(\n    url.rpc = \"http://127.0.0.1:18081/json_rpc\",\n  method = \"\",\n  params = list(),\n  userpwd = \"\",\n  num.as.string = FALSE,\n  nonce.as.string = FALSE,\n  keep.trying.rpc = FALSE,\n  curl = RCurl::getCurlHandle(),\n  ...\n){\n\n  json.ret &lt;- RJSONIO::toJSON(\n    list(\n      jsonrpc = \"2.0\",\n      id = \"0\",\n      method = method,\n      params = params\n    ), digits = 50\n  )\n\n  rcp.ret &lt;-    tryCatch(RCurl::postForm(url.rpc,\n    .opts = list(\n      userpwd = userpwd,\n      postfields = json.ret,\n      httpheader = c('Content-Type' = 'application/json', Accept = 'application/json')\n      # https://stackoverflow.com/questions/19267261/timeout-while-reading-csv-file-from-url-in-r\n    ),\n    curl = curl\n  ), error = function(e) {NULL})\n\n  if (keep.trying.rpc && length(rcp.ret) == 0) {\n    while (length(rcp.ret) == 0) {\n      rcp.ret &lt;-    tryCatch(RCurl::postForm(url.rpc,\n        .opts = list(\n          userpwd = userpwd,\n          postfields = json.ret,\n          httpheader = c('Content-Type' = 'application/json', Accept = 'application/json')\n          # https://stackoverflow.com/questions/19267261/timeout-while-reading-csv-file-from-url-in-r\n        ),\n        curl = curl\n      ), error = function(e) {NULL})\n    }\n  }\n\n  if (is.null(rcp.ret)) {\n    stop(\"Cannot connect to monerod. Is monerod running?\")\n  }\n\n  if (num.as.string) {\n    rcp.ret &lt;- gsub(\"(: )([-0123456789.]+)([,\\n\\r])\", \"\\\\1\\\"\\\\2\\\"\\\\3\", rcp.ret )\n  }\n\n  if (nonce.as.string & ! num.as.string) {\n    rcp.ret &lt;- gsub(\"(\\\"nonce\\\": )([-0123456789.]+)([,\\n\\r])\", \"\\\\1\\\"\\\\2\\\"\\\\3\", rcp.ret )\n  }\n\n  RJSONIO::fromJSON(rcp.ret, asText = TRUE) # , simplify = FALSE\n}\n\n\n\nsystem.time({\n\n  threads &lt;- max(2, min(floor(parallelly::availableCores()/2), 32L) - length(url.rpc))\n\n  future::plan(future::multisession(workers = threads))\n  options(future.globals.maxSize= 8000*1024^2)\n\n  set.seed(314)\n\n  # Randomize block heights to make processing time more uniform between parallel processes\n  block.heights &lt;- split(block.heights, sample(cut(block.heights, threads)))\n  # First randomly put heights into list elements (split() will sort them ascendingly in each list element)\n  block.heights &lt;- lapply(block.heights, sample)\n  # Then order the heights randomly within each list element\n  block.heights &lt;- unname(block.heights)\n\n  returned &lt;- future.apply::future_lapply(block.heights, function(block.heights) {\n\n    url.rpc &lt;- sample(url.rpc, 1)\n\n    handle &lt;- RCurl::getCurlHandle()\n\n    return.data &lt;- vector(\"list\", length(block.heights))\n\n    for (height.iter in seq_along(block.heights)) {\n\n      height &lt;- block.heights[height.iter]\n\n      block.data &lt;- xmr.rpc(url.rpc = paste0(url.rpc, \"/json_rpc\"),\n        method = \"get_block\",\n        params = list(height = height ),\n        keep.trying.rpc = TRUE,\n        curl = handle)$result\n\n      txs.to.collect &lt;- c(block.data$miner_tx_hash, block.data$tx_hashes)\n\n      rcp.ret &lt;-    tryCatch(RCurl::postForm(paste0(url.rpc, \"/get_transactions\"),\n        .opts = list(\n          postfields = paste0('{\"txs_hashes\":[\"', paste0(txs.to.collect, collapse = '\",\"'), '\"],\"decode_as_json\":true}'),\n          httpheader = c('Content-Type' = 'application/json', Accept = 'application/json')\n        ),\n        curl = handle\n      ), error = function(e) {NULL})\n\n      if (length(rcp.ret) == 0) {\n        while (length(rcp.ret) == 0) {\n          rcp.ret &lt;- tryCatch(RCurl::postForm(paste0(url.rpc, \"/get_transactions\"),\n            .opts = list(\n              postfields = paste0('{\"txs_hashes\":[\"', paste0(txs.to.collect, collapse = '\",\"'), '\"],\"decode_as_json\":true}'),\n              httpheader = c('Content-Type' = 'application/json', Accept = 'application/json')\n            ),\n            curl = handle\n          ), error = function(e) {NULL})\n        }\n      }\n\n      rcp.ret &lt;- RJSONIO::fromJSON(rcp.ret, asText = TRUE)\n\n      output.index.collected &lt;- vector(\"list\", length(txs.to.collect))\n      rings.collected &lt;- vector(\"list\", length(txs.to.collect) - 1)\n\n      for (i in seq_along(txs.to.collect)) {\n\n        tx.json &lt;- tryCatch(\n          RJSONIO::fromJSON(rcp.ret$txs[[i]]$as_json, asText = TRUE),\n          error = function(e) {NULL} )\n\n        if (is.null(tx.json)) {\n          # stop()\n          cat(paste0(\"tx: \", i, \" block: \", height, \"\\n\"), file = \"~/RingCT-problems.txt\", append = TRUE)\n          next\n        }\n\n        output.amounts &lt;- sapply(tx.json$vout, FUN = function(x) {x$amount})\n\n        tx_size_bytes &lt;- ifelse(i == 1,\n          nchar(rcp.ret$txs[[i]]$pruned_as_hex) / 2,\n          nchar(rcp.ret$txs[[i]]$as_hex) / 2)\n        # Coinbase has special structure\n        # Reference:\n        # https://libera.monerologs.net/monero-dev/20221231\n        # https://github.com/monero-project/monero/pull/8691\n        # https://github.com/monero-project/monero/issues/8311\n\n        calc.tx.weight.clawback &lt;- function(p, is.bpp) {\n          pow.of.two &lt;- 2^(1:4)\n          pow.of.two.index &lt;- findInterval(p, pow.of.two, left.open = TRUE) + 1\n\n          n_padded_outputs &lt;- pow.of.two[pow.of.two.index]\n\n          if (is.bpp) {\n            multiplier &lt;- 6\n          } else {\n            multiplier &lt;- 9\n          }\n\n          bp_base &lt;- (32 * (multiplier + 7 * 2)) / 2\n          nlr &lt;- ceiling(log2(64 * p))\n          bp_size &lt;- 32 * (multiplier + 2 * nlr)\n          transaction_clawback &lt;- (bp_base * n_padded_outputs - bp_size) * 4 / 5\n          floor(transaction_clawback) # With bpp, this is sometimes (always?) not an integer\n        }\n        # Equation from page 63 of Zero to Monero 2.0\n        # Updated with Bulletproofs+\n        # https://github.com/monero-project/monero/blame/c8214782fb2a769c57382a999eaf099691c836e7/src/cryptonote_basic/cryptonote_format_utils.cpp#L106\n\n        if (length(tx.json$vout) == 2 || i == 1) {\n          # i == 1 means the first tx, which is the coinbase tx\n          tx_weight_bytes &lt;- tx_size_bytes\n        } else {\n          tx_weight_bytes &lt;- tx_size_bytes +\n            calc.tx.weight.clawback(length(tx.json$vout), length(tx.json$rctsig_prunable$bpp) &gt; 0)\n        }\n\n        tx_fee &lt;- ifelse(i == 1 || is.null(tx.json$rct_signatures), NA, tx.json$rct_signatures$txnFee)\n        # missing non-RingCT tx fee\n\n        is.mordinal &lt;-\n          height &gt;= 2838965 &&\n          length(tx.json$vout) == 2 &&\n          i &gt; 1 && # not the first tx, which is the coinbase tx\n          length(tx.json$extra) &gt; 44 &&\n          tx.json$extra[45] == 16\n        # With \"&&\", evaluates each expression sequentially until it is false (if ever). Then stops.\n        # If all are TRUE, then returns true.\n\n        is.mordinal.transfer &lt;-\n          height &gt;= 2838965 &&\n          length(tx.json$vout) == 2 &&\n          i &gt; 1 && # not the first tx, which is the coinbase tx\n          length(tx.json$extra) &gt; 44 &&\n          tx.json$extra[45] == 17\n\n        output.index.collected[[i]] &lt;- data.table(\n          block_height = height,\n          block_timestamp = block.data$block_header$timestamp,\n          tx_num = i,\n          tx_hash = txs.to.collect[i],\n          tx_version = tx.json$version,\n          tx_fee = tx_fee,\n          tx_size_bytes = tx_size_bytes,\n          tx_weight_bytes = tx_weight_bytes,\n          number_of_inputs = length(tx.json$vin),\n          number_of_outputs = length(tx.json$vout),\n          output_num = seq_along(rcp.ret$txs[[i]]$output_indices),\n          output_index = rcp.ret$txs[[i]]$output_indices,\n          output_amount = output.amounts,\n          output_unlock_time = tx.json$unlock_time,\n          is_mordinal = is.mordinal,\n          is_mordinal_transfer = is.mordinal.transfer)\n\n        if (i == 1L) { next }\n        # Skip first tx since it is the coinbase and has no inputs\n\n        tx_hash &lt;- txs.to.collect[i]\n\n        rings &lt;- vector(\"list\", length(tx.json$vin))\n\n        for (j in seq_along(tx.json$vin)) {\n          rings[[j]] &lt;- data.table(\n            tx_hash = tx_hash,\n            input_num = j,\n            input_amount = tx.json$vin[[j]]$key$amount,\n            key_offset_num = seq_along(tx.json$vin[[j]]$key$key_offsets),\n            key_offsets = tx.json$vin[[j]]$key$key_offsets\n          )\n        }\n\n        rings.collected[[i-1]] &lt;- rbindlist(rings)\n\n      }\n\n      output.index.collected &lt;- data.table::rbindlist(output.index.collected)\n      rings.collected &lt;- rbindlist(rings.collected)\n\n      return.data[[height.iter]] &lt;- list(\n        output.index.collected = output.index.collected,\n        rings.collected = rings.collected)\n\n    }\n\n    return.data\n\n  }, future.seed = TRUE)\n})\n\n\nfuture::plan(future::sequential)\n# Shuts down R threads to free RAM\n\nreturned.temp &lt;- vector(\"list\", length(returned))\n\nfor (i in seq_along(returned)) {\n  returned.temp[[i]] &lt;- list(\n    output.index.collected = rbindlist(lapply(returned[[i]],\n      FUN = function(y) { y$output.index.collected })),\n    rings.collected = rbindlist(lapply(returned[[i]],\n      FUN = function(y) { y$rings.collected }))\n  )\n}\n\nreturned.temp &lt;- list(\n  output.index.collected = rbindlist(lapply(returned.temp,\n    FUN = function(y) { y$output.index.collected })),\n  rings.collected = rbindlist(lapply(returned.temp,\n    FUN = function(y) { y$rings.collected }))\n)\n\noutput.index &lt;- returned.temp$output.index.collected\nreturned.temp$output.index.collected &lt;- NULL\nxmr.rings &lt;- returned.temp$rings.collected\nrm(returned.temp)\n\nsetorder(xmr.rings, tx_hash, input_num, key_offset_num)\n\nxmr.rings[, output_index := cumsum(key_offsets), by = c(\"tx_hash\", \"input_num\")]\n\nxmr.rings &lt;- merge(xmr.rings, unique(output.index[, .(tx_hash, block_height,\n  block_timestamp, tx_fee, tx_size_bytes, tx_weight_bytes, is_mordinal, is_mordinal_transfer)]), by = \"tx_hash\")\n\nring.col.names &lt;- c(\"block_height\", \"block_timestamp\", \"tx_fee\", \"tx_size_bytes\",\n  \"tx_weight_bytes\", \"is_mordinal\", \"is_mordinal_transfer\")\n\nsetnames(xmr.rings, ring.col.names, paste0(ring.col.names, \"_ring\"))\n\noutput.index[, output_amount_for_index := ifelse(tx_num == 1, 0, output_amount)]\n\noutput.index &lt;- output.index[ !(tx_num == 1 & tx_version == 1), ]\n# Remove coinbase outputs that are ineligible for use in a RingCT ring\n# See https://libera.monerologs.net/monero-dev/20230323#c224570\n\nv16.fork.height &lt;- 2689608 # 2022-08-14\nxmr.rings &lt;- xmr.rings[block_height_ring &gt;= v16.fork.height, ]\n# Remove data from before August 2022 hard fork\n\n\nxmr.rings &lt;- merge(xmr.rings, output.index[, .(block_height, block_timestamp, tx_num, output_num,\n  output_index, output_amount, output_amount_for_index, output_unlock_time,\n  is_mordinal, is_mordinal_transfer, tx_fee, tx_size_bytes)],\n  # only dont need tx_hash column from output.index\n  by.x = c(\"input_amount\", \"output_index\"),\n  by.y = c(\"output_amount_for_index\", \"output_index\")) #, all = TRUE)\n\n\nxmr.rings &lt;- xmr.rings[input_amount == 0, ]\n# Remove non-RingCT rings\n\nsetorder(output.index, block_height, tx_num, output_num)\n\n\nxmr.rings.isoweek &lt;- unique(xmr.rings[, .(block_timestamp_ring = block_timestamp_ring)])\n\nxmr.rings.isoweek[, block_timestamp_ring_isoweek := paste0(lubridate::isoyear(as.POSIXct(block_timestamp_ring, origin = \"1970-01-01\", tz = \"UTC\")), \"-\",\n  formatC(lubridate::isoweek(as.POSIXct(block_timestamp_ring, origin = \"1970-01-01\", tz = \"UTC\")), width = 2, flag = \"0\"))]\n\nxmr.rings &lt;- merge(xmr.rings, xmr.rings.isoweek, by = \"block_timestamp_ring\")\n# speed improvement by splitting and then merging\n\niso.weeks &lt;- xmr.rings[, unique(block_timestamp_ring_isoweek)]\n\niso.weeks &lt;- iso.weeks[as.numeric(gsub(\"-\", \"\", iso.weeks, fixed = TRUE)) &gt;= 202233]\n# week after hard fork",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ring Gathering</span>"
    ]
  },
  {
    "objectID": "nonstandard-transactions.html",
    "href": "nonstandard-transactions.html",
    "title": "5  Nonstandard Transactions",
    "section": "",
    "text": "5.1 Code\nUsing the R session opened for the previous chapter, Chapter 4, run the code below.\nTransactions that were probably created with nonstandard software are identified. The “Rucknium” identification criteria is based on nonstandard fees and nonzero output unlock time. Information about nonstandard fees is here. Code for the “isthmus” criteria is here.\nThe transactions identified as nonstandard will be not be included in the ring member age analysis. The exclusion criteria removes low-hanging fruit. The Bonhomme-Jochmans-Robin estimator is intended to remove everything else.\nv16.fork.height &lt;- 2689608 # 2022-08-14\n\nfees &lt;- unique(output.index[block_height &gt;= v16.fork.height,\n  .(tx_hash, block_height, tx_fee, tx_weight_bytes, number_of_outputs)])\n\nfees[, fee_per_byte_nanoneros := floor((tx_fee/tx_weight_bytes)/1000)]\n\ntx.hash.nonstandard.fees &lt;- fees[ !(\n    fee_per_byte_nanoneros %between% c(18, 22) |\n    fee_per_byte_nanoneros %between% c(78, 82) |\n    fee_per_byte_nanoneros %between% c(315, 325) |\n    fee_per_byte_nanoneros %between% c(3900, 4100)\n  ), tx_hash]\n# Standard fees\n\nrm(fees)\n\ntx.hash.nonstandard.unlock.time &lt;- unique(output.index[output_unlock_time &gt; 0 & tx_num != 1, tx_hash])\n\nisthmus.anomalous.hashes &lt;- fread(\"anomalous_hashes.txt\", stringsAsFactors = FALSE)$tx_hash",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonstandard Transactions</span>"
    ]
  },
  {
    "objectID": "broadcast-time.html",
    "href": "broadcast-time.html",
    "title": "6  Broadcast Time",
    "section": "",
    "text": "6.1 Code\nA transaction’s distribution of decoys is based on the time of transaction construction, not on the time that it is confirmed on the blockchain. Since most transactions are confirmed shortly after being broadcast, blockchain confirmation time is nonetheless a good approximation of transaction construction time. However, we can improve over this approximation by using ephemeral data about when broadcasted transactions arrived in nodes’ txpools, which has been collected since late December 2022.\nThe transaction construction times are computed as follows:\nThe largest discrepancy between the time of construction and time of confirmation occurred during the March 2024 suspected spam attack (Rucknium 2024).\nThis code should be run in the same R session as previous chapters.\nthreads &lt;- 16\nfuture::plan(future::multisession(workers = threads))\n\n\nblocks &lt;- rbind(\nfread(\"xmr-block-archive-2023-01-18-20-48-07.csv\"),\nfread(\"xmr-block-archive-2023-03-09-12-20-37.csv\"),\nfread(\"xmr-block-archive-2024-01-14-07-00-47.csv\"),\nfread(\"xmr-block-archive-2024-03-11-20-32-01.csv\"),\nfread(\"xmr-block-archive-2023-12-27-20-14-43.csv\"),\nfread(\"xmr-block-archive-exporttime-2024-10-20-21-39-50.csv\"),\nfread(\"xmr-block-archive-exporttime-2024-10-21-17-06-02.csv\")\n)\n\n\n\nblocks &lt;- blocks[blocks$block_height != 0, ]\nsetorder(blocks, block_height, block_receive_time)\nblocks &lt;- unique(blocks, by = \"block_height\")\n\n\nmempool &lt;- rbind(\nfread(\"xmr-mempool-archive-2023-01-18-20-48-07.csv\"),\nfread(\"xmr-mempool-archive-2023-03-09-12-20-37.csv\"),\nfread(\"xmr-mempool-archive-2024-01-14-07-00-47.csv\"),\nfread(\"xmr-mempool-archive-2024-03-11-20-32-01.csv\"),\nfread(\"xmr-mempool-archive-2023-12-27-20-14-43.csv\"),\nfread(\"xmr-txpool-archive-exporttime-2024-10-20-21-39-50.csv\"),\nfread(\"xmr-txpool-archive-exporttime-2024-10-21-17-06-02.csv\")\n)\n\n\nsetorder(mempool, receive_time)\nmempool &lt;- unique(mempool, by = \"id_hash\")\n\nsetnames(blocks, \"block_receive_time\", \"canon.block_receive_time\")\nsetnames(mempool, \"receive_time\", \"canon.receive_time\")\n\n# Some of this script taken from https://github.com/Rucknium/misc-research/blob/main/Monero-TX-Confirm-Delay/xmr-data-prep.R\n\n\nblock_height.unique &lt;- blocks[, na.omit(unique(block_height))]\n\nall.blocks &lt;- min(block_height.unique[block_height.unique &gt; 0]):max(block_height.unique)\n# min():max() since some blocks are \"skipped\"\n# Need to have positive since rarely block height is corrupted  in RPC response\n# to \"0\"\n\n\n\nblockchain.data &lt;- future.apply::future_lapply(seq_along(all.blocks), function(i) {\n\n  block.data &lt;- xmr.rpc(url.rpc = \"http://127.0.0.1:58081/json_rpc\", method = \"get_block\",\n    params = list(height = all.blocks[i]))$result\n\n  if (length(block.data$tx_hashes) &gt; 0) {\n    y &lt;- data.table::data.table(\n      block_height = all.blocks[i],\n      id_hash = block.data$tx_hashes,\n      block_num_txes = block.data$block_header$num_txes,\n      block_reward = block.data$block_header$reward\n    )\n  } else {\n    y &lt;- data.table::data.table(\n      block_height = all.blocks[i],\n      id_hash = \"&lt;NO_TXS_IN_BLOCK&gt;\",\n      block_num_txes = block.data$block_header$num_txes,\n      block_reward = block.data$block_header$reward\n    )\n  }\n  return(y)\n})\n\n\nblockchain.data &lt;- data.table::rbindlist(blockchain.data)\n\nblocks.filled &lt;- merge(data.table(block_height = all.blocks),\n  blocks[, c(\"block_height\", \"canon.block_receive_time\")], all = TRUE)\n\nrm(blocks)\n\nblocks.filled$canon.block_receive_time &lt;- zoo::na.locf(blocks.filled$canon.block_receive_time, fromLast = TRUE)\n\nblockchain.data &lt;- merge(blocks.filled, blockchain.data)\n\nblockchain.data &lt;- merge(blockchain.data, mempool, by = \"id_hash\", all = TRUE)\n\nrm(mempool)\n\n\n\nmempool.archive.blocks &lt;- unique(blockchain.data[, .(block_height, canon.block_receive_time)])\nmempool.archive.blocks &lt;- mempool.archive.blocks[complete.cases(mempool.archive.blocks), ]\n# As of now, this only eliminates a single row that has block_height = NA, canon.block_receive_time = NA\nmempool.archive.blocks &lt;- rbind(mempool.archive.blocks,\n  data.table(block_height = min(mempool.archive.blocks$block_height), canon.block_receive_time = 0))\n# Fill in missing data from beginning of dataset when we findInterval() below\nsetorder(mempool.archive.blocks, canon.block_receive_time)\n\nblockchain.data[, block_height.at.construction :=\n  mempool.archive.blocks$block_height[findInterval(canon.receive_time, mempool.archive.blocks$canon.block_receive_time)] ]\n\nblockchain.data[block_height - block_height.at.construction &lt;= 0, block_height.at.construction := block_height - 1]\n\nsetnames(blockchain.data, \"block_height.at.construction\", \"block_height_ring.at.construction\")\n\nblockchain.data &lt;- blockchain.data[ (! duplicated(id_hash)) | id_hash == \"&lt;NO_TXS_IN_BLOCK&gt;\", ]\n\nxmr.rings &lt;- merge(xmr.rings, blockchain.data[, .(id_hash, block_height_ring.at.construction)], by.x = \"tx_hash\", by.y = \"id_hash\", all.x = TRUE)\n\nrm(blockchain.data)\n\nxmr.rings[substr(block_timestamp_ring_isoweek, 1, 4) == 2022, block_height_ring.at.construction := block_height_ring - 2]\n# Assume all txs had a delay of one block, before the mining pool config fix in January 2023.\n\n\nweeks.missing.mempool.data &lt;- c(\"2023-13\", \"2023-14\")\n\nxmr.rings[ is.na(block_height_ring.at.construction) & ! block_timestamp_ring_isoweek %chin% weeks.missing.mempool.data,\n  block_height_ring.at.construction := block_height_ring - 1]\n# The rest, if the tx \"skipped\" the mempool, assume that the tx was constructed right before the block it was\n# confirmed in. Exclude 2023-13 and 2023-14 weeks since we are missing mempool data for those weeks.\n\n\nstopifnot(all(complete.cases(\n  xmr.rings[! block_timestamp_ring_isoweek %chin% weeks.missing.mempool.data,\n   .(block_height_ring.at.construction, youngest.output.index, ring_member_age)]))\n)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Broadcast Time</span>"
    ]
  },
  {
    "objectID": "broadcast-time.html#code",
    "href": "broadcast-time.html#code",
    "title": "6  Broadcast Time",
    "section": "",
    "text": "Rucknium. 2023. “Centralized Mining Pools Are Delaying Monero Transaction Confirmations by 60 Seconds.” https://rucknium.me/posts/monero-pool-transaction-delay/.\n\n\n———. 2024. “March 2024 Suspected Black Marble Flooding Against Monero: Privacy, User Experience, and Countermeasures.” https://github.com/Rucknium/misc-research/blob/main/Monero-Black-Marble-Flood/pdf/monero-black-marble-flood.pdf.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Broadcast Time</span>"
    ]
  },
  {
    "objectID": "miscellaneous-preparation.html",
    "href": "miscellaneous-preparation.html",
    "title": "7  Miscellaneous Preparation",
    "section": "",
    "text": "7.1 Code\nThis script does three things:\nAgain, input this code in the same R session as used in previous chapters.\nxmr.rings[, is.nonstandard.rucknium :=\n  (tx_hash %chin% tx.hash.nonstandard.fees) |\n  (tx_hash %chin% tx.hash.nonstandard.unlock.time) |\n  is_mordinal_ring | is_mordinal_transfer_ring]\n\nxmr.rings[, is.nonstandard.isthmus := tx_hash %chin% isthmus.anomalous.hashes]\n\nrm(tx.hash.nonstandard.fees, tx.hash.nonstandard.unlock.time, isthmus.anomalous.hashes)\n\n\nyoungest.RingCT &lt;- output.index[ output_amount == 0 | tx_num == 1,\n  .(youngest.output.index = max(output_index)), by = \"block_height\"]\n\n# rm(output.index)\n# TODO: uncomment this\n\nyoungest.RingCT &lt;- merge(youngest.RingCT,\n  data.table(block_height = min(youngest.RingCT$block_height):max(youngest.RingCT$block_height)),\n  all = TRUE)\n\nyoungest.RingCT[, youngest.output.index :=\n  zoo::na.locf(youngest.output.index, na.rm = FALSE)]\n# Before RingCT outputs were mandatory, some blocks had zero RingCT-eligible outputs.\n# These lines make sure that these blocks are included in the dataset\n\nyoungest.RingCT[, block_height := block_height + 9L]\n# Used to be 10L before block_height_ring changed to block_height_ring.at.construction below\n\nxmr.rings &lt;- merge(xmr.rings, youngest.RingCT, all.x = TRUE, by.x = \"block_height_ring.at.construction\", by.y = \"block_height\")\n# Need all.x = TRUE because will have NAs for the first 10 blocks of youngest.output.index\n\n\n\nxmr.rings[, ring_member_age := youngest.output.index - output_index]\n# xmr.rings[, table(ring_member_age &lt; 0) ]\n# &gt; 1798220 /357314244\n# [1] 0.005032601\n\nxmr.rings[, ring_member_age := ring_member_age + 1]\n# Add 1 so youngest ring member is 1, not zero\n\n\nfetus.rings &lt;- xmr.rings[ring_member_age &lt;= 0, ]\n\nfetus.rings &lt;- merge(unique(fetus.rings[, .(tx_hash, input_num)]), xmr.rings, by = c(\"tx_hash\", \"input_num\"))\nsetcolorder(fetus.rings, colnames(xmr.rings))\nnrow(xmr.rings)\nxmr.rings &lt;- fsetdiff(xmr.rings, fetus.rings)\nnrow(xmr.rings)\n# This is anti-join\n\nn.fetus.rings &lt;- n.fetus.rings.initial &lt;- nrow(fetus.rings)\n\nborn.rings &lt;- fetus.rings[FALSE, ]\n\nyoungest.RingCT.rolling &lt;- copy(youngest.RingCT)\n\nfetus.rings[, ring_member_age := NULL]\nfetus.rings[, youngest.output.index := NULL]\n\ni &lt;- 0\nwhile (n.fetus.rings &gt; 0) {\n  i &lt;- i + 1\n  youngest.RingCT.rolling[, block_height := block_height - 1L]\n  birthing.rings &lt;- merge(fetus.rings, youngest.RingCT.rolling, all.x = TRUE, by.x = \"block_height_ring.at.construction\", by.y = \"block_height\")\n  birthing.rings[, ring_member_age := youngest.output.index - output_index]\n  birthing.rings[, any.fetus := any(ring_member_age &lt;= 0), by = c(\"tx_hash\", \"input_num\")]\n\n  if (all(birthing.rings$any.fetus)) { next }\n\n  fetus.rings &lt;- birthing.rings[(any.fetus), ]\n  fetus.rings[, ring_member_age := NULL]\n  fetus.rings[, youngest.output.index := NULL]\n  any.fetus.external &lt;- birthing.rings[, any.fetus]\n  birthing.rings[, any.fetus := NULL]\n\n  born.rings &lt;- rbind(born.rings, birthing.rings[(! any.fetus.external), ])\n\n  n.fetus.rings &lt;- nrow(fetus.rings)\n\n  if (i &gt;= 1000) { break }\n  # Don't go further than 1000 blocks. The few rings\n  # that cannot be repaired after 1000 blocks will be excluded from the analysis\n\n}\n# The vast majority are off-by-one-block\n\nstopifnot(nrow(born.rings) + n.fetus.rings == n.fetus.rings.initial)\n\nxmr.rings &lt;- rbind(xmr.rings, born.rings)\n\n\n\noutput.index.only.locked &lt;- output.index[\n  output_unlock_time != 0 &\n  (output_amount == 0 | tx_num == 1),\n  .(output_index, output_unlock_time)]\n\n\noutput.index.only.locked.timestamp &lt;- output.index.only.locked[output_unlock_time &gt; 500000000, ]\n# Only about 40 of these have timestamp interpretations instead of block height interpretations\n# https://thecharlatan.ch/Monero-Unlock-Time-Vulns/\n\nblock.timestamps &lt;- unique(output.index[, .(block_height, block_timestamp)])\n\nsetorder(block.timestamps, block_timestamp)\n\noutput.index.only.locked.timestamp[, output_unlock_time := {\n  interval.index &lt;- findInterval(output_unlock_time, block.timestamps$block_timestamp)\n  ifelse(interval.index == 0, 0, 1 + block.timestamps$block_height[interval.index])\n  # If the time is before the earliest block in the dataset, then findInterval() will return a zero index (invalid)\n  # So set it to the genesis block\n  # Technically the genesis block should be unix time zero, but the data gatherer only gets RingCT\n  # txs, which start well after the genesis block.\n  }\n]\n\noutput.index.only.locked.timestamp[output_unlock_time == current.height + 1, output_unlock_time := .Machine$integer.max]\n\noutput.index.only.locked &lt;- rbind(\n  output.index.only.locked[output_unlock_time &lt;= 500000000, ], output.index.only.locked.timestamp)\n\noutput.index.only.locked[, output_unlock_time := as.integer(output_unlock_time)]\noutput.index.only.locked[, output_index := as.integer(output_index)]\n\nsetDF(output.index.only.locked)\n\n# rm(youngest.RingCT)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Miscellaneous Preparation</span>"
    ]
  },
  {
    "objectID": "transformed-age.html",
    "href": "transformed-age.html",
    "title": "8  Transformed Age",
    "section": "",
    "text": "8.1 Code\nWhen a new block is mined, the distribution of the decoy selection changes a small amount due to several factors:\nOver the course of a week of data, these factors can produce distributions with differences important enough to matter.\nA sample of a random variable that is transformed by the random variable’s cumulative distribution function (CDF) will be distributed as a Uniform(0,1) variable. I use this fact to standardize the different decoy selection distributions.\nLet G(st)G(s_{t}) be the CDF of the decoy selection distribution. Let x1,x2,...,xnx_{1},x_{2},...,x_{n} be the set of ring member ages of transactions that were constructed at a specific block height. Then, for each ii , G(xi)=xi′G(x_{i})=x_{i}^{'} is the transformed value of xix_{i}.\nThere is another complication. In 2023, an off-by-one-block bug was discovered in the wallet2 reference wallet implementation. The bug was patched in version 0.18.2.2 of wallet2, released on April 4, 2023, but it was not a required update. Therefore, users who did not update to the latest software version would be using the old implementation with the off-by-one bug. The simultaneous use of two distributions creates a mixture distribution. In the code below, this problem is handled by computing the old and new versions of the decoy selection distribution and giving them weights. Estimating the share of users who were using the old and new software version was not feasible, so the mixing proportions were guessed. It was assumed that adoption followed a straight linear trend, starting at zero percent in April 2023 and ending at 75 percent in October 2024, the end of the dataset.\nThe code in this chapter takes about two weeks to finish on a powerful machine. For each of the half-a-million blocks in the dataset, we must compute the probability mass for each of Monero’s 100 million on-chain RingCT outputs (50 trillion probability masses in total).\nThe code should be run in the same R session as before. This is the last script to be run in this session since it will save the relevant data to storage, to be loaded into a new session in the next chapter. Some of the code is based on a python implementation of Monero’s decoy selection algorithm written by jeffro256.\nThe user can adjust these variables at the beginning of the script:\nthreads &lt;- 16\nfuture::plan(future::multisession(workers = threads))\n\nurl.rpc &lt;- \"http://127.0.0.1:18081\"\n\ncdf.dir &lt;- \"weekly-weighted-cdf\"\nring.member.ages.dir &lt;- \"weekly-ring-member-ages\"\naggregate.cdf.dir &lt;- \"aggregate-weighted-cdf\"\n\n\ndir.create(cdf.dir)\ndir.create(ring.member.ages.dir)\ndir.create(aggregate.cdf.dir)\n\n\n\ncalculate_num_usable_rct_outputs &lt;- function(crod) {\n\n  CRYPTONOTE_DEFAULT_TX_SPENDABLE_AGE = 10\n\n  # 1\n  num_usable_crod_blocks = length(crod) - (CRYPTONOTE_DEFAULT_TX_SPENDABLE_AGE - 1)\n\n  # 2\n  num_usable_rct_outputs = crod[num_usable_crod_blocks] # R indexes from 1\n\n  return(num_usable_rct_outputs)\n}\n\n\n\n\ndsa.to.uniform &lt;- function(x, block.height, crod, start_height.RingCT, output.index.only.locked, OBOE.prop) {\n# dsa.to.uniform &lt;- function(x, block.height, crod, output.index.only.locked) {\n\n  CRYPTONOTE_DEFAULT_TX_SPENDABLE_AGE = 10\n  DIFFICULTY_TARGET_V2 = 120\n\n  SECONDS_IN_A_YEAR =  60 * 60 * 24 * 365\n  BLOCKS_IN_A_YEAR = SECONDS_IN_A_YEAR / DIFFICULTY_TARGET_V2\n\n  calculate_average_output_flow &lt;- function(crod) {\n    # 1\n    num_blocks_to_consider_for_flow = min(c(length(crod), BLOCKS_IN_A_YEAR))\n\n    # 2\n    if (length(crod) &gt; num_blocks_to_consider_for_flow) {\n      num_outputs_to_consider_for_flow = crod[length(crod)] - crod[ length(crod) - num_blocks_to_consider_for_flow ]\n      # R indexes from 1\n    } else {\n      num_outputs_to_consider_for_flow = crod[length(crod)] # R indexes from 1\n    }\n\n    # 3\n    average_output_flow = DIFFICULTY_TARGET_V2 * num_blocks_to_consider_for_flow / num_outputs_to_consider_for_flow\n\n    return(average_output_flow)\n  }\n\n  calculate_num_usable_rct_outputs &lt;- function(crod) {\n    # 1\n    num_usable_crod_blocks = length(crod) - (CRYPTONOTE_DEFAULT_TX_SPENDABLE_AGE - 1)\n\n    # 2\n    num_usable_rct_outputs = crod[num_usable_crod_blocks] # R indexes from 1\n\n    return(num_usable_rct_outputs)\n  }\n\n\n\n  # crod &lt;- crod[1:(block.height - start_height.RingCT + 1)]\n  crod.at.tx.construction &lt;- crod[1:(block.height - start_height.RingCT)]\n\n  average_output_flow &lt;- calculate_average_output_flow(crod.at.tx.construction)\n  num_usable_rct_outputs &lt;- calculate_num_usable_rct_outputs(crod.at.tx.construction)\n\n  v &lt;- average_output_flow\n  z &lt;- num_usable_rct_outputs\n\n\n  GAMMA_SHAPE = 19.28\n  GAMMA_RATE = 1.61\n\n  CRYPTONOTE_DEFAULT_TX_SPENDABLE_AGE = 10\n  DIFFICULTY_TARGET_V2 = 120\n  DEFAULT_UNLOCK_TIME = CRYPTONOTE_DEFAULT_TX_SPENDABLE_AGE * DIFFICULTY_TARGET_V2\n  RECENT_SPEND_WINDOW = 15 * DIFFICULTY_TARGET_V2\n\n  G &lt;- function(x) {\n    actuar::plgamma(x, shapelog = GAMMA_SHAPE, ratelog = GAMMA_RATE)\n  }\n\n  G_star &lt;- function(x) {\n    (0 &lt;= x*v & x*v &lt;= 1800) *\n      (G(x*v + 1200) - G(1200) +\n          ( (x*v)/(1800) ) * G(1200)\n      )/G(z*v) +\n      (1800 &lt; x*v & x*v &lt;= z*v) * G(x*v + 1200)/G(z*v) +\n      (z*v &lt; x*v) * 1\n  }\n\n\n  crod.reversed &lt;- abs(diff(rev(c(0, crod.at.tx.construction))))[-(1:(CRYPTONOTE_DEFAULT_TX_SPENDABLE_AGE-1))]\n  # Remove first 10 blocks before cumsum() since can't spend from those outputs\n\n  unspendable.indices &lt;- output.index.only.locked[\n    (output.index.only.locked$output_index + 1L) &lt;= as.integer(num_usable_rct_outputs) &\n      output.index.only.locked$output_unlock_time &gt;= as.integer(block.height),\n    \"output_index\"]\n\n  stopifnot(all(unspendable.indices &lt; num_usable_rct_outputs))\n  unspendable.indices &lt;- abs(unspendable.indices - num_usable_rct_outputs) + 1\n\n\n  crod.reversed.for.OBOE &lt;- crod.reversed\n\n  #if (OBOE.prop != 1) {\n  if (TRUE) {\n\n  crod.reversed &lt;- cumsum(crod.reversed[crod.reversed &gt; 0])\n\n  crod.reversed &lt;- c(0, crod.reversed)\n  # Concatenate zero since the CDF should start at 0\n\n  y_0 &lt;- crod.reversed[-length(crod.reversed)] + 1\n  y_1 &lt;- crod.reversed[-1]\n  dsa.pmf &lt;- (G_star(y_1 + 1) - G_star(y_0)) / (y_1 + 1 - y_0)\n\n  dsa.pmf &lt;- rep(dsa.pmf, times = diff(crod.reversed))\n  dsa.pmf[length(dsa.pmf)] &lt;- dsa.pmf[length(dsa.pmf) - 1]\n  # Replace oldest output's mass by the penultimate one since\n  # it is calculated as a negative mass otherwise\n\n\n  # output.index.only.locked &lt;- copy(output.index.only.locked)\n\n\n\n  # sum.of.unspendable.mass &lt;- sum(dsa.pmf[unspendable.indices])\n\n\n  dsa.pmf[unspendable.indices] &lt;- 0\n\n#  dsa.pmf &lt;- dsa.pmf[ support.endpoints[1]:support.endpoints[2] ]\n\n  dsa.pmf.correct &lt;- dsa.pmf\n  # dsa.pmf.correct &lt;- dsa.pmf/sum(dsa.pmf)\n  # dsa.pmf &lt;- dsa.pmf * 1/(1 - sum.of.unspendable.mass)\n  #  dsa.cdf.correct &lt;- cumsum(dsa.pmf)\n  }\n\n\n  crod.reversed &lt;- crod.reversed.for.OBOE\n\n  first.post.unlock.block &lt;- sum(crod.reversed[1])\n  # NEW ^\n  crod.reversed &lt;- crod.reversed[-(1)]\n  # NEW ^\n\n  crod.reversed &lt;- cumsum(crod.reversed[crod.reversed &gt; 0])\n\n  crod.reversed &lt;- c(0, crod.reversed)\n  # Concatenate zero since the CDF should start at 0\n\n  y_0 &lt;- crod.reversed[-length(crod.reversed)] + 1\n  y_1 &lt;- crod.reversed[-1]\n  dsa.pmf &lt;- (G_star(y_1 + 1) - G_star(y_0)) / (y_1 + 1 - y_0)\n\n  dsa.pmf &lt;- rep(dsa.pmf, times = diff(crod.reversed))\n  dsa.pmf[length(dsa.pmf)] &lt;- dsa.pmf[length(dsa.pmf) - 1]\n\n  dsa.pmf &lt;- c(rep(0, first.post.unlock.block), dsa.pmf)\n\n  dsa.pmf[unspendable.indices] &lt;- 0\n\n#  dsa.pmf &lt;- dsa.pmf[ support.endpoints[1]:support.endpoints[2] ]\n\n  #if (OBOE.prop != 1) {\n  if (TRUE) {\n\n  dsa.pmf.OBOE &lt;- dsa.pmf\n\n  # gc()\n\n  dsa.pmf &lt;- OBOE.prop * dsa.pmf.OBOE + (1 - OBOE.prop) * dsa.pmf.correct\n  rm(dsa.pmf.OBOE, dsa.pmf.correct)\n  }\n\n  dsa.pmf &lt;- dsa.pmf / sum(dsa.pmf)\n\n  dsa.cdf &lt;- cumsum(dsa.pmf)\n\n  return(list(transformed.data = dsa.cdf[x], pmf.contribution = dsa.pmf, v = v, z = z))\n\n}\n\n\n\n\n# closeAllConnections()\n\n\n# https://github.com/monero-project/monero/releases/tag/v0.18.2.2\n# iso week 15 starts on April 10, 2023\nOBOE.est.weeks &lt;- intersect(iso.weeks, c(paste0(\"2023-\", 15:52), paste0(\"2024-\", formatC(1:52, width = 2, flag = \"0\"))))\nn.weeks.transition &lt;- sum(rev(iso.weeks) %in% OBOE.est.weeks)\nfinal.state &lt;- 0.25\n\nOBOE.prop.weekly &lt;- rep(1 - 0.02, length(rev(iso.weeks)))\nOBOE.prop.weekly[rev(iso.weeks) %in% OBOE.est.weeks] &lt;- final.state + (1 - final.state) * (seq_len(n.weeks.transition) - 1) / n.weeks.transition\nnames(OBOE.prop.weekly) &lt;- rev(iso.weeks)\n\n\nlatest.crod &lt;- xmr.rpc(url.rpc = url.rpc, method = \"get_output_distribution\",\n  params = list(amounts = list(0), from_height = 0, to_height = current.height,\n    binary = FALSE, cumulative = TRUE))\n\nstart_height.RingCT &lt;- latest.crod$result$distributions[[1]]$start_height\nlatest.crod &lt;- latest.crod$result$distributions[[1]]$distribution\n\n# iso.weeks &lt;- sort(iso.weeks)\n\n\n# for (i in setdiff(rev(iso.weeks), OBOE.est.weeks)) {\nfor (i in rev(iso.weeks)) {\n# Compute the most recent week first so the support of all.weeks.weighted.pmf will have\n# the maximum number of output indices\n\n  if (i %in% weeks.missing.mempool.data) { next }\n\n  OBOE.prop &lt;- OBOE.prop.weekly[i]\n\n  xmr.rings.week &lt;- xmr.rings[\n    block_timestamp_ring_isoweek == i,\n    .(block_height_ring, tx_hash, input_num, ring_member_age, is.nonstandard.rucknium, is.nonstandard.isthmus)]\n\n  setorder(xmr.rings.week, -block_height_ring, input_num)\n  # TODO: Do I need to order by input_num too?\n\n  num_usable_rct_outputs.max &lt;- calculate_num_usable_rct_outputs(\n    latest.crod[1:(xmr.rings.week[1, block_height_ring] - start_height.RingCT)])\n\n  # split.ring.member.age &lt;- split(xmr.rings.week[, ring_member_age + 1], xmr.rings.week[, block_height_ring])\n  split.ring.member.age &lt;- split(xmr.rings.week[, .(block_height_ring, ring_member_age)],\n    cut(xmr.rings.week[, block_height_ring], breaks = threads))\n\n  split.ring.member.age &lt;- copy(split.ring.member.age)\n  for (j in seq_along(split.ring.member.age)) {\n    split.ring.member.age[[j]] &lt;- split(split.ring.member.age[[j]][, ring_member_age], split.ring.member.age[[j]][, block_height_ring])\n  }\n  split.ring.member.age &lt;- copy(split.ring.member.age)\n\n  uniformed.ring_member_age &lt;- future.apply::future_lapply(split.ring.member.age, function(block.chunk) {\n\n    transformed.data &lt;- list()\n    weighted.pmf &lt;- vector(\"numeric\", num_usable_rct_outputs.max)\n    n.ring.members.total &lt;- 0\n    v.mean &lt;- 0\n    z.mean &lt;- 0\n    z.max &lt;- 0\n\n    for (block.height in names(block.chunk)) {\n\n      n.ring.members &lt;- length(block.chunk[[block.height]])\n\n      block.chunk[[block.height]] &lt;- unique(block.chunk[[block.height]])\n      # Take unique() to avoid some duplicate computations and\n      # because the data.table will be merged later with xmr.rings.week\n\n      inner.loop.results &lt;- dsa.to.uniform(block.chunk[[block.height]],\n        as.numeric(block.height), latest.crod, start_height.RingCT, output.index.only.locked, OBOE.prop)\n\n      transformed.data[[block.height]] &lt;- inner.loop.results$transformed.data\n      # block.height is character data type here\n\n      if (length(inner.loop.results$pmf.contribution) != num_usable_rct_outputs.max) {\n        inner.loop.results$pmf.contribution &lt;- c(inner.loop.results$pmf.contribution,\n          rep(0, num_usable_rct_outputs.max - length(inner.loop.results$pmf.contribution)))\n      }\n\n      weighted.pmf &lt;- weighted.pmf + inner.loop.results$pmf.contribution * n.ring.members\n\n      n.ring.members.total &lt;- n.ring.members.total + n.ring.members\n\n      v.mean &lt;- v.mean + inner.loop.results$v * n.ring.members\n\n      z.mean &lt;- z.mean + inner.loop.results$z * n.ring.members\n\n      z.max &lt;- max(c(z.max, inner.loop.results$z))\n\n    }\n\n    list(\n      transformed.data = data.table(\n        block_height_ring = rep(as.integer(names(block.chunk)), times = lengths(transformed.data)),\n        ring_member_age = unlist(block.chunk, use.names = FALSE),\n        ring_member_age.transformed = unlist(transformed.data, use.names = FALSE)),\n      weighted.pmf = weighted.pmf,  n.ring.members.total = n.ring.members.total,\n      v.mean = v.mean, z.mean = z.mean, z.max = z.max)\n\n  }, future.globals = c(\"dsa.to.uniform\", \"split.ring.member.age\", \"latest.crod\", \"output.index.only.locked\", \"OBOE.prop\",\n    \"start_height.RingCT\", \"calculate_average_output_flow\", \"calculate_num_usable_rct_outputs\", \"actuar::plgamma\",\n    \"num_usable_rct_outputs.max\"),\n    future.packages = \"data.table\")\n\n  weighted.pmf &lt;- uniformed.ring_member_age[[1]]$weighted.pmf\n  uniformed.ring_member_age[[1]]$weighted.pmf &lt;- NULL\n\n  for (j in seq_len(length(uniformed.ring_member_age) - 1)) {\n    weighted.pmf  &lt;- weighted.pmf + uniformed.ring_member_age[[j + 1]]$weighted.pmf\n    uniformed.ring_member_age[[j + 1]]$weighted.pmf &lt;- NULL\n  }\n\n  n.ring.members.weekly.total &lt;- sum(sapply(uniformed.ring_member_age, FUN = function(x) {x$n.ring.members.total}))\n\n  weighted.pmf &lt;- weighted.pmf / n.ring.members.weekly.total\n  weighted.v.mean &lt;- sum(sapply(uniformed.ring_member_age, FUN = function(x) {x$v.mean})) / n.ring.members.weekly.total\n  weighted.z.mean &lt;- sum(sapply(uniformed.ring_member_age, FUN = function(x) {x$z.mean})) / n.ring.members.weekly.total\n  weekly.z.max &lt;- max(sapply(uniformed.ring_member_age, FUN = function(x) {x$z.max}))\n  weekly.weighted.cdf &lt;- cumsum(weighted.pmf)\n\n  if (i == rev(iso.weeks)[1]) {\n    all.weeks.weighted.pmf &lt;- weighted.pmf * n.ring.members.weekly.total\n    all.weeks.n.ring.members &lt;- n.ring.members.weekly.total\n    all.weeks.weighted.v.mean &lt;- weighted.v.mean * n.ring.members.weekly.total\n    all.weeks.weighted.z.mean &lt;- weighted.z.mean * n.ring.members.weekly.total\n    all.weeks.weekly.z.max &lt;- weekly.z.max\n  } else {\n    all.weeks.weighted.pmf &lt;- all.weeks.weighted.pmf + n.ring.members.weekly.total *\n      c(weighted.pmf, rep(0, length(all.weeks.weighted.pmf) - length(weighted.pmf)))\n    all.weeks.n.ring.members &lt;- all.weeks.n.ring.members + n.ring.members.weekly.total\n    all.weeks.weighted.v.mean &lt;- all.weeks.weighted.v.mean + weighted.v.mean * n.ring.members.weekly.total\n    all.weeks.weighted.z.mean &lt;- all.weeks.weighted.z.mean + weighted.z.mean * n.ring.members.weekly.total\n    all.weeks.weekly.z.max &lt;- max(c(all.weeks.weekly.z.max, weekly.z.max))\n  }\n\n  rm(weighted.pmf)\n\n  weekly.weighted.cdf &lt;- list(list(weekly.weighted.cdf = weekly.weighted.cdf,\n    weighted.v.mean = weighted.v.mean, weighted.z.mean = weighted.z.mean, weekly.z.max = weekly.z.max))\n  names(weekly.weighted.cdf) &lt;- i\n\n  qs::qsave(weekly.weighted.cdf, file =  paste0(cdf.dir, i, \".qs\") )\n  # saveRDS(weekly.weighted.cdf, file =  paste0(cdf.dir, i, \".rds\") )\n\n  rm(weekly.weighted.cdf)\n\n  uniformed.ring_member_age &lt;- data.table::rbindlist(lapply(uniformed.ring_member_age, FUN = function(x) {x$transformed.data}))\n\n  stopifnot(uniqueN(uniformed.ring_member_age ) == nrow(uniformed.ring_member_age ))\n\n  xmr.rings.week &lt;- merge(xmr.rings.week, uniformed.ring_member_age, by = c(\"block_height_ring\", \"ring_member_age\"))\n\n  rm(uniformed.ring_member_age)\n\n  setorder(xmr.rings.week, tx_hash, input_num)\n\n  xmr.rings.week[, ring_mem_id := rep(1:16, nrow(xmr.rings.week)/16)]\n\n  xmr.rings.week &lt;- data.table::dcast(xmr.rings.week,\n    tx_hash + input_num + is.nonstandard.rucknium + is.nonstandard.isthmus ~ ring_mem_id, value.var = \"ring_member_age.transformed\")\n\n  nonstandard.labels &lt;- xmr.rings.week[, c(\"is.nonstandard.rucknium\", \"is.nonstandard.isthmus\")]\n\n  set.seed(314)\n\n  xmr.rings.week &lt;- apply(xmr.rings.week, 1, FUN = function(x) {\n    sample(x[(length(x) - 15):length(x)])\n  })\n\n  mode(xmr.rings.week) &lt;- \"numeric\"\n\n  xmr.rings.week &lt;- t(xmr.rings.week)\n\n  xmr.rings.week &lt;- cbind(nonstandard.labels, xmr.rings.week)\n\n  colnames(xmr.rings.week) &lt;- c(\"is.nonstandard.rucknium\" , \"is.nonstandard.isthmus\",\n    paste0(\"ring.member.\", formatC(1:16, width = 2, flag = \"0\")) )\n\n  xmr.rings.week &lt;- list(xmr.rings.week)\n  names(xmr.rings.week) &lt;- i\n\n  qs::qsave(xmr.rings.week, file =  paste0(ring.member.ages.dir, i, \".qs\") )\n  # saveRDS(xmr.rings.week, file =  paste0(ring.member.ages.dir, i, \".rds\") )\n  # xmr.ring.member.ages\n\n  cat(i, base::date(), \"\\n\")\n\n}\n\n# TODO: Do I need these objects?\nall.weeks.weighted.cdf &lt;- cumsum(all.weeks.weighted.pmf / all.weeks.n.ring.members)\nall.weeks.weighted.v.mean &lt;- all.weeks.weighted.v.mean / all.weeks.n.ring.members\nall.weeks.weighted.z.mean &lt;- all.weeks.weighted.z.mean / all.weeks.n.ring.members\n# all.weeks.weekly.z.max already has the max\n\nqs::qsave(list(all.weeks.weighted.cdf = all.weeks.weighted.cdf,\n    all.weeks.weighted.v.mean = all.weeks.weighted.v.mean,\n    all.weeks.weighted.z.mean = all.weeks.weighted.z.mean,\n    all.weeks.weekly.z.max = all.weeks.weekly.z.max),\n  file = paste0(aggregate.cdf.dir, \"aggregate-weeks-cdf.qs\"))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transformed Age</span>"
    ]
  },
  {
    "objectID": "bjr-and-patra-sen.html",
    "href": "bjr-and-patra-sen.html",
    "title": "9  BJR and Patra-Sen",
    "section": "",
    "text": "9.1 Code\nThis chapter performs the Bonhomme-Jochmans-Robin (BJR) and Patra-Sen estimation. Section 3.2 and Section 3.3 explain the purpose of the BJR and Patra-Sen estimators, respectively.\nThe BJR estimator takes the transformed age data from Chapter 8 as input. It outputs four estimated nonparametric cumulative distribution functions (CDFs) of the four components of the mixture distribution and their estimated mixing proportions. It estimates the CDFs at 101 support points: {0.001,0.01,0.02,...,0.98,0.99,0.999}\\{0.001,0.01,0.02,...,0.98,0.99,0.999\\}. For a good estimate of the CDFs, I set the support points to have roughly equal probability mass between them. We have transformed the data into a roughly uniform distribution, so the CDF support points are set at a uniform distance from each other, between 0 and 1.\nThe estimated component with the highest mixing proportion is selected. This component is used as an input to the Patra-Sen estimator. A Uniform(0,1) distribution is used for the “decoy” distribution, Fb(x)F_{b}(x). The output of the Patra-Sen estimation is the estimated real spend distribution.\nWe could stop here, but we can do a second-pass estimation for better results. The estimated real spend distribution from the initial Patra-Sen estimation will not have roughly equal probability mass between each CDF support point. We can interpolate the estimated real spend CDF to get new CDF support points that do have roughly equal probability mass between each point. Then the BJR estimation is performed again with the new CDF support points.\nThis procedure is followed three times:\nThe results from these three data subsets can be compared.\nThis code will take about two weeks to run on a powerful machine. The code in this chapter can run while the code in Chapter 8 is running, but it needs to stay “behind” the transformed age data as it is being written to the cdf.dir and ring.member.ages.dir directories.\nThe threads variable is the number of CPU threads to use. The cdf.dir and ring.member.ages.dir variables should be the same as specified in the previous chapter, Chapter 8 . results.dir is the name of the directory where the BJR estimate will be written to.\nlibrary(decoyanalysis)\nlibrary(future)\nlibrary(data.table)\n\nthreads &lt;- 64 - 17\n\ncdf.dir &lt;- \"weekly-weighted-cdf\"\nring.member.ages.dir &lt;- \"weekly-ring-member-ages\"\nresults.dir &lt;- \"results\"\n\n\n\ndir.create(results.dir)\n\nresults.dir.run &lt;- paste0(results.dir, \"/results-01/\")\n\ndir.create(results.dir.run)\n\nresults.dir.run.bjr &lt;- paste0(results.dir, \"bjr/\")\n\ndir.create(results.dir.run.bjr)\n\n\noptions(future.globals.maxSize = 8000*1024^2)\n\nfuture::plan(future::multicore(workers = threads))\n\ncluster.threads &lt;- NULL\n\nis.raw &lt;- FALSE\n\nif (is.raw) {\n  ring.member.ages.dir &lt;- \"weekly-ring-member-ages-raw\"\n}\n\n\n\n\nfor (i in c(\"all\", \"rucknium\", \"rucknium_isthmus\")) {\n  dir.create(paste0(results.dir.run.bjr, i))\n  dir.create(paste0(results.dir.run.bjr, i, \"/first-pass\"))\n}\n\nxmr.rings.ages.weeks &lt;- list.files(ring.member.ages.dir)\nxmr.rings.ages.weeks &lt;- xmr.rings.ages.weeks[grepl(\".qs\", xmr.rings.ages.weeks)]\nalready.done &lt;- Reduce(intersect, list(list.files(paste0(results.dir.run.bjr, \"all\")),\n  list.files(paste0(results.dir.run.bjr, \"rucknium\")), list.files(paste0(results.dir.run.bjr, \"rucknium_isthmus\"))))\nxmr.rings.ages.weeks &lt;- setdiff(xmr.rings.ages.weeks, already.done)\nxmr.rings.ages.weeks &lt;- rev(sort(xmr.rings.ages.weeks))\n\n\nII &lt;- 10\nK &lt;- 4\n\n# cdf.points &lt;- (0:100)/100\ncdf.points &lt;- c(0.001, (1:99)/100, 0.999)\n#cdf.points &lt;- c(0.0001, (1:99)/100, 0.9999)\n# Distribution should be approximately uniform\n\n\n\nwhile ( length(xmr.rings.ages.weeks) &gt; 0) {\n  week &lt;- xmr.rings.ages.weeks[1]\n\n  y.with.labels &lt;- qs::qread(paste0(ring.member.ages.dir, week))[[1]]\n\n  if (is.raw) {\n    cdf.points &lt;- unname(quantile(c(y), probs = (0:100)/100, na.rm = TRUE))\n  }\n\n  for (label in c(\"all\", \"rucknium\", \"rucknium_isthmus\")) {\n\n    if ( ! (label == \"rucknium_isthmus\" &\n        y.with.labels[, sum(is.nonstandard.rucknium) == sum(is.nonstandard.rucknium | is.nonstandard.isthmus)] ) ) {\n      # If is.nonstandard.isthmus does not have additional TRUEs, then just skip and output the Rucknium one\n\n      y &lt;- switch(label,\n        all = y.with.labels[, -(1:2), with = FALSE],\n        rucknium = y.with.labels[! y.with.labels$is.nonstandard.rucknium, -(1:2), with = FALSE],\n        rucknium_isthmus = y.with.labels[! y.with.labels$is.nonstandard.rucknium &\n            ! y.with.labels$is.nonstandard.isthmus, -(1:2), with = FALSE])\n\n      print(system.time(bjr.results &lt;- bjr(y, II = II, K = K, cdf.points = cdf.points,\n        estimate.mean.sd = FALSE, basis = \"Chebychev\", control = list(cluster.threads = cluster.threads ))))\n        \n      qs::qsave(bjr.results, file = paste0(results.dir.run.bjr, label, \"/first-pass/\", week))\n\n      wallet2.dist.index &lt;- which.max(bjr.results$mixing.proportions)\n      Fn.hat.value &lt;- bjr.results$cdf$CDF[, wallet2.dist.index]\n      supp.points &lt;- bjr.results$cdf$cdf.points\n      decoy &lt;- punif\n      Fb &lt;- decoy\n      M = 16\n      alpha &lt;- 1/M\n\n      patra.sen.bjr.results &lt;- patra.sen.bjr(Fn.hat.value, supp.points, Fb, alpha)\n\n      new.cdf.points &lt;-  approx(c(0, patra.sen.bjr.results$Fs.hat, 1), c(0, supp.points, 1), supp.points)$y\n\n      print(system.time(bjr.results.new &lt;- bjr(y, II = II, K = K, cdf.points = new.cdf.points,\n        estimate.mean.sd = FALSE, basis = \"Chebychev\", control = list(cluster.threads = cluster.threads ))))\n\n    }\n\n    qs::qsave(bjr.results.new, file = paste0(results.dir.run.bjr, label, \"/\", week))\n\n  }\n  \n  xmr.rings.ages.weeks &lt;- list.files(ring.member.ages.dir)\n  xmr.rings.ages.weeks &lt;- xmr.rings.ages.weeks[grepl(\".qs\", xmr.rings.ages.weeks)]\n  already.done &lt;- Reduce(intersect, list(list.files(paste0(results.dir.run.bjr, \"all\")),\n    list.files(paste0(results.dir.run.bjr, \"rucknium\")), list.files(paste0(results.dir.run.bjr, \"rucknium_isthmus\"))))\n  xmr.rings.ages.weeks &lt;- setdiff(xmr.rings.ages.weeks, already.done)\n  xmr.rings.ages.weeks &lt;- rev(sort(xmr.rings.ages.weeks))\n\n  cat(week, \"\\n\")\n\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>BJR and Patra-Sen</span>"
    ]
  },
  {
    "objectID": "nonparametric-real-spend.html",
    "href": "nonparametric-real-spend.html",
    "title": "10  Nonparametric Real Spend",
    "section": "",
    "text": "10.1 Reverse the transformation of output age\nThe previous chapter, Chapter 9 , gave us the estimated value of the real spend CDF at 101 points. For the parametric fitting step, we need the probability mass function (PMF) value of the real spend age distribution at each of Monero’s 100 million spendable RingCT outputs. In this chapter we will also produce summary statistics of the estimated real spend age distribution.\nIn the previous chapter, Chapter 9 , we obtained an estimate of the real spend age distribution after it was transformed by the CDF of the decoy distribution in Chapter 8 . To get back to the un-transformed distribution, we need to compute G−1(st)G^{-1}(s_{t}), the inverse CDF (i.e. the quantile function) of the decoy distribution. Let FS′̂(x)\\hat{F_{S}^{'}}(x) be the estimated CDF of the transformed real spend distribution, evaluated at support points x1,x2,...,x101x_{1},x_{2},...,x_{101}. The value of the estimated un-transformed real spend distribution FŜ\\hat{F_{S}} at the iith support point is defined in Equation 10.1 :\nFŜ(FS′̂(xi))=G−1(xi)(10.1)\n\\hat{F_{S}}\\left(\\hat{F_{S}^{'}}(x_{i})\\right)=G^{-1}(x_{i})\n \\qquad(10.1)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Nonparametric Real Spend</span>"
    ]
  },
  {
    "objectID": "nonparametric-real-spend.html#interpolation-of-the-cdf",
    "href": "nonparametric-real-spend.html#interpolation-of-the-cdf",
    "title": "10  Nonparametric Real Spend",
    "section": "10.2 Interpolation of the CDF",
    "text": "10.2 Interpolation of the CDF\nWe have evaluated the CDF at only 101 points. We want a smooth interpolation through all points, i.e. every transaction output. Hippel, Hunter, and Drown (2017) suggest fitting a cubic spline (piecewise polynomial) with monotonicity restrictions. We use the splinebins() function in the binsmooth package to accomplish the interpolation. Later, the PMF can be computed by taking the first difference of the CDF.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Nonparametric Real Spend</span>"
    ]
  },
  {
    "objectID": "nonparametric-real-spend.html#computing-summary-statistics",
    "href": "nonparametric-real-spend.html#computing-summary-statistics",
    "title": "10  Nonparametric Real Spend",
    "section": "10.3 Computing summary statistics",
    "text": "10.3 Computing summary statistics\nHaving produced the estimated real spend CDF FŜ\\hat{F_{S}}, we can compute summary statistics. Quantiles, including the median, are computed easily by evaluating the inverse CDF at the desired quantile. Other statistics, such as mean, standard deviation, skewness, and kurtosis, can be computed by evaluating a Riemann–Stieltjes integral with the appropriate integrand.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Nonparametric Real Spend</span>"
    ]
  },
  {
    "objectID": "nonparametric-real-spend.html#code",
    "href": "nonparametric-real-spend.html#code",
    "title": "10  Nonparametric Real Spend",
    "section": "10.4 Code",
    "text": "10.4 Code\nThis code can be run in a new R session.\n\nlibrary(decoyanalysis)\nlibrary(data.table)\n\ncdf.dir &lt;- \"weekly-weighted-cdf\"\nresults.dir &lt;- \"results\"\n\nresults.dir.run &lt;- paste0(results.dir, \"/results-01/\")\n\nexclusion.weeks &lt;- NULL\n\nnonparametric.real.spend &lt;- list()\n\nfor (label in c(\"all\", \"rucknium\", \"rucknium_isthmus\")) {\n  \n  results.dir.run.label &lt;- paste0(results.dir.run, \"bjr/\", label, \"/\")\n  \n  week.set &lt;- list.files(results.dir.run.label)\n  week.set &lt;- week.set[grepl(\".qs\", week.set)]\n  \n  weekly.real.spend.cdfs &lt;- list()\n  \n  summary.stats &lt;- data.table(week = week.set, mean = 0, median = 0, sd = 0, skewness = 0, kurtosis = 0)\n  \n  mixing.proportions &lt;- data.table(week = week.set, component_1 = 0,  component_2 = 0,  component_3 = 0,  component_4 = 0)\n  \n  for (week.to.analyze in week.set) {\n    bjr.results &lt;- qs::qread(paste0(results.dir.run.label, week.to.analyze))\n    cat(week.to.analyze, round(100 * rev(sort(bjr.results$mixing.proportions)), 3),  \"\\n\")\n    # print(round(100 * rev(sort(bjr.results$mixing.proportions)), 3))\n    mixing.proportions[week == week.to.analyze, component_1 := rev(sort(bjr.results$mixing.proportions))[1]]\n    mixing.proportions[week == week.to.analyze, component_2 := rev(sort(bjr.results$mixing.proportions))[2]]\n    mixing.proportions[week == week.to.analyze, component_3 := rev(sort(bjr.results$mixing.proportions))[3]]\n    mixing.proportions[week == week.to.analyze, component_4 := rev(sort(bjr.results$mixing.proportions))[4]]\n  }\n  \n  \n  support.max &lt;- 0\n  all.weeks.weighted.v.mean &lt;- c()\n  \n  \n  for (week.to.analyze in setdiff(week.set, exclusion.weeks)) {\n    \n    \n    if (week.to.analyze %in% summary.stats[mean != 0, week]) { next }\n    \n    cat(week.to.analyze, \"\\n\")\n    \n    \n    weekly.weighted.cdf &lt;- qs::qread(paste0(cdf.dir, week.to.analyze))\n    \n    weighted.v.mean &lt;- weekly.weighted.cdf[[1]]$weighted.v.mean\n    all.weeks.weighted.v.mean[week.to.analyze] &lt;- weighted.v.mean\n    weekly.z.max &lt;- weekly.weighted.cdf[[1]]$weekly.z.max\n    support.max &lt;- max(c(support.max, weekly.z.max))\n    weekly.weighted.cdf &lt;- weekly.weighted.cdf[[1]]$weekly.weighted.cdf\n    \n    intermediate.stepfun &lt;- stepfun(seq_along(weekly.weighted.cdf), c(0, weekly.weighted.cdf))\n    \n    weekly.weighted.inv.cdf &lt;- Vectorize(function(x) { gbutils::cdf2quantile(x, cdf = intermediate.stepfun) })\n    \n    bjr.results &lt;- qs::qread(paste0(results.dir.run.label, week.to.analyze))\n    \n    \n    wallet2.dist.index &lt;- which.max(bjr.results$mixing.proportions)\n    Fn.hat.value &lt;- bjr.results$cdf$CDF[, wallet2.dist.index]\n    supp.points &lt;- bjr.results$cdf$cdf.points\n    \n    decoy &lt;- punif\n    Fb &lt;- decoy\n    M = 16\n    alpha &lt;- 1/M\n    \n    patra.sen.bjr.results &lt;- patra.sen.bjr(Fn.hat.value, supp.points, Fb, alpha)\n    \n    Fs.hat &lt;- patra.sen.bjr.results$Fs.hat\n    \n    supp.points.transformed &lt;- vector(\"numeric\", length(supp.points))\n    \n    for (i in seq_along(supp.points)) {\n      if (supp.points[i] &lt; 0.000001) {\n        supp.points.transformed[i] &lt;- 0\n        next\n      }\n      if (supp.points[i] &gt; max(weekly.weighted.cdf)) {\n        # Sometimes max(weekly.weighted.cdf) is not equal to 1\n        supp.points.transformed[i] &lt;- length(weekly.weighted.cdf)\n        next\n      }\n      supp.points.transformed[i] &lt;- weekly.weighted.inv.cdf(supp.points[i])\n    }\n    \n    regularized.real.spend.cdf &lt;- aggregate(Fs.hat, by = list(Fs.hat.transformed = supp.points.transformed), FUN = max)\n    \n    Fs.hat.transformed.reg &lt;- regularized.real.spend.cdf$Fs.hat.transformed\n    supp.points.reg &lt;- regularized.real.spend.cdf$x\n    # Regularized eliminates duplicate support points\n    \n    weekly.real.spend.cdfs[[week.to.analyze]] &lt;- binsmooth::splinebins(\n      bEdges = c(Fs.hat.transformed.reg, max(Fs.hat.transformed.reg) + 1),\n      bCounts = c(diff(c(0, supp.points.reg)), 0))$splineCDF\n    \n    real.spend.ecdf &lt;- stepfun(c(Fs.hat.transformed.reg), c(0, supp.points.reg))\n    \n    days.unit &lt;- weighted.v.mean/(60^2*24)\n    est.mean &lt;- spatstat.univar::stieltjes(function(x){x * days.unit}, real.spend.ecdf)[[1]]\n    est.var &lt;- spatstat.univar::stieltjes(function(x){(x * days.unit - est.mean)^2}, real.spend.ecdf)[[1]]\n    \n    if ( ! week.to.analyze %in% summary.stats$week) {\n      summary.stats &lt;- rbind(summary.stats, data.table(week = week.to.analyze), fill = TRUE)\n    }\n    \n    summary.stats[week == week.to.analyze, mean := est.mean]\n    summary.stats[week == week.to.analyze, median := Fs.hat.transformed.reg[findInterval(0.5, supp.points.reg)] * days.unit ]\n    summary.stats[week == week.to.analyze, percentile.25 := Fs.hat.transformed.reg[findInterval(0.25, supp.points.reg)] * days.unit ]\n    summary.stats[week == week.to.analyze, sd := sqrt(est.var)]\n    summary.stats[week == week.to.analyze, skewness := spatstat.univar::stieltjes(function(x){(x * days.unit - est.mean)^3}, real.spend.ecdf)[[1]] / est.var^(3/2)]\n    summary.stats[week == week.to.analyze, kurtosis := spatstat.univar::stieltjes(function(x){(x * days.unit - est.mean)^4}, real.spend.ecdf)[[1]] / est.var^(2)]\n    \n    \n  }\n  \n  setorder(summary.stats, week)\n  \n  print(summary.stats)\n  \n  nonparametric.real.spend[[label]] &lt;- list(weekly.real.spend.cdfs = weekly.real.spend.cdfs, all.weeks.weighted.v.mean = all.weeks.weighted.v.mean, support.max = support.max,\n    mixing.proportions = mixing.proportions, summary.stats = summary.stats)\n\n}\n\n\nqs::qsave(nonparametric.real.spend, file = paste0(results.dir.run, \"nonparametric-real-spend.qs\"))\n\n\n\n\n\nHippel, Paul T. von, David J. Hunter, and McKalie Drown. 2017. “Better Estimates from Binned Income Data: Interpolated CDFs and Mean-Matching.” Sociological Science 4 (26): 641–55. https://doi.org/10.15195/v4.a26.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Nonparametric Real Spend</span>"
    ]
  },
  {
    "objectID": "real-spend-visualization.html",
    "href": "real-spend-visualization.html",
    "title": "11  Real Spend Visualization",
    "section": "",
    "text": "11.1 Summary statistics\nThis chapter plots the results of the real spend distribution estimation, week-by-week.\nFigure 11.1 plots mean, median, 25th percentile, standard deviation, skewness, and kurtosis. Several events are indicated with colored vertical lines. The summary statistics were relatively stable until the first indicated event, the announcement of the Zcash community forum that Binance requested a protocol change: “A representative from Binance’s listing team recently approached the Zcash core teams about a way to prevent users from sending ZEC from a shielded address to a transparent address on the Binance exchange.” Thereafter, the summary statistics are more volatile week-to-week. Uncertainty in the exchange rate market may have large effects on the real spend age distribution as speculative inflows and outflows from centralized exchanges change from week to week. The share of Monero transactions related to centralized exchanges is unknown, but an estimated 75 percent of bitcoin transactions are inflows or outflows from exchange-like entities (Makarov and Schoar 2021).\nDuring the March 2024 suspected spam attack, the median age of spent outputs increased. The age distribution shifted in the opposite direction during the earlier 2021 suspected spam wave (Krawiec-Thayer et al. 2021). The shift towards older outputs could be explained by the spammer’s spending script and/or changes in normal user behavior because of a congested txpool. The median age peaked to 11 days during the suspected spam-related consolidation transactions in April 2024. Probably, the spammer was consolidating outputs that were created during the spam a few weeks prior.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real Spend Visualization</span>"
    ]
  },
  {
    "objectID": "real-spend-visualization.html#summary-statistics",
    "href": "real-spend-visualization.html#summary-statistics",
    "title": "11  Real Spend Visualization",
    "section": "",
    "text": "Figure 11.1",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real Spend Visualization</span>"
    ]
  },
  {
    "objectID": "real-spend-visualization.html#sec-nonparametric-real-spend-mixing-proportions",
    "href": "real-spend-visualization.html#sec-nonparametric-real-spend-mixing-proportions",
    "title": "11  Real Spend Visualization",
    "section": "11.2 Mixing proportions",
    "text": "11.2 Mixing proportions\nFigure 11.2 plots the mixing proportions of the distribution components estimated by the BJR estimator. The iith mixing proportion in each week does not necessarily refer to the same distribution week-to-week. (This is due to the labeling identification problem of mixture distributions.) The mixing proportions are simply sorted from highest to lowest.\nLarge deviations in the mixing proportions are observed during significant exogenous events like delisting announcements and spam. It is likely that the BJR estimator split the single wallet2-associated ring distribution into two distributions during those events because the real spend distribution itself split into users (or spammers) who were reacting to the event and those who did not react. Another possibility, though unlikely, is that users who use nonstandard wallets were more likely to transact during these events. The next chapter will discuss how these events can be handled for the overall parametric fitting step.\n\n\n\n\nFigure 11.2",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real Spend Visualization</span>"
    ]
  },
  {
    "objectID": "real-spend-visualization.html#animations",
    "href": "real-spend-visualization.html#animations",
    "title": "11  Real Spend Visualization",
    "section": "11.3 Animations",
    "text": "11.3 Animations\n\n11.3.1 PMF, non-log Y axis\nBelow is an animation of the probability mass function (PMF) of the estimated real spend distribution of the first 48 hours of the distribution. Each week is one frame. The animation can be controlled by clicking buttons on the panel below the animation.\n\n\n    \n\n    \n    \n\n    \n    \n\n\n\n11.3.2 PMF, log Y axis\nBelow is the same animation, but with a log Y axis.\n\n    \n\n    \n    \n\n    \n    \n\n\n\n11.3.3 CDF\nBelow is an animation of the cumulative distribution function (CDF) of the estimated real spend distribution of the first 48 hours of the distribution.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real Spend Visualization</span>"
    ]
  },
  {
    "objectID": "real-spend-visualization.html#code",
    "href": "real-spend-visualization.html#code",
    "title": "11  Real Spend Visualization",
    "section": "11.4 Code",
    "text": "11.4 Code\nThis code can be run in a new R session\n\nlibrary(ggplot2)\nlibrary(dRacula)\nlibrary(animation)\n\nresults.dir &lt;- \"results\"\n\nresults.dir.run &lt;- paste0(results.dir, \"/results-01/\")\n\n\nnonparametric.real.spend &lt;- qs::qread(file = paste0(results.dir.run, \"nonparametric-real-spend.qs\"))\n\nnonparametric.real.spend &lt;- nonparametric.real.spend$rucknium\n\n\n# Static timeline plots\n\n\ndark.mode &lt;- TRUE\n\nif (dark.mode) {\n  brewed.cols &lt;- dracula_tibble$hex\n} else {\n  brewed.cols &lt;- RColorBrewer::brewer.pal(8, \"Accent\")[-4]\n}\n\n\n\nISOweek2date.convert &lt;- function(x) {\n  x &lt;- gsub(\"[.]qs\", \"\", x)\n  x &lt;- strsplit(x, \"-\")\n  x &lt;- sapply(x, FUN = function(z) { paste0(z[1], \"-W\", z[2], \"-1\") } )\n  ISOweek::ISOweek2date(x)\n}\n\n\n\n\nvert.lines &lt;- data.frame(\n  week = ISOweek2date.convert(c(\"2023-45\", \"2024-01\", \"2024-06\", \"2024-10\",\n    \"2024-13\", \"2024-16\", \"2024-40\")),\n  week.iso = c(\"2023-45\", \"2024-01\", \"2024-06\", \"2024-10\",\n    \"2024-13\", \"2024-16\", \"2024-40\"),\n  label = c(\"Disclosure of Zcash Binance request (2023-10)\", \"Binance Monitoring Tag on Monero (2024-01)\",\n    \"Binance delisting announcement (2024-02)\", \"Spam starts (2024-03)\", \"Spam ends (2024-03)\",\n    \"Spam-related consolidations (2024-04)\", \"Kraken EU delisting ann. (2024-10)\"),\n  color = brewed.cols\n)\n  # 1) Nov 9 https://forum.zcashcommunity.com/t/important-potential-binance-delisting/45954\n  # 2) https://monero.observer/binance-marks-monero-potential-delisting/\n  # 3) https://monero.observer/binance-decides-to-finally-delist-monero-20-february-2024/\n  # 4) https://github.com/Rucknium/misc-research/blob/main/Monero-Black-Marble-Flood/pdf/monero-black-marble-flood.pdf\n  # 5) https://github.com/Rucknium/misc-research/blob/main/Monero-Black-Marble-Flood/pdf/monero-black-marble-flood.pdf\n  # 6) Suspected spam-related consolidation transactions April 16 and 17\n  #    https://libera.monerologs.net/monero-research-lab/20240417#c366338\n  # 7) https://monero.observer/kraken-to-delist-monero-european-economic-area-31-october-2024/\n\n\nest.summary.stats &lt;- reshape(nonparametric.real.spend$summary.stats,\n  dir = \"long\", idvar = \"week\",\n  varying = setdiff(names(nonparametric.real.spend$summary.stats), \"week\"), v.names = \"y\")  \nnames(est.summary.stats) &lt;- c(\"week\", \"stat\", \"y\")\n# est.summary.stats$stat &lt;- factor(est.summary.stats$stat, labels = attr(est.summary.stats, \"reshapeLong\")$varying$y)\nest.summary.stats$stat &lt;- factor(est.summary.stats$stat,\n  labels = c(\"mean\", \"median\", \"percentile.25\", \"sd\", \"skewness\", \"kurtosis\"))\n# Manually specify to control the order\nest.summary.stats$week &lt;- ISOweek2date.convert(est.summary.stats$week)\n\n\nest.summary.stats$dummy.color &lt;- factor(rep(NA, nrow(est.summary.stats)),\n  levels = brewed.cols, labels = brewed.cols)\n\n\npng(\"images/real-spend-summary-stats-dark-mode.png\", width = 1000, height = 1000)\n\ngg &lt;- ggplot(data = est.summary.stats, aes(week, y, group = stat, color = dummy.color)) +\n  labs(title = \"Summary statistics of estimated real spend age distribution. Unit of stats is days.\") + \n  geom_line(show.legend = TRUE) +\n  # geom_vline(aes(xintercept = week, color = color), data = vert.lines, linetype = 2) +\n  # This format above doesn't work. Must specify them individually:\n  geom_vline(xintercept = vert.lines$week[1], color = vert.lines$color[1], linetype = 1) +\n  geom_vline(xintercept = vert.lines$week[2], color = vert.lines$color[2], linetype = 1) +\n  geom_vline(xintercept = vert.lines$week[3], color = vert.lines$color[3], linetype = 1) +\n  geom_vline(xintercept = vert.lines$week[4], color = vert.lines$color[4], linetype = 1) +\n  geom_vline(xintercept = vert.lines$week[5], color = vert.lines$color[5], linetype = 1) +\n  geom_vline(xintercept = vert.lines$week[6], color = vert.lines$color[6], linetype = 1) +\n  geom_vline(xintercept = vert.lines$week[7], color = vert.lines$color[7], linetype = 1) +\n  scale_x_date(date_breaks = \"4 week\", guide = guide_axis(angle = 90),\n    date_labels = \"%y-%m-%d\", sec.axis = dup_axis(), expand = c(0, 0)) +\n  facet_grid(rows = vars(stat), scales = \"free\") +\n  scale_color_manual(limits = vert.lines$label, values = vert.lines$color, name = NULL,\n    drop = FALSE, guide = guide_legend(nrow = 3, byrow = FALSE, override.aes = list(linewidth = 3)))\n\n\nif (dark.mode) {\n  gg +\n    theme_dracula() +\n    theme(\n      legend.position = \"top\",\n      axis.title.y = element_blank(),\n      axis.title.x = element_blank(),\n      plot.title = element_text(size = 25),\n      legend.text = element_text(size = 16.5),\n      axis.text.y = element_text(size = 15),\n      axis.text.x = element_text(size = 17),\n      strip.text = element_text(size = 17),\n      panel.border = element_rect(color = \"white\", fill = NA)\n    )\n} else {\n  gg +\n    theme(\n      legend.position = \"top\",\n      axis.title.y = element_blank(),\n      axis.title.x = element_blank(),\n      plot.title = element_text(size = 25),\n      legend.text = element_text(size = 16.5),\n      axis.text.y = element_text(size = 15),\n      axis.text.x = element_text(size = 17),\n      strip.text = element_text(size = 17),\n      panel.border = element_rect(color = \"white\", fill = NA)\n    )\n  # theme() must go after theme_dracula()\n}\n\n\ndev.off()\n\n\n\n\n\n\nest.mixing.proportions &lt;- reshape(nonparametric.real.spend$mixing.proportions,\n  dir = \"long\", idvar = \"week\",\n  varying = setdiff(names(nonparametric.real.spend$mixing.proportions), \"week\"), v.names = \"y\")  \nnames(est.mixing.proportions) &lt;- c(\"week\", \"component\", \"y\")\n\nest.mixing.proportions$week &lt;- ISOweek2date.convert(est.mixing.proportions$week)\n\n\n\nest.mixing.proportions$dummy.color &lt;- factor(rep(NA, nrow(est.mixing.proportions)),\n  levels = brewed.cols, labels = brewed.cols)\n\n\n\n\npng(\"images/mixing-proportions-dark-mode.png\", width = 1000, height = 1000)\n\ngg &lt;- ggplot(data = est.mixing.proportions, aes(week, y, group = component, color = dummy.color)) +\n  labs(title = \"Estimated mixing proportions of ring distribution components\") + \n  geom_line(show.legend = TRUE) +\n  # geom_vline(aes(xintercept = week, color = color), data = vert.lines, linetype = 2) +\n  # This format above doesn't work. Must specify them individually:\n  geom_vline(xintercept = vert.lines$week[1], color = vert.lines$color[1], linetype = 1) +\n  geom_vline(xintercept = vert.lines$week[2], color = vert.lines$color[2], linetype = 1) +\n  geom_vline(xintercept = vert.lines$week[3], color = vert.lines$color[3], linetype = 1) +\n  geom_vline(xintercept = vert.lines$week[4], color = vert.lines$color[4], linetype = 1) +\n  geom_vline(xintercept = vert.lines$week[5], color = vert.lines$color[5], linetype = 1) +\n  geom_vline(xintercept = vert.lines$week[6], color = vert.lines$color[6], linetype = 1) +\n  geom_vline(xintercept = vert.lines$week[7], color = vert.lines$color[7], linetype = 1) +\n  scale_x_date(date_breaks = \"4 week\", guide = guide_axis(angle = 90),\n    date_labels = \"%y-%m-%d\", sec.axis = dup_axis(), expand = c(0, 0)) +\n  scale_y_continuous(labels = scales::percent) +\n  facet_grid(rows = vars(component), scales = \"free\") +\n  scale_color_manual(limits = vert.lines$label, values = vert.lines$color, name = NULL,\n    drop = FALSE, guide = guide_legend(nrow = 3, byrow = FALSE, override.aes = list(linewidth = 3)))\n\n\nif (dark.mode) {\n  gg +\n    theme_dracula() +\n    theme(\n      legend.position = \"top\",\n      axis.title.y = element_blank(),\n      axis.title.x = element_blank(),\n      plot.title = element_text(size = 25),\n      legend.text = element_text(size = 16.5),\n      axis.text.y = element_text(size = 15),\n      axis.text.x = element_text(size = 17),\n      strip.text = element_text(size = 17),\n      panel.border = element_rect(color = \"white\", fill = NA)\n    )\n} else {\n  gg +\n    theme(\n      legend.position = \"top\",\n      axis.title.y = element_blank(),\n      axis.title.x = element_blank(),\n      plot.title = element_text(size = 25),\n      legend.text = element_text(size = 16.5),\n      axis.text.y = element_text(size = 15),\n      axis.text.x = element_text(size = 17),\n      strip.text = element_text(size = 17),\n      panel.border = element_rect(color = \"white\", fill = NA)\n    )\n  # theme() must go after theme_dracula()\n}\n\n\ndev.off()\n\n\n\n\n\n# Animation plots\n\n\n\n\nweekly.real.spend.cdfs &lt;- nonparametric.real.spend$weekly.real.spend.cdfs\n\nall.weeks.weighted.v.mean &lt;- nonparametric.real.spend$all.weeks.weighted.v.mean\n\nweighted.v.mean &lt;- mean(all.weeks.weighted.v.mean)\n\nsupport.max &lt;- nonparametric.real.spend$support.max\n\ndays.unit &lt;- weighted.v.mean/(60^2*24)\nminutes.unit &lt;- weighted.v.mean/(60)\n\n\n\nGAMMA_SHAPE = 19.28\nGAMMA_RATE = 1.61\n\nG &lt;- function(x) {\n  actuar::plgamma(x, shapelog = GAMMA_SHAPE, ratelog = GAMMA_RATE)\n}\n\nG_star &lt;- function(x) {\n  (0 &lt;= x*v & x*v &lt;= 1800) *\n    (G(x*v + 1200) - G(1200) +\n        ( (x*v)/(1800) ) * G(1200)\n    )/G(z*v) +\n    (1800 &lt; x*v & x*v &lt;= z*v) * G(x*v + 1200)/G(z*v) +\n    (z*v &lt; x*v) * 1\n}\n\n\n\n\nsupport.viz &lt;- 1:ceiling(60*48/minutes.unit)\n\n\nempirical.cdf &lt;- weekly.real.spend.cdfs\n\nyear.week.range &lt;- sort(names(weekly.real.spend.cdfs))\n\nweekly.historical.length &lt;- 52/2\n\ncol.palette &lt;- viridis::magma(weekly.historical.length)\n\nani.options(interval = 1, ani.height = 1000, ani.width = 1000, verbose = FALSE)\n\n# PMF\n\nfor (log.y in c(FALSE, TRUE)) {\n  \n  saveHTML({\n    \n    for (i in seq_along(year.week.range) ) {\n      \n      subset.seq &lt;- 1:i\n      subset.seq &lt;- subset.seq[max(1, i - weekly.historical.length + 1):max(subset.seq)]\n      col.palette.subset &lt;- col.palette[max(1, weekly.historical.length - i + 1):weekly.historical.length]\n      col.palette.subset[length(col.palette.subset)] &lt;- \"green\"\n      \n      year.week.range.subset &lt;- year.week.range[subset.seq]\n      \n      par(bg = 'black', fg = 'white', cex = 2)\n      \n      plot(1,\n        main = paste0(\"PMF of estimated real spend age,\\nby week of transaction\"),\n        sub = paste0(\"ISO week \", gsub(\".qs\", \"\", year.week.range[i]), \" (Week starting \",\n          # as.Date(paste0(year.week.range[i], \"-1\"), \"%Y-%U.qs-%u\"), \")\"),\n          ISOweek2date.convert(year.week.range[i]), \")\" ),\n        ylab = ifelse(log.y, \"PMF (log scale)\", \"PMF\"), xlab = \"Age of spent output (log scale)\",\n        col.lab = \"white\", col.axis = \"white\", col.main = \"white\", col.sub = \"white\",\n        col = \"transparent\", log = ifelse(log.y, \"xy\", \"x\"),\n        xlim = (range(support.viz) + 1)* minutes.unit,\n        ylim = ifelse(rep(log.y, 2), c(.0000002, 0.01), c(0, 0.005)),\n        xaxt = \"n\", yaxs = \"i\")\n      \n      for ( j in seq_along(subset.seq)) {\n        lines(\n          (support.viz[-length(support.viz)][-1]) * minutes.unit,\n          diff(empirical.cdf[[ year.week.range.subset[j] ]]( c(support.viz[-length(support.viz)] + 1))),\n          col = col.palette.subset[j],\n          lwd = ifelse(j == length(subset.seq), 2, 0.5)\n        )\n      }\n      \n      v &lt;- all.weeks.weighted.v.mean[ year.week.range[i] ]\n      z &lt;- support.max\n      \n      status.quo.decoy &lt;- diff(G_star(as.numeric(support.viz)))\n      \n      lines(support.viz[-1] * minutes.unit, status.quo.decoy, col = \"red\", lwd = 2)\n      \n      cat(i, \" \")\n      \n      legend(\"topright\", legend = c(\"Decoy status quo\"), fill = c(\"red\"))\n      \n      axis(side = 1, at = c(1/6, 1, 10, 60, 60*12, 60*48),\n        labels = c(\"10 sec\", \"1 min\", \"10 min\", \"1 hour\", \"12 hrs\", \"48 hrs\"),\n        col.axis = \"white\", las = 2, cex.axis = 0.8)\n      \n      axis(side = 4, col.axis = \"white\")\n      \n      if (gsub(\".qs\", \"\", year.week.range[i]) %in% vert.lines$week.iso) {\n        event.label &lt;- vert.lines$label[vert.lines$week.iso == gsub(\".qs\", \"\", year.week.range[i]) ]\n        event.label &lt;- paste0(\"Event: \", gsub(\" [(].*[)]\", \"\", event.label))\n        # y.event.pos &lt;- ifelse(rep(log.y, 2), c(.0000002, 0.01), c(0, 0.005))\n        mtext(event.label, side = 3, line = -4, adj = 0.98, col = \"white\", cex = 1.5)\n      }\n      \n      # abline(h = 0, lty = 2)\n      \n      \n    }\n  }, \n    img.name = ifelse(log.y, \"real-spend-pmf-by-week-y-log\", \"real-spend-pmf-by-week-not-y-log\"),\n    navigator = TRUE,\n    htmlfile = ifelse(log.y, \"real-spend-pmf-by-week-y-log.html\", \"real-spend-pmf-by-week-not-y-log.html\"),\n    # htmlfile has no effect for the Quarto book since only pieces of the file are used.\n    autobrowse = FALSE\n    )\n  \n\n  \n}\n\n\n\n\n# CDF\n\n\nfor (log.y in c(FALSE)) {\n  \n  saveHTML({\n    \n    for (i in seq_along(year.week.range) ) {\n      \n      subset.seq &lt;- 1:i\n      subset.seq &lt;- subset.seq[max(1, i - weekly.historical.length + 1):max(subset.seq)]\n      col.palette.subset &lt;- col.palette[max(1, weekly.historical.length - i + 1):weekly.historical.length]\n      col.palette.subset[length(col.palette.subset)] &lt;- \"green\"\n      \n      year.week.range.subset &lt;- year.week.range[subset.seq]\n      \n      par(bg = 'black', fg = 'white', cex = 2)\n      \n      plot(1,\n        main = paste0(\"CDF of estimated real spend age,\\nby week of transaction\"),\n        sub = paste0(\"ISO week \", gsub(\".qs\", \"\", year.week.range[i]), \" (Week starting \",\n          # as.Date(paste0(year.week.range[i], \"-1\"), \"%Y-%U.qs-%u\"), \")\"),\n          ISOweek2date.convert(year.week.range[i]), \")\" ),\n        ylab = ifelse(log.y, \"CDF (log scale)\", \"CDF\"), xlab = \"Age of spent output (log scale)\",\n        col.lab = \"white\", col.axis = \"white\", col.main = \"white\", col.sub = \"white\",\n        col = \"transparent\", log = ifelse(log.y, \"xy\", \"x\"),\n        xlim = (range(support.viz) + 1)* minutes.unit,\n        ylim = ifelse(rep(log.y, 2), c(.0000002, 1), c(0, 1)),\n        xaxt = \"n\", yaxt = \"n\")\n      \n      for ( j in seq_along(subset.seq)) {\n        lines((support.viz[-length(support.viz)]) * minutes.unit,\n          empirical.cdf[[ year.week.range.subset[j] ]]( c(support.viz[-length(support.viz)] + 1)),\n          col = col.palette.subset[j],\n          lwd = ifelse(j == length(subset.seq), 2, 0.5)\n        )\n      }\n      \n      v &lt;- all.weeks.weighted.v.mean[ year.week.range[i] ]\n      z &lt;- support.max\n      \n      status.quo.decoy &lt;- G_star(as.numeric(support.viz))\n      \n      lines(support.viz * minutes.unit, status.quo.decoy, col = \"red\", lwd = 2)\n      \n      cat(i, \" \")\n      \n      legend(\"topleft\", legend = c(\"Decoy status quo\"), fill = c(\"red\"))\n      \n      axis(side = 1, at = c(1/6, 1, 10, 60, 60*12, 60*48),\n        labels = c(\"10 sec\", \"1 min\", \"10 min\", \"1 hour\", \"12 hrs\", \"48 hrs\"),\n        col.axis = \"white\", las = 2, cex.axis = 0.8)\n      \n      axis(side = 2, at = seq(0, 1, by = 0.1), col.axis = \"white\", cex.axis = 0.8, las = 1)\n      axis(side = 4, at = seq(0, 1, by = 0.1), col.axis = \"white\", cex.axis = 0.8, las = 1)\n      \n      if (gsub(\".qs\", \"\", year.week.range[i]) %in% vert.lines$week.iso) {\n        event.label &lt;- vert.lines$label[vert.lines$week.iso == gsub(\".qs\", \"\", year.week.range[i]) ]\n        event.label &lt;- paste0(\"Event: \", gsub(\" [(].*[)]\", \"\", event.label))\n        # y.event.pos &lt;- ifelse(rep(log.y, 2), c(.0000002, 0.01), c(0, 0.005))\n        mtext(event.label, side = 3, line = -4, adj = 0.02, col = \"white\", cex = 1.5)\n      }\n      \n      # abline(h = 0, lty = 2)\n      \n      \n    }\n  }, \n    img.name = ifelse(log.y, \"real-spend-cdf-by-week-y-log\", \"real-spend-cdf-by-week-not-y-log\"),\n    navigator = TRUE,\n    htmlfile = ifelse(log.y, \"real-spend-cdf-by-week-y-log.html\", \"real-spend-cdf-by-week-not-y-log.html\"),\n    # htmlfile has no effect for the Quarto book since only pieces of the file are used.\n    autobrowse = FALSE\n    )\n  \n\n}\n\n\n\n\n\nKrawiec-Thayer, Mitchell P., Neptune, Rucknium, Jberman, and Carrington. 2021. “Fingerprinting a Flood: Forensic Statistical Analysis of the Mid-2021 Monero Transaction Volume Anomaly.” https://mitchellpkt.medium.com/fingerprinting-a-flood-forensic-statistical-analysis-of-the-mid-2021-monero-transaction-volume-a19cbf41ce60.\n\n\nMakarov, Igor, and Antoinette Schoar. 2021. “Blockchain Analysis of the Bitcoin Market.” Working Paper 29396. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w29396.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real Spend Visualization</span>"
    ]
  },
  {
    "objectID": "parametric-fit.html",
    "href": "parametric-fit.html",
    "title": "12  Parametric Fit",
    "section": "",
    "text": "12.1 Objective function\nHaving estimated the nonparametric PMF, we now need to fit a parametric distribution.\nFirst, the weekly PMFs must be aggregated. The value of the weekly PMFs at each support point can simply be averaged. Some weeks are excluded because of data availability problems or exogenous shocks:\nAs stated in Section 11.2 , the mixing proportion of the second-largest component distribution was estimated to be much larger during some of the exogenous events, suggesting inappropriate splitting of the wallet2 ring distribution. In theory, the first and second estimated components could be re-combined by computing their weighted sum, with the estimated mixing proportions as their weights. The simpler solution is to exclude these weeks as anomalous, which is what I have done.\nUsually, fitting a parametric distribution to nonparametric data involves minimization or maximization of an objective function by adjusting the parametric distribution’s parameters until the best fit is achieved. In OSPEAD, the new decoy parametric probability distribution will be chosen by minimizing the probability that the Maximum A Posteriori (MAP) Decoder Attack described by Aeeneh et al. (2021) is successful. Let ZZ be the total number of outputs eligible to be spent in a RingCT ring. Let fSf_S be the real spend probability mass function (PMF). Let fDf_D be a proposed decoy PMF. Let 𝟏{}\\mathbf{1}\\{\\} be the indicator function that takes value 1 when the statement within the braces is true and 0 otherwise. The average success probability of the MAP Decoder attack is\nLMAPDecoder=∑i=1ZfS(i)(∑j=1ZfD(j)𝟏{fS(j)fD(j)&lt;fS(i)fD(i)})M(12.1)\nL_{MAP\\:Decoder}=\\sum_{i=1}^{Z}f_{S}\\left(i\\right)\\left(\\sum_{j=1}^{Z}f_{D}\\left(j\\right)\\mathbf{1}\\left\\{ \\tfrac{f_{S}\\left(j\\right)}{f_{D}\\left(j\\right)}&lt;\\tfrac{f_{S}\\left(i\\right)}{f_{D}\\left(i\\right)}\\right\\} \\right)^{M}\n \\qquad(12.1)\nThe fS(i)f_{S}\\left(i\\right) weights the attack success probability by the probability mass at the iith output. Very old outputs are rarely spent, so they are given low weight. Equation 12.1 can be modified to give more weight to old outputs. The fS(i)f_{S}\\left(i\\right) weight can be changed into a convex combination of fS(i)f_{S}\\left(i\\right) and giving each output an equal weight, parameterized by λ\\lambda: (λfS(i)+(1−λ)1Z)\\left(\\lambda f_{S}\\left(i\\right)+(1-\\lambda)\\frac{1}{Z}\\right). The more general objective function is\nLMAPDecoder(λ)=∑i=1Z(λfS(i)+(1−λ)1Z)∑j=1Z(fD(j)𝟏{fS(j)fD(j)&lt;fS(i)fD(i)})M(12.2)\nL_{MAP\\:Decoder}(\\lambda)=\\sum_{i=1}^{Z}\\left(\\lambda f_{S}\\left(i\\right)+(1-\\lambda)\\frac{1}{Z}\\right)\\sum_{j=1}^{Z}\\left(f_{D}\\left(j\\right)\\mathbf{1}\\left\\{ \\tfrac{f_{S}\\left(j\\right)}{f_{D}\\left(j\\right)}&lt;\\tfrac{f_{S}\\left(i\\right)}{f_{D}\\left(i\\right)}\\right\\} \\right)^{M}\n \\qquad(12.2)\nWe shall use λ=1\\lambda=1 and λ=0.5\\lambda=0.5.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parametric Fit</span>"
    ]
  },
  {
    "objectID": "parametric-fit.html#sec-parametric-distributions",
    "href": "parametric-fit.html#sec-parametric-distributions",
    "title": "12  Parametric Fit",
    "section": "12.2 Parametric distributions",
    "text": "12.2 Parametric distributions\nThe following parametric distributions will be fit:\n\nGamma\nNon-central F\nRight-Pareto Lognormal (RPLN) (Reed and Jorgensen 2004)\nGeneralized Extreme Value (GEV)\nBirnbaum-Saunders (BS)\nGeneralized Beta of the Second Kind (GB2)\n\nEach distribution will be fitted on the raw nonparametric distribution and a log transformation of it.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parametric Fit</span>"
    ]
  },
  {
    "objectID": "parametric-fit.html#implementation-details",
    "href": "parametric-fit.html#implementation-details",
    "title": "12  Parametric Fit",
    "section": "12.3 Implementation details",
    "text": "12.3 Implementation details\n\n12.3.1 Computational expense\nNotice the double summation over ZZ in Equation 12.1 . ZZ is over 100 million. A naive implementation of Equation 12.1 would require over 10 quadrillion comparisons. Furthermore, Equation 12.1 needs to be evaluated several hundred times during the numerical optimization algorithm. The computational issues were handled by:\n\nDeveloping a fast implementation of Equation 12.1 that leverages sorting logic. The implementation is in the code section as map.decoder.success.prob() below.\nSelecting only 10 percent of all outputs as a sample instead of the entire population. The probability of the iith output being selected is proportional to the probability mass fS(i)f_S(i). About 96 percent of the total probability mass is included because high-probability outputs are more likely to be selected.\n\n\n\n12.3.2 Excess tail penalty\nDuring fitting, the probability mass that is older than the oldest output is added back to the rest of the probability distribution. This procedure reflects the fact that a wallet would re-draw a random output that is older than the oldest RingCT output. During fitting, I sometimes observed excessive “flattening” of the probability distribution where the portion of the distribution that extended past the oldest output exceeded 70 percent. There was a need to discourage the optimizer from flattening the distribution.\nI added an excess tail penalty. When 10 percent or more of the distribution’s mass extends past the oldest output, a proportional penalty is added to the optimizer’s objective function.\n\n\n12.3.3 Parameter initial values\nI use the Nelder-Mead algorithm to minimize the objective functions. Nelder-Mead, like most optimization algorithms, needs initial parameter values to start the optimization procedure. The starting values are obtained by drawing 10,000 observations from the real spend distribution and estimating the appropriate maximum likelihood model parameter estimates. Of course, the maximum likelihood methods need starting values. See the documentation of the maximum likelihood R functions in the code below for details. There was no automatic method to select starting values for the noncentral F distribution. In that case, I manually specified reasonable starting values.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parametric Fit</span>"
    ]
  },
  {
    "objectID": "parametric-fit.html#sec-parametric-fit-code",
    "href": "parametric-fit.html#sec-parametric-fit-code",
    "title": "12  Parametric Fit",
    "section": "12.4 Code",
    "text": "12.4 Code\nThis code can be run in a new R session. It may take a day or two to run on a powerful machine.\n\nlibrary(data.table)\n\n\nresults.dir &lt;- \"results\"\n\n\nexcl.weeks &lt;- c(\"2023-13.qs\", \"2023-14.qs\", \"2023-51.qs\", \"2023-52.qs\", \"2024-01.qs\", \"2024-02.qs\",\n  \"2024-06.qs\", \"2024-09.qs\", \"2024-10.qs\", \"2024-11.qs\", \"2024-12.qs\", \"2024-13.qs\", \"2024-14.qs\",\n  \"2024-15.qs\", \"2024-16.qs\", \"2024-17.qs\", \"2024-19.qs\")\n\nlambda.params &lt;- c(1, 0.5)\n\n\nresults.dir.run &lt;- paste0(results.dir, \"/results-01/\")\n\n\nnonparametric.real.spend &lt;- qs::qread(file = paste0(results.dir.run, \"nonparametric-real-spend.qs\"))\n\nnonparametric.real.spend &lt;- nonparametric.real.spend$rucknium\n\n\nweekly.real.spend.cdfs &lt;- nonparametric.real.spend$weekly.real.spend.cdfs\n\nall.weeks.weighted.v.mean &lt;- nonparametric.real.spend$all.weeks.weighted.v.mean\n\nweighted.v.mean &lt;- mean(all.weeks.weighted.v.mean)\n\nsupport.max &lt;- nonparametric.real.spend$support.max\n\n\n\nall.weeks.weighted.v.mean &lt;- all.weeks.weighted.v.mean[! names(all.weeks.weighted.v.mean) %in% excl.weeks]\n\naggregate.real.spend.pmf &lt;- diff(weekly.real.spend.cdfs[[1]](as.numeric(0:(support.max + 1))))\n\nfor (week in setdiff(names(weekly.real.spend.cdfs)[-1], excl.weeks)) {\n  cat(week, \"\\n\")\n  aggregate.real.spend.pmf &lt;- aggregate.real.spend.pmf +\n    diff(weekly.real.spend.cdfs[[week]](as.numeric(0:(support.max + 1))))\n}\n\naggregate.real.spend.pmf &lt;- aggregate.real.spend.pmf/sum(aggregate.real.spend.pmf)\n\nstopifnot(all.equal(1, sum(aggregate.real.spend.pmf)))\n\n\n\n\nget.decoy.pmf &lt;- function(cdf, v, z, sub.supp, log.trans = FALSE, ...) {\n  \n  G &lt;- function(x) {\n    cdf(x, ...)\n  }\n  \n  if (log.trans) {\n    G_star &lt;- function(x) {\n      G(log1p(x*v))/G(log1p(z*v))\n    }\n  } else {\n    G_star &lt;- function(x) {\n      G(x*v)/G(z*v)\n    }\n  }\n  \n  G_star(sub.supp) - G_star(sub.supp - 1)\n  \n}\n\n\n\n\n\nparam.trans &lt;- list()\n\nf_D.lgamma &lt;- function(param, v, z, sub.supp, get.decoy.pmf) {\n  list(decoy.pmf =  get.decoy.pmf(actuar::plgamma, v, z, sub.supp = sub.supp, shapelog = exp(param[1]), ratelog = exp(param[2])),\n    tail.beyond.support = 1 - actuar::plgamma(z*v, shapelog = exp(param[1]), ratelog = exp(param[2])))\n}\n\nparam.trans$lgamma &lt;- c(exp, exp)\n\nf_D.gamma &lt;- function(param, v, z, sub.supp, get.decoy.pmf, log.trans) {\n  list(decoy.pmf =  get.decoy.pmf(stats::pgamma, v, z, sub.supp = sub.supp, log.trans = log.trans, shape = exp(param[1]), rate = exp(param[2])),\n    tail.beyond.support = 1 - stats::pgamma(ifelse(log.trans, log1p(z*v), z*v), shape = exp(param[1]), rate = exp(param[2])))\n}\n\nparam.trans$gamma &lt;- c(exp, exp)\n\n\nf_D.f &lt;- function(param, v, z, sub.supp, get.decoy.pmf, log.trans) {\n  list(decoy.pmf =  get.decoy.pmf(cdf = stats::pf, v, z, sub.supp, log.trans = log.trans, df1 = exp(param[1]), df2 = exp(param[2]), ncp = exp(param[3])),\n    tail.beyond.support = 1 - stats::pf(ifelse(log.trans, log1p(z*v), z*v),  df1 = exp(param[1]), df2 = exp(param[2]), ncp = exp(param[3])))\n}\n\nparam.trans$f &lt;- c(exp, exp, exp)\n\n\nf_D.rpln &lt;- function(param, v, z, sub.supp, get.decoy.pmf, log.trans, return.log = FALSE, ...) {\n  list(decoy.pmf =  get.decoy.pmf(distributionsrd::prightparetolognormal, v, z, sub.supp, log.trans = log.trans, shape2 = exp(param[1]),\n    meanlog = param[2], sdlog = exp(param[3]), lower.tail = TRUE, log.p = FALSE),\n    tail.beyond.support = 1 - distributionsrd::prightparetolognormal(ifelse(log.trans, log1p(z*v), z*v), shape2 = exp(param[1]), meanlog = param[2], sdlog = exp(param[3]), lower.tail = TRUE, log.p = FALSE))\n}\n\n\nparam.trans$rpln &lt;- c(exp, I, exp)\n\nf_D.gev &lt;- function(param, v, z, sub.supp, get.decoy.pmf, log.trans) {\n  list(decoy.pmf =  get.decoy.pmf(VGAM::pgev, v, z, sub.supp = sub.supp, log.trans = log.trans, location = param[1], scale = exp(param[2]), shape = param[3]),\n    tail.beyond.support = 1 - VGAM::pgev(ifelse(log.trans, log1p(z*v), z*v), location = param[1], scale = exp(param[2]), shape = param[3]))\n}\n\nparam.trans$gev &lt;- c(I, exp, I)\n\n\nf_D.bs &lt;- function(param, v, z, sub.supp, get.decoy.pmf, log.trans) {\n  list(decoy.pmf =  get.decoy.pmf(cdf = bsgof::pbs, v, z, sub.supp, log.trans = log.trans, alpha = exp(param[1]), beta = exp(param[2])),\n    tail.beyond.support = 1 - bsgof::pbs(ifelse(log.trans, log1p(z*v), z*v), alpha = exp(param[1]), beta = exp(param[2])))\n}\n\nparam.trans$bs &lt;- c(exp, exp)\n\n\n\nf_D.gb2 &lt;- function(param, v, z, sub.supp, get.decoy.pmf, log.trans) {\n  list(decoy.pmf =  get.decoy.pmf(cdf = GB2::pgb2, v, z, sub.supp, log.trans = log.trans, shape1 = exp(param[1]), scale = exp(param[2]), shape2 = exp(param[3]), shape3 = exp(param[4])),\n    tail.beyond.support = 1 - GB2::pgb2(ifelse(log.trans, log1p(z*v), z*v), shape1 = exp(param[1]), scale = exp(param[2]), shape2 = exp(param[3]), shape3 = exp(param[4])))\n}\n\nparam.trans$gb2 &lt;- c(exp, exp, exp, exp)\n\n\nmap.decoder.success.prob &lt;- function(f_S, f_D) {\n  # This implementation assumes that every output selected in a ring\n  # is unique (i.e. does not handle multi-output aggregates like \n  # pseudo-blocks). The inequality comparison is strict.\n  \n  cut.vector &lt;- f_S/f_D\n  rm(f_S)\n  names(cut.vector) &lt;- as.character(1:length(cut.vector))\n  \n  y &lt;- data.table(f_D = f_D, cut.vector.var = cut.vector)\n  \n  setorder(y, cut.vector.var)\n  \n  cut.vector &lt;- sort(cut.vector)\n  cut.vector.unique &lt;- cut.vector[!duplicated(cut.vector)]\n  # Using duplicated() keeps names. unique() does not keep names.\n  cut.vector.name.unique &lt;- as.integer(names(cut.vector.unique))\n  \n  y[, cut.vector.cut := cut.vector.name.unique[ .bincode(cut.vector.var, c(-1, cut.vector.unique), right = FALSE)] ]\n  \n  setorder(y, cut.vector.var)\n  y[, cut.vector.var := NULL]\n  \n  y[, f_D := cumsum(f_D)]\n  rm(f_D)\n  y &lt;- y[, .(success.prob = f_D[.N]), by = cut.vector.cut]\n  \n  y &lt;- merge(y, data.table(cut.vector.cut = cut.vector.name.unique, cut.vector = cut.vector.unique))\n  rm(cut.vector.name.unique)\n  y[, cut.vector.cut := NULL]\n  \n  y &lt;- merge(y, data.table(cut.vector.cut.names = as.integer(names(cut.vector)), cut.vector = unname(cut.vector)),\n    all = TRUE, by = \"cut.vector\")\n  y[, cut.vector := NULL]\n  setorder(y, cut.vector.cut.names)\n  y[, cut.vector.cut.names := NULL]\n  \n  setDF(y)\n  \n  y$success.prob[is.na(y$success.prob)] &lt;- 0\n  # At the point(s) where f_S/f_D is at a minimum, the attack would always\n  # choose another block height, so the attack success probability is zero\n  \n  y &lt;- y$success.prob\n  \n  # gc()\n  \n  y\n  \n}\n\n\nL_Attack &lt;- function(param, f_D, v, z, sub.supp, theta_i, get.decoy.pmf, map.decoder.success.prob, lambda = 1, log.trans) {\n  \n  f_D.return &lt;- f_D(param, v, z, sub.supp, get.decoy.pmf, log.trans)\n  \n  a_i &lt;- f_D.return$decoy.pmf\n  tail.penalty &lt;- f_D.return$tail.beyond.support\n  rm(f_D.return)\n  \n  if (any(! is.finite(a_i))) { return(1) }\n  a_i[a_i &lt;= 0] &lt;- .Machine$double.eps\n  \n  values.map.decoder.success.prob &lt;- map.decoder.success.prob(f_S = theta_i/sum(theta_i), f_D = a_i/sum(a_i))\n  \n  n.decoys &lt;- 15\n  \n  cat(round(tail.penalty, 5), \"&lt;&gt;\")\n  tail.penalty &lt;- tail.penalty - 0.10 # 0.05\n  \n  if (tail.penalty &lt;= 0) {tail.penalty &lt;- 0}\n  \n  stopifnot(length(values.map.decoder.success.prob) == length(theta_i))\n  \n  weight &lt;- lambda * (theta_i/sum(theta_i)) + (1 - lambda) * 1/length(theta_i)\n  \n  sum(weight * (values.map.decoder.success.prob)^n.decoys) + tail.penalty\n  \n}\n\n\ntheta_i &lt;- aggregate.real.spend.pmf\ntheta_i[theta_i == 0] &lt;- .Machine$double.eps\n\nz &lt;- length(theta_i)\nv &lt;- mean(all.weeks.weighted.v.mean)\n\nset.seed(314)\nsub.supp &lt;- wrswoR::sample_int_expj(length(theta_i), ceiling(length(theta_i)/10), prob = theta_i)\n# wrswoR::sample_int_expj is much faster than sample() when the prob vector is long\n# start at 2 so G(x - 1) is not 0\nsub.supp &lt;- sort(sub.supp)\ntheta_i &lt;- theta_i[sub.supp]\n\n\ngen.data &lt;- stepfun( cumsum(theta_i/sum(theta_i)), c(0, as.numeric(sub.supp)) )\n\n\nmle.generated.data &lt;- gen.data(runif(10000))\nmle.generated.data &lt;- mle.generated.data[mle.generated.data &gt; 0]\n\n\ngamma.fit &lt;- Rfast::gammamle(mle.generated.data)\ngamma.fit.coef &lt;- log(gamma.fit$param)\n\ngamma.fit &lt;- Rfast::gammamle(log1p(mle.generated.data))\ngamma.fit.coef.log &lt;- log(gamma.fit$param)\n\n\nf.fit &lt;- fitdistrplus::fitdist(mle.generated.data, \"f\", method = \"mle\", start = list(df1 = exp(-1), df2 = exp(0), ncp = exp(2)))\nf.fit.coef &lt;- log(coef(f.fit))\n\nf.fit &lt;- fitdistrplus::fitdist(log1p(mle.generated.data), \"f\", method = \"mle\", start = list(df1 = exp(-1), df2 = exp(0), ncp = exp(2)))\nf.fit.coef.log &lt;- log(coef(f.fit))\n\n\nrpln.fit &lt;- distributionsrd::rightparetolognormal.mle(mle.generated.data)\nrpln.fit.coef &lt;- c(log(rpln.fit$coefficients[[\"shape2\"]]), rpln.fit$coefficients[[\"meanlog\"]],\n  log(rpln.fit$coefficients[[\"sdlog\"]]) )\n\nrpln.fit &lt;- distributionsrd::rightparetolognormal.mle(log1p(mle.generated.data))\nrpln.fit.coef.log &lt;- c(log(rpln.fit$coefficients[[\"shape2\"]]), rpln.fit$coefficients[[\"meanlog\"]],\n  log(rpln.fit$coefficients[[\"sdlog\"]]) )\n\n\ngev.fit &lt;- VGAM::vglm(mle.generated.data ~ 1, VGAM::gevff, control = VGAM::vglm.control(maxit = 1000, trace = TRUE))\ngev.fit.coef &lt;- VGAM::Coef(gev.fit)\ngev.fit.coef[2] &lt;- log(gev.fit.coef[2])\n\ngev.fit &lt;- VGAM::vglm(log1p(mle.generated.data) ~ 1, VGAM::gevff, control = VGAM::vglm.control(maxit = 1000, trace = TRUE))\ngev.fit.coef.log &lt;- VGAM::Coef(gev.fit)\ngev.fit.coef.log[2] &lt;- log(gev.fit.coef.log[2])\n\n\nbs.fit &lt;- bsgof::bs.mle(mle.generated.data)\nbs.fit.coef &lt;- log(c(bs.fit$alpha, bs.fit$beta))\n\nbs.fit &lt;- bsgof::bs.mle(log1p(mle.generated.data))\nbs.fit.coef.log &lt;- log(c(bs.fit$alpha, bs.fit$beta))\n\ngb2.fit &lt;- GB2::mlfit.gb2(mle.generated.data)\ngb2.fit.coef &lt;- log(gb2.fit[[2]]$par)\n\ngb2.fit &lt;- GB2::mlfit.gb2(log1p(mle.generated.data + 1))\ngb2.fit.coef.log &lt;- log(gb2.fit[[2]]$par)\n\n\nrun.iters.simple &lt;- expand.grid(\n  f_D = c(\n    f_D.gamma = f_D.gamma,\n    f_D.f = f_D.f,\n    f_D.rpln = f_D.rpln,\n    f_D.gev = f_D.gev,\n    f_D.bs = f_D.bs,\n    f_D.gb2 = f_D.gb2\n  ),\n  flavor = 1,\n  week = 0,\n  est.method = 0,\n  L = c(L_Attack = L_Attack),\n  lambda = lambda.params,\n  log.trans = c(FALSE, TRUE)\n)\n\n\n\nstart.params &lt;- list(\n  f_D.gamma = list(no.log = gamma.fit.coef, log = gamma.fit.coef.log),\n  f_D.f = list(no.log = f.fit.coef, log = f.fit.coef.log),\n  f_D.rpln = list(no.log = rpln.fit.coef, log = rpln.fit.coef.log),\n  f_D.gev = list(no.log = gev.fit.coef, log = gev.fit.coef.log),\n  f_D.bs = list(no.log = bs.fit.coef, log = bs.fit.coef.log),\n  f_D.gb2 = list(no.log = gb2.fit.coef, log = gb2.fit.coef.log))\n\n\n\n\n\nthreads &lt;- floor(nrow(run.iters.simple)/2)\nfuture::plan(future::multisession(workers = threads))\n\nkeep.time &lt;- base::date()\n\nfit.results &lt;- future.apply::future_lapply(1:nrow(run.iters.simple), function(iter) {\n  \n  f_D.fun &lt;- run.iters.simple$f_D[[iter]]\n  L.fun &lt;- run.iters.simple$L[[iter]]\n  flavor &lt;- run.iters.simple$flavor[[iter]]\n  lambda &lt;- run.iters.simple$lambda[[iter]]\n  log.trans &lt;- run.iters.simple$log.trans[[iter]]\n  if (log.trans) {\n    start.params.optim &lt;- start.params[[names(run.iters.simple$f_D[iter])]]$log\n  } else {\n    start.params.optim &lt;- start.params[[names(run.iters.simple$f_D[iter])]]$no.log\n  }\n  \n  \n  optim.results &lt;- optim(\n    start.params.optim,\n    L.fun,\n    f_D = f_D.fun,\n    v = v,\n    z = z,\n    sub.supp = sub.supp,\n    get.decoy.pmf = get.decoy.pmf,\n    map.decoder.success.prob = map.decoder.success.prob,\n    log.trans = log.trans,\n    lambda = lambda, \n    method = \"Nelder-Mead\", \n    control = list(trace = 6, maxit = 10000),\n    theta_i = theta_i)\n  \n  optim.results\n  \n}, future.globals = c(\"run.iters.simple\", \"start.params\", \"v\", \"z\", \"sub.supp\", \"get.decoy.pmf\",\n  \"map.decoder.success.prob\", \"theta_i\", \"lamba.iter\"),\n  future.packages = \"data.table\", future.seed = TRUE)\n\nprint(keep.time)\nbase::date()\n\n\n\n\n\nAeeneh, Sina, João Otávio Chervinski, Jiangshan Yu, and Nikola Zlatanov. 2021. “New Attacks on the Untraceability of Transactions in CryptoNote-Style Blockchains.” In 2021 IEEE International Conference on Blockchain and Cryptocurrency (ICBC), 1–5. https://doi.org/10.1109/ICBC51069.2021.9461130.\n\n\nReed, William J., and Murray Jorgensen. 2004. “The Double Pareto-Lognormal Distribution—a New Parametric Model for Size Distributions.” Communications in Statistics - Theory and Methods 33 (8): 1733–53. https://doi.org/10.1081/STA-120037438.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parametric Fit</span>"
    ]
  },
  {
    "objectID": "performance-evaluation.html",
    "href": "performance-evaluation.html",
    "title": "13  Performance Evaluation",
    "section": "",
    "text": "13.1 Attack success probability\nWith the parametric fit done in Chapter 12 , we can evaluate how well the parametric distributions would protect users against the MAP Decoder attack.\nWith ring size 16, the minimum possible attack success probability is 1/16=6.25%1/16=6.25\\%.\nTable 13.1 shows the top six best distributions, the average attack success probability computed from Equation 12.1 , and the fitted parameter values. Note that some of the distributions have the same names for parameters, but they have different meanings for each parametric distribution. Consult the sources in Section 12.2 and the documentation for the distributions’ R implementations in Section 12.4 for their meanings.\nThe lowest attack success probability is achieved by Log-GB2, the log transformation of the generalized beta distribution of the second kind. If Monero’s default decoy selection algorithm were Log-GB2 with the specified parameter values, an adversary applying the MAP decoder attack would have successfully guessed the real spend in 7.6 percent of rings, on average, since the August 2022 hard fork. That corresponds to an effective ring size of 13.2.\nThe last row of Table 13.1 shows the estimated average attack success probability that an adversary can achieve against real Monero users who were using the default wallet2 decoy selection algorithm, since the August 2022 hard fork: 23.5 percent. This corresponds to an effective ring size of 4.2.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Performance Evaluation</span>"
    ]
  },
  {
    "objectID": "performance-evaluation.html#attack-success-probability",
    "href": "performance-evaluation.html#attack-success-probability",
    "title": "13  Performance Evaluation",
    "section": "",
    "text": "Table 13.1: Attack success probability with fitted parametric decoy distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecoy distribution\nAttack success probability\nParam 1\nParam 2\nParam 3\nParam 4\n\n\n\n\nLog-GB2\n7.590%\nshape1 = 4.462\nscale = 20.62\nshape2 = 0.5553\nshape3 = 7.957\n\n\nLog-GEV\n7.730%\nlocation = 8.382\nscale = 3.851\nshape = -0.3404\n\n\n\nRPLN\n7.746%\nshape2 = 6.684\nmeanlog = 9.725\nsdlog = 3.964\n\n\n\nGB2\n7.794%\nshape1 = 0.1282\nscale = 4753\nshape2 = 8.582\nshape3 = 7.179\n\n\nLog-F\n8.212%\ndf1 = 1.678\ndf2 = 9.067e+06\nncp = 17.06\n\n\n\nLog-Gamma\n8.470%\nshape = 4.315\nrate = 0.3751\n\n\n\n\nDecoy status quo\n23.540%\nshape = 19.28\nrate = 1.61",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Performance Evaluation</span>"
    ]
  },
  {
    "objectID": "performance-evaluation.html#visualizations-of-fitted-distributions",
    "href": "performance-evaluation.html#visualizations-of-fitted-distributions",
    "title": "13  Performance Evaluation",
    "section": "13.2 Visualizations of fitted distributions",
    "text": "13.2 Visualizations of fitted distributions\nFigure 13.1 compares the probability mass functions of the top three fitted decoy distributions, the status quo decoy distribution, and the real spend distribution. I included the CDF of the real spend on the bottom panel because PMFs on a log scale can appear misleading.\nOutputs in the first two minutes would usually be included in a single block. Therefore, variations in the probability mass within those two minutes are not very meaningful.\nThe figure shows that the probability mass of the status quo decoy distribution is much lower than the real spend. Therefore, the status quo decoy distribution is not doing a good job of protecting the privacy of users who spend young outputs.\n\n\n\n\nFigure 13.1\n\n\n\n\n\n\n\nFigure 13.2 shows the PMFs of the top 4-6 distributions from Table 13.1 .\n\n\n\n\nFigure 13.2\n\n\n\n\n\n\n\nFigure 13.3 is the same plot as Figure 13.1 , except the vertical axis is log scale. The log scale makes it easier to see the differences in the PMFs for older outputs.\n\n\n\n\nFigure 13.3\n\n\n\n\n\n\n\nFigure 13.4 is the same plot as Figure 13.2 , except the vertical axis is log scale.\n\n\n\n\nFigure 13.4",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Performance Evaluation</span>"
    ]
  },
  {
    "objectID": "performance-evaluation.html#attack-success-probability-by-output-age",
    "href": "performance-evaluation.html#attack-success-probability-by-output-age",
    "title": "13  Performance Evaluation",
    "section": "13.3 Attack success probability by output age",
    "text": "13.3 Attack success probability by output age\nThe MAP decoder attack success probability varies with the age of the spent output. Where the decoy PMF is below the real spend PMF, the attack success probability is high. Where the decoy PMF is above the real spend PMF, the attack success probability is low. Figure 13.5 shows the attack success probability by output age for the top three fitted decoy distributions and the status quo decoy distribution.\nThere are four regions where the fitted decoy distributions do a poor job of protecting the real spend. The first occurs at about 10-20 seconds of age. As mentioned above, in practice outputs from zero to two minutes of age will be aggregated together, so this region is not of much concern.\nThe second region is 30 minutes to one hour. Notice that 20 minutes coincides with the “cliff” in the status quo decoy distribution that forms because the unspendable portion of the log-gamma distribution before the 10 block lock is redistributed to the early part of the spendable distribution. The 101-point resolution of the BJR estimator may have had a difficult time with this cliff in the decoy distribution because it is not smooth. The BJR estimator may be overestimating the thickness of the real spend distribution in this region. In other words, this region of high attack success may be a false reading.\nThe third region extends from the second day to the second week of age.\nThe fourth region occurs after one year of age. Notice that the log-GB2 and the log-GEV both have high attack success probability in this region, but the RPLN distribution does not. This example shows that the distribution family does affect the regions of the age distribution that would be most susceptible to the MAP Decoder attack, even when the average attack success probability, shown in Table 13.1 , is similar between the distributions. Note also that the CDF shows that the share of rings spending outputs older than one year is very small.\n\n\n\n\nFigure 13.5\n\n\n\n\n\n\n\nFigure 13.6 shows the same data as Figure 13.5 , except for the top 4-6 fitted distributions.\n\n\n\n\nFigure 13.6",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Performance Evaluation</span>"
    ]
  },
  {
    "objectID": "performance-evaluation.html#attack-success-probability-for-different-λ-weights",
    "href": "performance-evaluation.html#attack-success-probability-for-different-λ-weights",
    "title": "13  Performance Evaluation",
    "section": "13.4 Attack success probability for different λ weights",
    "text": "13.4 Attack success probability for different λ weights\nWe also fitted decoy distributions with different λ weights in Equation 12.2 . The mean attack success probability of these fitted distributions with λ = 0.5 in Table 13.2 are higher than the λ = 1 fitted distributions, which is completely expected. The λ = 0.5 fitted distributions are optimizing to a different objective function, so the value of the λ = 1 objective function is not as good. However, the “loss” of the objective function is not very large, at around 0.2 percentage points for each of them.\n\n\n\n\nTable 13.2: Attack success probability, λ = 1, 0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecoy distribution\nλ\nAttack success probability\nParam 1\nParam 2\nParam 3\nParam 4\n\n\n\n\nLog-GB2\n1.0\n7.590%\nshape1 = 4.462\nscale = 20.62\nshape2 = 0.5553\nshape3 = 7.957\n\n\nLog-GB2\n0.5\n7.783%\nshape1 = 4.631\nscale = 19.76\nshape2 = 0.5308\nshape3 = 6.235\n\n\n\n\n\n\n\n\n\n\n\nLog-GEV\n1.0\n7.730%\nlocation = 8.382\nscale = 3.851\nshape = -0.3404\n\n\n\nLog-GEV\n0.5\n7.841%\nlocation = 8.608\nscale = 3.956\nshape = -0.3369\n\n\n\n\n\n\n\n\n\n\n\n\nRPLN\n1.0\n7.746%\nshape2 = 6.684\nmeanlog = 9.725\nsdlog = 3.964\n\n\n\nRPLN\n0.5\n7.981%\nshape2 = 6.121\nmeanlog = 9.977\nsdlog = 4.064\n\n\n\n\n\n\n\n\n\n\n\n\nGB2\n1.0\n7.794%\nshape1 = 0.1282\nscale = 4753\nshape2 = 8.582\nshape3 = 7.179\n\n\nGB2\n0.5\n7.984%\nshape1 = 0.05374\nscale = 8.369e+04\nshape2 = 40.93\nshape3 = 43.62\n\n\n\n\n\n\n\n\n\n\n\nLog-F\n1.0\n8.212%\ndf1 = 1.678\ndf2 = 9.067e+06\nncp = 17.06\n\n\n\nLog-F\n0.5\n8.463%\ndf1 = 1.563\ndf2 = 3.555e+07\nncp = 16.68\n\n\n\n\n\n\n\n\n\n\n\n\nLog-Gamma\n1.0\n8.470%\nshape = 4.315\nrate = 0.3751\n\n\n\n\nLog-Gamma\n0.5\n8.622%\nshape = 4.517\nrate = 0.3892\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecoy status quo\n\n23.540%\nshape = 19.28\nrate = 1.61\n\n\n\n\n\n\n\n\n\nFigure 13.7 shows the attack success probability for the fitted GB2 decoy distributions when λ = 1 and λ = 0.5. One can see that the λ = 0.5 distribution more heavily weights the older outputs, as expected. That distribution has lower attack success probability against the older outputs than the λ = 1 distribution. Of course, the lower attack success probability there comes at the cost of having higher attack success probability against younger outputs.\nEquivalent plots for the log-GEV and RPLN distribution are in Figure 13.8 and Figure 13.9 .\n\n\n\n\nFigure 13.7\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.8\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.9",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Performance Evaluation</span>"
    ]
  },
  {
    "objectID": "performance-evaluation.html#code",
    "href": "performance-evaluation.html#code",
    "title": "13  Performance Evaluation",
    "section": "13.5 Code",
    "text": "13.5 Code\n\nlibrary(ggplot2)\nlibrary(dRacula)\n\n\nthreads &lt;- 1\n# Best to have one thread when do.sub.support == FALSE and when RAM is limited\nfuture::plan(future::multisession(workers = threads))\n\noptions(future.globals.maxSize = 8000*1024^2)\n\n\n\n# Some of this code could be re-factored to improve categorical selections.\n\n\nanalysis.subset &lt;- order(sapply(fit.results, FUN = function(x) x$value))\nanalysis.subset &lt;- setdiff(analysis.subset, which(run.iters.simple$lambda != 1))\n# Remove results where lambda does not equal 1\nanalysis.subset &lt;- analysis.subset[1:6]\n# Get top 6\n\nanalysis.subset.names &lt;- gsub(\"f_D.\", \"\", names(run.iters.simple$f_D))\n\nanalysis.subset.names &lt;- ifelse(analysis.subset.names == \"gamma\", \"Gamma\", toupper(analysis.subset.names))\nanalysis.subset.names[run.iters.simple$log.trans] &lt;- paste0(\"Log-\", analysis.subset.names[run.iters.simple$log.trans])\nanalysis.subset.names &lt;- analysis.subset.names\n\nlow.lambda.subset &lt;- which(run.iters.simple$lambda == 0.5 & \n    analysis.subset.names %in% analysis.subset.names[analysis.subset])\n\nanalysis.subset &lt;- c(analysis.subset, low.lambda.subset)\n# Add the lambda == 0.5 ones that correspond to the lambda == 1 \"best\" distributions\n\nanalysis.subset &lt;- c(analysis.subset, nrow(run.iters.simple) + 1)\nanalysis.subset.names &lt;- c(analysis.subset.names, \"Decoy status quo\")\n# Last one is status quo decoy\n\n\nkeep.time &lt;- base::date()\n\n\nperformance.fit.results &lt;- future.apply::future_lapply((1:(nrow(run.iters.simple)+1))[analysis.subset], function(iter) {\n  \n  GAMMA_SHAPE = 19.28\n  GAMMA_RATE = 1.61\n  \n  G &lt;- function(x) {\n    actuar::plgamma(x, shapelog = GAMMA_SHAPE, ratelog = GAMMA_RATE)\n  }\n  \n  G_star &lt;- function(x) {\n    (0 &lt;= x*v & x*v &lt;= 1800) *\n      (G(x*v + 1200) - G(1200) +\n          ( (x*v)/(1800) ) * G(1200)\n      )/G(z*v) +\n      (1800 &lt; x*v & x*v &lt;= z*v) * G(x*v + 1200)/G(z*v) +\n      (z*v &lt; x*v) * 1\n  }\n  \n  \n  summary.performance &lt;- data.table(new.decoy = 0)\n  \n  \n  do.sub.support &lt;- FALSE\n  \n  do.current.decoy &lt;- iter == nrow(run.iters.simple) + 1\n  \n  guess.prob.each.output &lt;- vector(\"numeric\", z)\n  \n  \n  weekly.z.max &lt;- z\n  \n  \n  theta_i &lt;- aggregate.real.spend.pmf\n  \n  theta_i[theta_i == 0] &lt;- .Machine$double.eps\n  \n  if (do.current.decoy) {\n    \n    if (FALSE) {\n      a_i &lt;- aggregate.real.spend.pmf\n    } else {\n      a_i &lt;- diff(G_star(as.numeric(0:weekly.z.max)))\n    }\n    \n    \n    a_i[a_i &lt;= 0] &lt;- .Machine$double.eps\n    \n    if (do.sub.support) {\n      \n      set.seed(314)\n      sub.supp &lt;- wrswoR::sample_int_expj(length(theta_i), ceiling(length(theta_i)/10), prob = theta_i)\n      # start at 2 so G(x - 1) is not 0\n      theta_i &lt;- theta_i[sub.supp]\n      a_i &lt;- a_i[sub.supp]\n    } else {\n      sub.supp &lt;- seq_along(theta_i)\n    }\n    \n    stopifnot(length(theta_i) == length(a_i))\n    \n    values.map.decoder.success.prob &lt;- map.decoder.success.prob(f_S = theta_i/sum(theta_i), f_D = a_i/sum(a_i))\n    \n    guess.prob.each.output[sub.supp] &lt;- guess.prob.each.output[sub.supp] + values.map.decoder.success.prob\n    \n    n.decoys &lt;- 15\n    \n    summary.performance[, new.decoy :=\n        sum((theta_i/sum(theta_i)) * (values.map.decoder.success.prob)^n.decoys)]\n    \n  } else {\n    \n    f_D.fun &lt;- run.iters.simple$f_D[[iter]]\n    fitted.par &lt;- fit.results[[iter]]$par\n    log.trans &lt;- run.iters.simple$log.trans[[iter]]\n    \n    if (do.sub.support) {\n      \n      set.seed(314)\n      sub.supp &lt;- wrswoR::sample_int_expj(length(theta_i), ceiling(length(theta_i)/10), prob = theta_i)\n      # start at 2 so G(x - 1) is not 0\n      theta_i &lt;- theta_i[sub.supp]\n      \n    } else {\n      sub.supp &lt;- seq_along(theta_i)\n    }\n    \n    \n    f_D.return &lt;- f_D.fun(fitted.par, v, z, sub.supp, get.decoy.pmf, log.trans = log.trans)\n    \n    \n    # a_i &lt;- c(f_D.return$decoy.pmf, 0)\n    a_i &lt;- f_D.return$decoy.pmf\n    \n    rm(f_D.return)\n    \n    a_i[a_i &lt;= 0] &lt;- .Machine$double.eps\n    \n    \n    \n    stopifnot(length(theta_i) == length(a_i))\n    \n    values.map.decoder.success.prob &lt;- map.decoder.success.prob(f_S = theta_i/sum(theta_i), f_D = a_i/sum(a_i))\n    \n    guess.prob.each.output[sub.supp] &lt;- guess.prob.each.output[sub.supp] + values.map.decoder.success.prob\n    \n    rm(sub.supp)\n    \n    n.decoys &lt;- 15\n    \n    summary.performance[, new.decoy :=\n        sum((theta_i/sum(theta_i)) * (values.map.decoder.success.prob)^n.decoys)]\n    \n  }\n  # Note that this code originally iterated through multiple weeks. Some\n  # of the design decisions relect that history. \n  \n  \n  setnames(summary.performance, \"new.decoy\",\n    ifelse(do.current.decoy, \"current.decoy\", names(run.iters.simple$f_D)[iter]))\n  \n  list(\n    summary.performance = summary.performance,\n    decoy.pmf = a_i,\n    guess.prob.each.output = guess.prob.each.output/nrow(summary.performance))\n  \n}, future.globals = c(\"fit.results\", \"run.iters.simple\", \"aggregate.real.spend.pmf\",\n  \"map.decoder.success.prob\", \"get.decoy.pmf\", \"v\", \"z\", \"actuar::plgamma\", \"wrswoR::sample_int_expj\"),\n  future.packages = \"data.table\", future.seed = TRUE)\n\nprint(keep.time)\nbase::date()\n\n\nfuture::plan(future::sequential)\n# Stop workers to free RAM\ngc()\n\n\nextracted.params &lt;- lapply(analysis.subset[ - length(analysis.subset)], FUN = function(x) {\n  # - length(analysis.subset) to remove the status quo decoy\n  deparsed &lt;- paste0(deparse(run.iters.simple$f_D[[x]]), collapse = \" \")\n  deparsed &lt;- gsub(\"(.* log.trans, *)|(, *tail.beyond.support.*)|(, lower.tail.*)\", \"\", deparsed)\n  deparsed &lt;- gsub(\" = [^ ]*\", \"\", deparsed)\n  deparsed &lt;- strsplit(deparsed, \" \")[[1]]\n  deparsed &lt;- deparsed[deparsed != \"\"]\n  \n  for (i in seq_along(fit.results[[x]]$par)) {\n    param.value &lt;- param.trans[[gsub(\"f_D.\", \"\", names(run.iters.simple$f_D)[x])]][[i]](fit.results[[x]]$par[i])\n    deparsed[i] &lt;- paste0(deparsed[i] , \" = \", formatC(param.value, width = 5))\n  }\n  \n  deparsed\n  \n})\n\nGAMMA_SHAPE = 19.28\nGAMMA_RATE = 1.61\n\nextracted.params[[length(extracted.params) + 1]] &lt;-\n  c(paste0(\"shape = \", GAMMA_SHAPE), paste0(\"rate = \", GAMMA_RATE))\n\nextracted.params &lt;- t(sapply(extracted.params, \"length&lt;-\", max(lengths(extracted.params))))\n# https://stackoverflow.com/questions/15201305/how-to-convert-a-list-consisting-of-vector-of-different-lengths-to-a-usable-data\n\ncolnames(extracted.params) &lt;- paste0(\"Param \", seq_len(ncol(extracted.params)))\n\nn.decoys &lt;- 15\n\nMAP.decoder.effectiveness &lt;- sapply(performance.fit.results, FUN = function(x) {\n  sum((aggregate.real.spend.pmf/sum(aggregate.real.spend.pmf)) * (x$guess.prob.each.output)^n.decoys)\n})\n\ngc()\n\nMAP.decoder.effectiveness &lt;- data.frame(\n  dist.name = analysis.subset.names[analysis.subset],\n  MAP.decoder.effectiveness = formatC(MAP.decoder.effectiveness, width = 5))\n\nMAP.decoder.effectiveness &lt;- cbind(MAP.decoder.effectiveness, extracted.params)\n\nMAP.decoder.effectiveness &lt;- as.data.frame(\n  lapply(MAP.decoder.effectiveness, FUN = function(x) {x[is.na(x)] &lt;- \"\"; x} ))\n\ncolnames(MAP.decoder.effectiveness)[1:2] &lt;- c(\"Decoy distribution\", \"Attack success probability\")\ncolnames(MAP.decoder.effectiveness) &lt;- gsub(\"[.]\", \" \", colnames(MAP.decoder.effectiveness))\n\nMAP.decoder.effectiveness$`Attack success probability` &lt;-\n  scales::percent(as.numeric(MAP.decoder.effectiveness$`Attack success probability`))\n\nknitr::kable(MAP.decoder.effectiveness[ ! analysis.subset %in% low.lambda.subset, ],\n  format = \"pipe\", row.names = FALSE, digits = 3,\n  align = paste0(\"lr\", paste0(rep(\"l\", ncol(extracted.params)), collapse = \"\")))\n\n\n\nMAP.decoder.effectiveness$`λ` &lt;- ifelse(analysis.subset %in% low.lambda.subset, 0.5, 1)\n\nMAP.decoder.effectiveness$`Decoy distribution` &lt;- factor(\n  MAP.decoder.effectiveness$`Decoy distribution`,\n  levels = c(MAP.decoder.effectiveness$`Decoy distribution`[1:6], \"Decoy status quo\")\n)\n\nMAP.decoder.effectiveness &lt;- MAP.decoder.effectiveness[, c(\n  \"Decoy distribution\", \"λ\",  \"Attack success probability\",\n  paste0(\"Param \", seq_len(ncol(extracted.params))) ) ]\n\nknitr::kable(MAP.decoder.effectiveness[\n  order(MAP.decoder.effectiveness$`Decoy distribution`,  - MAP.decoder.effectiveness$`λ`), ],\n  format = \"pipe\", row.names = FALSE, digits = 3,\n  align = paste0(\"lrr\", paste0(rep(\"l\", ncol(extracted.params)), collapse = \"\")))\n\n\n\n\n\nguess.prob.each.output &lt;- performance.fit.results[[1]]$guess.prob.each.output\n\ndisplay.x &lt;- unique(floor(exp(seq(1, 100, by = 0.1))))\ndisplay.x &lt;- display.x[display.x &lt;= length(guess.prob.each.output)]\ndisplay.x &lt;- display.x[guess.prob.each.output[display.x] != 0]\n\n\n\ndecoy.plot.data &lt;- lapply(setdiff(analysis.subset, low.lambda.subset), function(x) {\n  \n  if (x == nrow(run.iters.simple) + 1) {\n    \n    type &lt;- analysis.subset.names[x]\n    \n    GAMMA_SHAPE = 19.28\n    GAMMA_RATE = 1.61\n    \n    G &lt;- function(x) {\n      actuar::plgamma(x, shapelog = GAMMA_SHAPE, ratelog = GAMMA_RATE)\n    }\n    \n    G_star &lt;- function(x) {\n      (0 &lt;= x*v & x*v &lt;= 1800) *\n        (G(x*v + 1200) - G(1200) +\n            ( (x*v)/(1800) ) * G(1200)\n        )/G(z*v) +\n        (1800 &lt; x*v & x*v &lt;= z*v) * G(x*v + 1200)/G(z*v) +\n        (z*v &lt; x*v) * 1\n    }\n    \n    weekly.z.max &lt;- z\n    \n    f_D.return &lt;- diff(G_star(as.numeric(0:weekly.z.max)))[display.x]\n    \n    \n  } else {\n    type &lt;- paste0(\"Decoy \", analysis.subset.names[x])\n    \n    f_D.fun &lt;- run.iters.simple$f_D[[x]]\n    fitted.par &lt;- fit.results[[x]]$par\n    log.trans &lt;- run.iters.simple$log.trans[[x]]\n    \n    f_D.return &lt;- f_D.fun(fitted.par, v, z, sub.supp = display.x, get.decoy.pmf, log.trans = log.trans)$decoy.pmf\n  }\n  \n  cat(type, \" \", which(x == analysis.subset), \" \", x, \"\\n\")\n  \n  data.frame(x = display.x, y = f_D.return, type = type)\n  \n})\n\n\ndecoy.plot.data &lt;- rbind(\n  do.call(rbind, decoy.plot.data),\n  data.frame(x = display.x, y = aggregate.real.spend.pmf[display.x], type = \"PMF real spend\"),\n  data.frame(x = display.x, y = cumsum(aggregate.real.spend.pmf)[display.x], type = \"CDF real spend\")\n)\n\n\n\n\ndecoy.plot.data$panel &lt;- ifelse(decoy.plot.data$type == \"CDF real spend\",\n  \"CDF\", \"PMF\")\n\ndecoy.plot.data$panel &lt;- factor(decoy.plot.data$panel, levels = c(\"PMF\", \"CDF\"))\n\ndecoy.plot.data$type &lt;- factor(decoy.plot.data$type,\n  levels = c(\"PMF real spend\", \n    unique(decoy.plot.data$type)[grepl(\"Decoy\", unique(decoy.plot.data$type))],\n    \"CDF real spend\"))\n\n\npng(\"images/decoy-distributions-top-1-3-not-log.png\", width = 1000, height = 1000)\n\n\nggplot(decoy.plot.data[decoy.plot.data$type %in% c(\"PMF real spend\", \"CDF real spend\",\n  \"Decoy status quo\", paste0(\"Decoy \", analysis.subset.names[analysis.subset][1:3])), ],\n  aes(x = x, y = y, colour = type)) +\n  labs( title = \"PMF comparison: Top 3 Improved Decoy Distributions\",\n    y = \"Probability\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1/v, 10/v, 60/v, 60*2/v, 60*30/v, 60^2/v, 60^2*12/v, 60^2*24/v, 60^2*24*7/v, 60^2*24*28/v, 60^2*24*365/v),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hr\", \"12 hr\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  ggh4x::facet_manual(facets = vars(panel), design = \"A\\nB\", scale = \"free_y\", heights = c(8, 2)) +\n  guides(linewidth = \"none\", linetype = \"none\",\n    colour = guide_legend(override.aes = list(linewidth = 5))) +\n  ggh4x::facetted_pos_scales(y = list(scale_y_continuous(), scale_y_continuous())) +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA))\n# The order of the themes() matter. Must have theme_dracula() first.\n\ndev.off()\n\n\npng(\"images/decoy-distributions-top-4-6-not-log.png\", width = 1000, height = 1000)\n\n\nggplot(decoy.plot.data[decoy.plot.data$type %in% c(\"PMF real spend\", \"CDF real spend\",\n  \"Decoy status quo\", paste0(\"Decoy \", analysis.subset.names[analysis.subset][4:6])), ],\n  aes(x = x, y = y, colour = type)) +\n  labs( title = \"PMF comparison: Top 4-6 Improved Decoy Distributions\",\n    y = \"Probability\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1/v, 10/v, 60/v, 60*2/v, 60*30/v, 60^2/v, 60^2*12/v, 60^2*24/v, 60^2*24*7/v, 60^2*24*28/v, 60^2*24*365/v),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hr\", \"12 hr\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  ggh4x::facet_manual(facets = vars(panel), design = \"A\\nB\", scale = \"free_y\", heights = c(8, 2)) +\n  guides(linewidth = \"none\", linetype = \"none\",\n    colour = guide_legend(override.aes = list(linewidth = 5))) +\n  ggh4x::facetted_pos_scales(y = list(scale_y_continuous(), scale_y_continuous())) +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA))\n# The order of the themes() matter. Must have theme_dracula() first.\n\ndev.off()\n\n\n\npng(\"images/decoy-distributions-top-1-3-log.png\", width = 1000, height = 1000)\n\n\nggplot(decoy.plot.data[decoy.plot.data$type %in% c(\"PMF real spend\", \"CDF real spend\",\n  \"Decoy status quo\", paste0(\"Decoy \", analysis.subset.names[analysis.subset][1:3])), ],\n  aes(x = x, y = y, colour = type)) +\n  labs( title = \"PMF comparison: Top 3 Improved Decoy Distributions (Log Vertical Scale)\",\n    y = \"Probability (Top: log scale. Bottom: normal scale.)\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1/v, 10/v, 60/v, 60*2/v, 60*30/v, 60^2/v, 60^2*12/v, 60^2*24/v, 60^2*24*7/v, 60^2*24*28/v, 60^2*24*365/v),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hr\", \"12 hr\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  ggh4x::facet_manual(facets = vars(panel), design = \"A\\nB\", scale = \"free_y\", heights = c(8, 2)) +\n  guides(linewidth = \"none\", linetype = \"none\",\n    colour = guide_legend(override.aes = list(linewidth = 5))) +\n  ggh4x::facetted_pos_scales(y = list(scale_y_log10(), scale_y_continuous())) +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA))\n# The order of the themes() matter. Must have theme_dracula() first.\n\ndev.off()\n\n\n\npng(\"images/decoy-distributions-top-4-6-log.png\", width = 1000, height = 1000)\n\n\nggplot(decoy.plot.data[decoy.plot.data$type %in% c(\"PMF real spend\", \"CDF real spend\",\n  \"Decoy status quo\", paste0(\"Decoy \", analysis.subset.names[analysis.subset][4:6])), ],\n  aes(x = x, y = y, colour = type)) +\n  labs( title = \"PMF comparison: Top 4-6 Improved Decoy Distributions (Log Vertical Scale)\",\n    y = \"Probability (Top: log scale. Bottom: normal scale.)\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1/v, 10/v, 60/v, 60*2/v, 60*30/v, 60^2/v, 60^2*12/v, 60^2*24/v, 60^2*24*7/v, 60^2*24*28/v, 60^2*24*365/v),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hr\", \"12 hr\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  ggh4x::facet_manual(facets = vars(panel), design = \"A\\nB\", scale = \"free_y\", heights = c(8, 2)) +\n  guides(linewidth = \"none\", linetype = \"none\",\n    colour = guide_legend(override.aes = list(linewidth = 5))) +\n  ggh4x::facetted_pos_scales(y = list(scale_y_log10(), scale_y_continuous())) +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA))\n# The order of the themes() matter. Must have theme_dracula() first.\n\n\ndev.off()\n\n\n\n\n# Attack success probability\n\n\nMAP.decoder.plot.data &lt;- lapply(setdiff(analysis.subset, low.lambda.subset), function(x) {\n  if (x == nrow(run.iters.simple) + 1) {\n    type &lt;- \"Decoy status quo\"\n  } else {\n    type &lt;- type &lt;- paste0(\"Decoy \", analysis.subset.names[x])\n  }\n  cat(type, \" \", which(x == analysis.subset), \" \", x, \"\\n\")\n  guess.prob.each.output &lt;- performance.fit.results[[which(x == analysis.subset)]]$guess.prob.each.output\n  n.decoys &lt;- 15\n  data.frame(x = display.x, y = guess.prob.each.output[display.x]^n.decoys, type = type)\n})\n\n\n\nMAP.decoder.plot.data &lt;- rbind(\n  do.call(rbind, MAP.decoder.plot.data),\n  data.frame(x = display.x, y = cumsum(aggregate.real.spend.pmf)[display.x], type = \"CDF real spend\")\n)\n\n\nMAP.decoder.plot.data$type &lt;- gsub(\"f_D.\", \"Decoy \", MAP.decoder.plot.data$type)\n\n\n\nMAP.decoder.plot.data$panel &lt;- ifelse(MAP.decoder.plot.data$type == \"CDF real spend\",\n  \"CDF\", \"MAP decoder attack success probability\")\n\nMAP.decoder.plot.data$panel &lt;- factor(MAP.decoder.plot.data$panel,\n  levels = c(\"MAP decoder attack success probability\", \"CDF\"))\n\nMAP.decoder.plot.data$type &lt;- factor(MAP.decoder.plot.data$type,\n  levels = c(\"PMF real spend\", \n    unique(MAP.decoder.plot.data$type)[grepl(\"Decoy\", unique(MAP.decoder.plot.data$type))],\n    \"CDF real spend\"))\n\n\n\npng(\"images/attack-success-top-1-3.png\", width = 1000, height = 1000)\n\n\n\nggplot(MAP.decoder.plot.data[MAP.decoder.plot.data$type %in% c(\"PMF real spend\", \"CDF real spend\",\n  \"Decoy status quo\", paste0(\"Decoy \", analysis.subset.names[analysis.subset][1:3])), ],\n  aes(x = x, y = y, colour = type)) +\n  labs( title = \"Attack Success Comparison: Top 3 Improved Decoy Distributions\",\n    y = \"Probability\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1/v, 10/v, 60/v, 60*2/v, 60*30/v, 60^2/v, 60^2*12/v, 60^2*24/v, 60^2*24*7/v, 60^2*24*28/v, 60^2*24*365/v),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hr\", \"12 hr\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  ggh4x::facet_manual(facets = vars(panel), design = \"A\\nB\", scale = \"free_y\", heights = c(8, 2)) +\n  guides(linewidth = \"none\", linetype = \"none\",\n    colour = guide_legend(override.aes = list(linewidth = 5))) +\n  ggh4x::facetted_pos_scales(y = list(scale_y_continuous(), scale_y_continuous())) +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA))\n# The order of the themes() matter. Must have theme_dracula() first.\n\n\ndev.off()\n\n\n\npng(\"images/attack-success-top-4-6.png\", width = 1000, height = 1000)\n\n\nggplot(MAP.decoder.plot.data[MAP.decoder.plot.data$type %in% c(\"PMF real spend\", \"CDF real spend\",\n  \"Decoy status quo\", paste0(\"Decoy \", analysis.subset.names[analysis.subset][4:6])), ],\n  aes(x = x, y = y, colour = type)) +\n  labs( title = \"Attack Success Comparison: Top 4-6 Improved Decoy Distributions\",\n    y = \"Probability\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1/v, 10/v, 60/v, 60*2/v, 60*30/v, 60^2/v, 60^2*12/v, 60^2*24/v, 60^2*24*7/v, 60^2*24*28/v, 60^2*24*365/v),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hr\", \"12 hr\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  ggh4x::facet_manual(facets = vars(panel), design = \"A\\nB\", scale = \"free_y\", heights = c(8, 2)) +\n  guides(linewidth = \"none\", linetype = \"none\",\n    colour = guide_legend(override.aes = list(linewidth = 5))) +\n  ggh4x::facetted_pos_scales(y = list(scale_y_continuous(), scale_y_continuous())) +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA))\n# The order of the themes() matter. Must have theme_dracula() first.\n\n\ndev.off()\n\n\n\n\n\n\n\n\n# Compare different lambdas\n\n\n\n\nMAP.decoder.plot.data &lt;- lapply(analysis.subset, function(x) {\n  if (x == nrow(run.iters.simple) + 1) {\n    type &lt;- \"Decoy status quo\"\n  } else {\n    type &lt;- type &lt;- paste0(\"Decoy \", analysis.subset.names[x])\n  }\n  cat(type, \" \", which(x == analysis.subset), \" \", x, \"\\n\")\n  guess.prob.each.output &lt;- performance.fit.results[[which(x == analysis.subset)]]$guess.prob.each.output\n  n.decoys &lt;- 15\n  data.frame(x = display.x, y = guess.prob.each.output[display.x]^n.decoys,\n    type = type, lambda = run.iters.simple$lambda[x])\n})\n\n\n\nMAP.decoder.plot.data &lt;- rbind(\n  do.call(rbind, MAP.decoder.plot.data),\n  data.frame(x = display.x, y = cumsum(aggregate.real.spend.pmf)[display.x],\n    type = \"CDF real spend\", lambda = 1)\n)\n\n\nMAP.decoder.plot.data$type &lt;- gsub(\"f_D.\", \"Decoy \", MAP.decoder.plot.data$type)\n\nMAP.decoder.plot.data$type.reserved &lt;- MAP.decoder.plot.data$type\n\n\n\n# Start specific distributions\n\n\nlambda.distribution &lt;- \"Decoy Log-GB2\"\n\nMAP.decoder.plot.data$type[MAP.decoder.plot.data$type == lambda.distribution] &lt;- \n  paste0(lambda.distribution, \", lambda = \", \n    MAP.decoder.plot.data$lambda[MAP.decoder.plot.data$type == lambda.distribution])\n\n\nMAP.decoder.plot.data$panel &lt;- ifelse(MAP.decoder.plot.data$type == \"CDF real spend\",\n  \"CDF\", \"MAP decoder attack success probability\")\n\nMAP.decoder.plot.data$panel &lt;- factor(MAP.decoder.plot.data$panel,\n  levels = c(\"MAP decoder attack success probability\", \"CDF\"))\n\nMAP.decoder.plot.data$type &lt;- factor(MAP.decoder.plot.data$type,\n  levels = c(\"PMF real spend\", \n    unique(MAP.decoder.plot.data$type)[grepl(\"Decoy\", unique(MAP.decoder.plot.data$type))],\n    \"CDF real spend\"))\n\n\n\npng(paste0(\"images/attack-success-lambda-\", tolower(gsub(\" \", \"-\", lambda.distribution)), \".png\"),\n  width = 1000, height = 1000)\n\n\nggplot(MAP.decoder.plot.data[MAP.decoder.plot.data$type %in% c(\"PMF real spend\", \"CDF real spend\",\n  \"Decoy status quo\") | grepl(lambda.distribution, MAP.decoder.plot.data$type), ],\n  aes(x = x, y = y, colour = type)) +\n  labs( title = paste0(\"Attack Success Comparison with Different Lambdas: \", lambda.distribution),\n    y = \"Probability\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1/v, 10/v, 60/v, 60*2/v, 60*30/v, 60^2/v, 60^2*12/v, 60^2*24/v, 60^2*24*7/v, 60^2*24*28/v, 60^2*24*365/v),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hr\", \"12 hr\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  ggh4x::facet_manual(facets = vars(panel), design = \"A\\nB\", scale = \"free_y\", heights = c(8, 2)) +\n  guides(linewidth = \"none\", linetype = \"none\",\n    colour = guide_legend(override.aes = list(linewidth = 5))) +\n  ggh4x::facetted_pos_scales(y = list(scale_y_continuous(), scale_y_continuous())) +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA))\n# The order of the themes() matter. Must have theme_dracula() first.\n\ndev.off()\n\n\n\n\n\n\n\nlambda.distribution &lt;- \"Decoy Log-GEV\"\n\nMAP.decoder.plot.data$type &lt;- MAP.decoder.plot.data$type.reserved\n\nMAP.decoder.plot.data$type[MAP.decoder.plot.data$type == lambda.distribution] &lt;- \n  paste0(lambda.distribution, \", lambda = \", \n    MAP.decoder.plot.data$lambda[MAP.decoder.plot.data$type == lambda.distribution])\n\nMAP.decoder.plot.data$type &lt;- factor(MAP.decoder.plot.data$type,\n  levels = c(\"PMF real spend\", \n    unique(MAP.decoder.plot.data$type)[grepl(\"Decoy\", unique(MAP.decoder.plot.data$type))],\n    \"CDF real spend\"))\n\n\n\npng(paste0(\"images/attack-success-lambda-\", tolower(gsub(\" \", \"-\", lambda.distribution)), \".png\"),\n  width = 1000, height = 1000)\n\n\nggplot(MAP.decoder.plot.data[MAP.decoder.plot.data$type %in% c(\"PMF real spend\", \"CDF real spend\",\n  \"Decoy status quo\") | grepl(lambda.distribution, MAP.decoder.plot.data$type), ],\n  aes(x = x, y = y, colour = type)) +\n  labs( title = paste0(\"Attack Success Comparison with Different Lambdas: \", lambda.distribution),\n    y = \"Probability\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1/v, 10/v, 60/v, 60*2/v, 60*30/v, 60^2/v, 60^2*12/v, 60^2*24/v, 60^2*24*7/v, 60^2*24*28/v, 60^2*24*365/v),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hr\", \"12 hr\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  ggh4x::facet_manual(facets = vars(panel), design = \"A\\nB\", scale = \"free_y\", heights = c(8, 2)) +\n  guides(linewidth = \"none\", linetype = \"none\",\n    colour = guide_legend(override.aes = list(linewidth = 5))) +\n  ggh4x::facetted_pos_scales(y = list(scale_y_continuous(), scale_y_continuous())) +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA))\n# The order of the themes() matter. Must have theme_dracula() first.\n\ndev.off()\n\n\n\n\n\n\nlambda.distribution &lt;- \"Decoy RPLN\"\n\nMAP.decoder.plot.data$type &lt;- MAP.decoder.plot.data$type.reserved\n\nMAP.decoder.plot.data$type[MAP.decoder.plot.data$type == lambda.distribution] &lt;- \n  paste0(lambda.distribution, \", lambda = \", \n    MAP.decoder.plot.data$lambda[MAP.decoder.plot.data$type == lambda.distribution])\n\nMAP.decoder.plot.data$type &lt;- factor(MAP.decoder.plot.data$type,\n  levels = c(\"PMF real spend\", \n    unique(MAP.decoder.plot.data$type)[grepl(\"Decoy\", unique(MAP.decoder.plot.data$type))],\n    \"CDF real spend\"))\n\n\n\npng(paste0(\"images/attack-success-lambda-\", tolower(gsub(\" \", \"-\", lambda.distribution)), \".png\"),\n  width = 1000, height = 1000)\n\n\nggplot(MAP.decoder.plot.data[MAP.decoder.plot.data$type %in% c(\"PMF real spend\", \"CDF real spend\",\n  \"Decoy status quo\") | grepl(lambda.distribution, MAP.decoder.plot.data$type), ],\n  aes(x = x, y = y, colour = type)) +\n  labs( title = paste0(\"Attack Success Comparison with Different Lambdas: \", lambda.distribution),\n    y = \"Probability\",\n    x = \"Output age (log scale)\") +\n  geom_line() +\n  scale_x_log10(\n    guide = guide_axis(angle = 90),\n    breaks = c(1/v, 10/v, 60/v, 60*2/v, 60*30/v, 60^2/v, 60^2*12/v, 60^2*24/v, 60^2*24*7/v, 60^2*24*28/v, 60^2*24*365/v),\n    labels = c(\"1 sec\", \"10 sec\", \"1 min\", \"2 min\", \"30 min\", \"1 hr\", \"12 hr\", \"1 day\", \"1 week\", \"1 month\", \"1 year\")) +\n  # scale_colour_brewer(palette = \"Dark2\") +\n  ggh4x::facet_manual(facets = vars(panel), design = \"A\\nB\", scale = \"free_y\", heights = c(8, 2)) +\n  guides(linewidth = \"none\", linetype = \"none\",\n    colour = guide_legend(override.aes = list(linewidth = 5))) +\n  ggh4x::facetted_pos_scales(y = list(scale_y_continuous(), scale_y_continuous())) +\n  theme_dracula() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"white\", fill = NA))\n# The order of the themes() matter. Must have theme_dracula() first.\n\ndev.off()",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Performance Evaluation</span>"
    ]
  },
  {
    "objectID": "appendix-bjr-benchmarks.html",
    "href": "appendix-bjr-benchmarks.html",
    "title": "14  Appendix: BJR Benchmarks",
    "section": "",
    "text": "14.1 Comparison of R and original Octave implementation\nWhen restricted to one CPU thread, my R implementation of the BJR estimator is about 240 times faster than Jochmans’ original Octave/MATLAB implementation.\nWhen processing 10,000 rings and using 10 CPU threads, my multi-threaded R implementation is 7 times faster than my R implementation in single-threaded mode.\nThe benchmarks were performed on an AMD Ryzen Threadripper 3970X CPU.\nThe Octave implementation is a modification of the original implementation included with the BJR paper. Code that was not involved in estimating the CDF, such as mean estimation code, was removed. The hardcoded limit of 4 repeated measurements was raised to 16.\nGenerate the test data and estimate the model in an R session:\nlibrary(decoyanalysis)\n\nset.seed(314)\n\ngenerated.data &lt;- gen.bjr.normal.data(N = 100, T = 16, K = 2,\n  theta1 = c(0, 3), theta2 = c(1, 1), omega = c(0.3, 0.7))\n\nwrite.table(generated.data, file = \"octave/bjr-benchmark-data.csv\",\n  sep = \",\", col.names = FALSE, row.names = FALSE)\n\nsystem.time({\n  generated.data.results &lt;- bjr(generated.data,\n    II = 10, K = 2, cdf.points = seq(-4, 7, by = 0.25),\n    estimate.mean.sd = FALSE)\n})\nThe system.time() function will print the number of seconds it took to complete the estimation. “elapsed” is the wall-clock time. On the benchmark machine, the elapsed time was 9.6 seconds.\nNext, install Octave, which is an open-source implementation of the proprietary MATLAB programming language. On Linux, it is probably easiest to install the Flatpak version:\nsudo flatpak remote-add --if-not-exists flathub https://dl.flathub.org/repo/flathub.flatpakrepo\nsudo flatpak install flathub org.octave.Octave\nGo to the octave directory of OSPEAD-docs and initiate an Octave session:\ncd OSPEAD-docs/octave\nflatpak run org.octave.Octave\nThen import the test data generated by R and estimate the model:\ny = importdata(\"bjr-benchmark-data.csv\", \",\");\nK = 2;\nII = 10;\ntic; [OUT XI L] = bjrCDFonly16T(y, K, II); toc\nOnce done, Octave should print the elapsed time. On the benchmark machine, the Octave implementation took 2307.5 seconds to run.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix: BJR Benchmarks</span>"
    ]
  },
  {
    "objectID": "appendix-bjr-benchmarks.html#comparison-of-r-single--and-multi-threaded-implementation",
    "href": "appendix-bjr-benchmarks.html#comparison-of-r-single--and-multi-threaded-implementation",
    "title": "14  Appendix: BJR Benchmarks",
    "section": "14.2 Comparison of R single- and multi-threaded implementation",
    "text": "14.2 Comparison of R single- and multi-threaded implementation\n\nlibrary(decoyanalysis)\n\nthreads &lt;- 10\n\nset.seed(314)\n\ngenerated.data.large &lt;- gen.bjr.normal.data(N = 10000, T = 16, K = 2,\n  theta1 = c(0, 3), theta2 = c(1, 1), omega = c(0.3, 0.7))\n\nfuture::plan(future::sequential)\n\nsystem.time({\n  generated.data.results &lt;- bjr(generated.data.large,\n    II = 10, K = 2, cdf.points = seq(-4, 7, by = 0.25),\n    estimate.mean.sd = FALSE)\n})\n\nfuture::plan(future::multisession(workers = threads))\n\nsystem.time({\n  generated.data.results &lt;- bjr(generated.data.large,\n    II = 10, K = 2, cdf.points = seq(-4, 7, by = 0.25),\n    estimate.mean.sd = FALSE)\n})\n\nfuture::plan(future::sequential)\n\nThis time, the number of rings is set to 10,000 instead of 100. The first printed elapsed time will be the single-threaded execution time. On the benchmark machine, this time was 434.8 seconds. The second printed time will be the execution time with 10 threads. On the benchmark machine, this time was 62.9 seconds.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix: BJR Benchmarks</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aeeneh, Sina, João Otávio Chervinski, Jiangshan Yu, and Nikola Zlatanov.\n2021. “New Attacks on the Untraceability of Transactions in\nCryptoNote-Style Blockchains.” In 2021 IEEE International\nConference on Blockchain and Cryptocurrency (ICBC), 1–5. https://doi.org/10.1109/ICBC51069.2021.9461130.\n\n\nBonhomme, Stéphane, Koen Jochmans, and Jean-Marc Robin. 2016.\n“Non-Parametric Estimation of Finite Mixtures from Repeated\nMeasurements.” Journal of the Royal Statistical Society.\nSeries B (Statistical Methodology) 78 (1): 211–29. http://www.jstor.org/stable/24775334.\n\n\nHippel, Paul T. von, David J. Hunter, and McKalie Drown. 2017.\n“Better Estimates from Binned Income Data: Interpolated CDFs and\nMean-Matching.” Sociological Science 4 (26): 641–55. https://doi.org/10.15195/v4.a26.\n\n\nKrawiec-Thayer, Mitchell P., Neptune, Rucknium, Jberman, and Carrington.\n2021. “Fingerprinting a Flood: Forensic Statistical Analysis of\nthe Mid-2021 Monero Transaction Volume Anomaly.” https://mitchellpkt.medium.com/fingerprinting-a-flood-forensic-statistical-analysis-of-the-mid-2021-monero-transaction-volume-a19cbf41ce60.\n\n\nMakarov, Igor, and Antoinette Schoar. 2021. “Blockchain Analysis\nof the Bitcoin Market.” Working Paper 29396. Working Paper\nSeries. National Bureau of Economic Research. https://doi.org/10.3386/w29396.\n\n\nNoether, Surae, Sarang Noether, and Adam Mackenzie. 2014. “A Note\non Chain Reactions in Traceability in CryptoNote 2.0.” Research\nBulletin. https://www.getmonero.org/resources/research-lab/pubs/MRL-0001.pdf.\n\n\nPatra, Rohit Kumar, and Bodhisattva Sen. 2016. “Estimation of a\nTwo-Component Mixture Model with Applications to Multiple\nTesting.” Journal of the Royal Statistical Society. Series B\n(Statistical Methodology) 78 (4): 869–93. http://www.jstor.org/stable/24775367.\n\n\nReed, William J., and Murray Jorgensen. 2004. “The Double\nPareto-Lognormal Distribution—a New Parametric Model for Size\nDistributions.” Communications in Statistics - Theory and\nMethods 33 (8): 1733–53. https://doi.org/10.1081/STA-120037438.\n\n\nRucknium. 2023. “Centralized Mining Pools Are Delaying Monero\nTransaction Confirmations by 60 Seconds.” https://rucknium.me/posts/monero-pool-transaction-delay/.\n\n\n———. 2024. “March 2024 Suspected Black Marble Flooding Against\nMonero: Privacy, User Experience, and Countermeasures.” https://github.com/Rucknium/misc-research/blob/main/Monero-Black-Marble-Flood/pdf/monero-black-marble-flood.pdf.",
    "crumbs": [
      "References"
    ]
  }
]